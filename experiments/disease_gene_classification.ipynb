{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Disease type classification for disease genes.\n",
    "\n",
    "## Contents\n",
    "* [Load gene and disease network.](#Load-gene-and-disease-network)\n",
    "* [Load the training data.](#Load-the-training-data.)\n",
    "* [Define-the-training process.](#Define-the-training-process.)\n",
    "* [Train the classificatrion model.](#Train-the-classificatrion-model.)\n",
    "* [Eval the results.](#Eval-the-results.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import logging\n",
    "import os\n",
    "import os.path as osp\n",
    "import torch\n",
    "import random\n",
    "import time\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "from torch.nn import Linear\n",
    "import numpy as np\n",
    "from IPython.display import display, HTML\n",
    "from torch_geometric.nn import GCNConv, SAGEConv, GraphConv\n",
    "from sklearn.metrics import label_ranking_average_precision_score, label_ranking_loss, roc_auc_score\n",
    "from sklearn.metrics import roc_curve, precision_recall_curve, auc, average_precision_score\n",
    "from torch_geometric.data import InMemoryDataset, Data\n",
    "from sklearn.model_selection import KFold, StratifiedKFold, train_test_split\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import gzip\n",
    "from livelossplot import PlotLosses\n",
    "import sklearn\n",
    "\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Expreiment hyperparameters\n",
    "\n",
    "#  NEGATIVE_SAMPLES determines how the negative examples for training are created.\n",
    "#  Choose from {'random', 'random_only_disease_gene'}\n",
    "#    * random: Choose a random (gene, disease) pair which is not in the positive set.\n",
    "#    * random_only_disease_genes: Like random but gene must be assigned to at least one disease.\n",
    "NEGATIVE_SAMPLES = 'random'\n",
    "EXPERIMENT_SLUG = 'final'\n",
    "device = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load gene and disease network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(edge_attr=[371502, 1], edge_index=[2, 371502], x=[17247, 6209])\n",
      "Data(edge_attr=[282464], edge_index=[2, 282464], x=[8827, 11460])\n"
     ]
    }
   ],
   "source": [
    "# Define Constants.\n",
    "HERE = osp.abspath('')\n",
    "ROOT = osp.join(HERE, '..', '..')\n",
    "DATA_SOURCE_PATH = osp.join(ROOT, 'data_sources')\n",
    "GENE_DATASET_ROOT = osp.join(DATA_SOURCE_PATH, 'gene_net_dataset_fn_and_hpo_features')\n",
    "DISEASE_DATASET_ROOT = osp.join(DATA_SOURCE_PATH, 'disease_net_no_hpo_sim_based')\n",
    "RESULTS_STORAGE = osp.join(HERE, 'results', EXPERIMENT_SLUG)\n",
    "MODEL_TMP_STORAGE = osp.join('/', 'var', 'tmp', 'dg_tmp')\n",
    "sys.path.insert(0, osp.abspath(ROOT))\n",
    "\n",
    "# Generate the disease and gene network.\n",
    "from GeneNet import GeneNet\n",
    "from DiseaseNet import DiseaseNet\n",
    "from TheModel import TheModel\n",
    "\n",
    "gene_dataset = GeneNet(\n",
    "    root=GENE_DATASET_ROOT,\n",
    "    humannet_version='FN',\n",
    "    features_to_use='hpo',\n",
    "    skip_truncated_svd=True\n",
    ")\n",
    "\n",
    "disease_dataset = DiseaseNet(\n",
    "    root=DISEASE_DATASET_ROOT,\n",
    "    hpo_count_freq_cutoff=40,\n",
    "    edge_source='feature_similarity',\n",
    "    feature_source='disease_publications',\n",
    "    skip_truncated_svd=True,\n",
    "    svd_components=2048,\n",
    "    svd_n_iter=12\n",
    ")\n",
    "gene_net_data = gene_dataset[0]\n",
    "disease_net_data = disease_dataset[0]\n",
    "print(gene_net_data)\n",
    "print(disease_net_data)\n",
    "gene_net_data = gene_net_data.to(device)\n",
    "disease_net_data = disease_net_data.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate training data.\n",
    "disease_genes = pd.read_table(\n",
    "    osp.join(DATA_SOURCE_PATH, 'genes_diseases.tsv'),\n",
    "    names=['EntrezGene ID', 'OMIM ID'],\n",
    "    sep='\\t', \n",
    "    low_memory=False, \n",
    "    dtype={'EntrezGene ID': pd.Int64Dtype()}\n",
    ")\n",
    "\n",
    "disease_id_index_feature_mapping = disease_dataset.load_disease_index_feature_mapping()\n",
    "gene_id_index_feature_mapping = gene_dataset.load_node_index_mapping()\n",
    "\n",
    "all_genes = list(gene_id_index_feature_mapping.keys())\n",
    "all_diseases = list(disease_id_index_feature_mapping.keys())\n",
    "\n",
    "# 1. generate positive pairs.\n",
    "# Filter the pairs to only include the ones where the corresponding nodes are available.\n",
    "# i.e. gene_id should be in all_genes and disease_id should be in all_diseases.\n",
    "positives = disease_genes[\n",
    "    disease_genes[\"OMIM ID\"].isin(all_diseases) & disease_genes[\"EntrezGene ID\"].isin(all_genes)\n",
    "]\n",
    "covered_diseases = list(set(positives['OMIM ID']))\n",
    "covered_genes = list(set(positives['EntrezGene ID']))\n",
    "\n",
    "# 2. Generate negatives.\n",
    "# Pick equal amount of pairs not in the positives.\n",
    "negatives_list = []\n",
    "while len(negatives_list) < len(positives):\n",
    "    if NEGATIVE_SAMPLES == 'random_only_disease_genes' :\n",
    "        gene_id = covered_genes[np.random.randint(0, len(covered_genes))]\n",
    "    else:\n",
    "        gene_id = all_genes[np.random.randint(0, len(all_genes))]\n",
    "    disease_id = covered_diseases[np.random.randint(0, len(covered_diseases))]\n",
    "    if not ((positives['OMIM ID'] == disease_id) & (positives['EntrezGene ID'] == gene_id)).any():\n",
    "        negatives_list.append([disease_id, gene_id])\n",
    "negatives = pd.DataFrame(np.array(negatives_list), columns=['OMIM ID', 'EntrezGene ID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>disease_id</th>\n",
       "      <th>gene_id</th>\n",
       "      <th>disease_class</th>\n",
       "      <th>node_index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>27</td>\n",
       "      <td>1836</td>\n",
       "      <td>Bone</td>\n",
       "      <td>1582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>160</td>\n",
       "      <td>1836</td>\n",
       "      <td>Connective tissue</td>\n",
       "      <td>1582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>430</td>\n",
       "      <td>1836</td>\n",
       "      <td>Skeletal</td>\n",
       "      <td>1582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>496</td>\n",
       "      <td>1836</td>\n",
       "      <td>Bone</td>\n",
       "      <td>1582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1586</td>\n",
       "      <td>Endocrine</td>\n",
       "      <td>5881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2673</th>\n",
       "      <td>1615</td>\n",
       "      <td>5195</td>\n",
       "      <td>multiple</td>\n",
       "      <td>9456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2674</th>\n",
       "      <td>1615</td>\n",
       "      <td>5824</td>\n",
       "      <td>multiple</td>\n",
       "      <td>218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2675</th>\n",
       "      <td>1615</td>\n",
       "      <td>5825</td>\n",
       "      <td>multiple</td>\n",
       "      <td>167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2676</th>\n",
       "      <td>1615</td>\n",
       "      <td>9409</td>\n",
       "      <td>multiple</td>\n",
       "      <td>6077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2677</th>\n",
       "      <td>1615</td>\n",
       "      <td>8504</td>\n",
       "      <td>multiple</td>\n",
       "      <td>219</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2678 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      disease_id  gene_id      disease_class  node_index\n",
       "0             27     1836               Bone        1582\n",
       "1            160     1836  Connective tissue        1582\n",
       "2            430     1836           Skeletal        1582\n",
       "3            496     1836               Bone        1582\n",
       "4              1     1586          Endocrine        5881\n",
       "...          ...      ...                ...         ...\n",
       "2673        1615     5195           multiple        9456\n",
       "2674        1615     5824           multiple         218\n",
       "2675        1615     5825           multiple         167\n",
       "2676        1615     9409           multiple        6077\n",
       "2677        1615     8504           multiple         219\n",
       "\n",
       "[2678 rows x 4 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Disease classification data preparation.\n",
    "# Load the disease classes.\n",
    "GENE_CLASS_LABELS_FILE = osp.join(DATA_SOURCE_PATH, 'extracted_disease_class_assignments.tsv')\n",
    "# Load the training data\n",
    "disease_class_training_data = pd.read_csv(GENE_CLASS_LABELS_FILE, sep='\\t')\n",
    "# drop duplicates\n",
    "unique_labeled_disease_class_genes = disease_class_training_data.drop_duplicates()\n",
    "gene_id_node_index_df = pd.DataFrame(data=[(gene_id, node_index) for gene_id, node_index in gene_id_index_feature_mapping.items()], columns=['gene_id', 'node_index'])\n",
    "disease_id_node_index_df = disease_id_node_index_df = pd.DataFrame(\n",
    "    data=[(disease_id, node_index) for disease_id, node_index in disease_id_index_feature_mapping.items()], columns=['disease_id', 'disease_node_index']\n",
    ")\n",
    "\n",
    "# Create the gene index\n",
    "# Join in the gene node indexes\n",
    "disease_class_training_data = pd.merge(\n",
    "    unique_labeled_disease_class_genes, \n",
    "    gene_id_node_index_df, \n",
    "    left_on='gene_id', \n",
    "    right_on='gene_id',\n",
    "    validate='many_to_many'\n",
    ")\n",
    "\n",
    "disease_class_counts = disease_class_training_data['disease_class'].value_counts()\n",
    "disease_class_target_classes = [\n",
    "\t'Ophthamological',\n",
    "\t'Connective tissue',\n",
    "\t'Endocrine',\n",
    "\t'Skeletal',\n",
    "\t'Metabolic',\n",
    "\t'Cardiovascular',\n",
    "\t'Dermatological',\n",
    "\t'Renal',\n",
    "\t'Hematological',\n",
    "\t'Immunological',\n",
    "\t'Muscular',\n",
    "\t'Developmental'\n",
    "]\n",
    "\n",
    "\n",
    "def get_negative_disease_class_data(pos_class, n):\n",
    "    # n = n // 2\n",
    "    return disease_class_training_data[disease_class_training_data['disease_class'] != pos_class].sample(n=n, random_state=42)\n",
    "\n",
    "def get_positive_disease_class_data(pos_class):\n",
    "    return disease_class_training_data[disease_class_training_data['disease_class'] == pos_class].copy()\n",
    "\n",
    "def get_disease_class_training_data(pos_class):\n",
    "    pos = get_positive_disease_class_data(pos_class)\n",
    "    pos['label'] = 1\n",
    "    neg = get_negative_disease_class_data(pos_class, len(pos))\n",
    "    neg['label'] = 0\n",
    "    data = pd.concat([pos, neg], ignore_index=True)\n",
    "    x = data.iloc[:,3:].values\n",
    "    y = data.iloc[:,4:5].values.ravel()\n",
    "    \n",
    "    return x, torch.tensor(y), data\n",
    "disease_class_training_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TheModel(\n",
    "    gene_feature_dim=gene_net_data.x.shape[1],\n",
    "    disease_feature_dim=disease_net_data.x.shape[1],\n",
    "    fc_hidden_dim=3000,\n",
    "    gene_net_hidden_dim=830,\n",
    "    disease_net_hidden_dim=500\n",
    ").to(device)\n",
    "\n",
    "def train_disease_classification(model_parameter_file):\n",
    "    # Load the pretrained model.\n",
    "    model.load_state_dict(torch.load(model_parameter_file))\n",
    "\n",
    "    # Set classification training hyperparameters.\n",
    "    lr_classification=0.00000347821\n",
    "    weight_decay_classification=0.5165618\n",
    "    folds=5\n",
    "    max_epochs=100\n",
    "    info_each_epoch = 1\n",
    "    early_stopping_window=15\n",
    "    final_disease_class_metrics = dict()\n",
    "    losses = {\n",
    "                    'train': [],\n",
    "                    'val': [],\n",
    "                    'AUC': 0,\n",
    "                    'TPR': None,\n",
    "                    'FPR': None\n",
    "            }\n",
    "    for disease_class in disease_class_target_classes:\n",
    "        for fold in range(folds):\n",
    "            losses[f'train_disease_class_{disease_class}_{fold}'] = []\n",
    "            losses[f'val_disease_class_{disease_class}_{fold}'] = []\n",
    "            final_disease_class_metrics[f'{disease_class}_{fold}'] = {\n",
    "                'roc_auc': 0,\n",
    "                'pr_auc': 0,\n",
    "                'fmax': 0\n",
    "            }\n",
    "\n",
    "\n",
    "    torch.save(model.state_dict(), '/var/tmp/dg_tmp/tmp_model_state.ptm')\n",
    "    class_count = 0\n",
    "    for disease_class in disease_class_target_classes:\n",
    "        class_count += 1\n",
    "        print(f'Evaluate pretrained model on disease class {disease_class} ({class_count}/{len(disease_class_target_classes)})')\n",
    "        x_disease_class, y_disease_class, _ = get_disease_class_training_data(disease_class)\n",
    "        optimizer_disease_class = torch.optim.Adam(model.parameters(), lr=lr_classification, weight_decay=weight_decay_classification)\n",
    "        criterion_disease_class = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "        kf = KFold(n_splits=folds, shuffle=True, random_state=42)\n",
    "        fold = -1\n",
    "        for train_fold_index, test_fold_index in kf.split(x_disease_class):\n",
    "            fold += 1\n",
    "            print(f'Starting Fold: {fold}')\n",
    "            model.load_state_dict(torch.load('/var/tmp/dg_tmp/tmp_model_state.ptm'))\n",
    "            model.mode = 'Classify'\n",
    "            # Split into train and validation.\n",
    "            x_test = x_disease_class[test_fold_index]\n",
    "            y_test = y_disease_class[test_fold_index].to(device)\n",
    "            id_tr, id_val = train_test_split(range(x_disease_class[train_fold_index].shape[0]), test_size=0.1, random_state=42)\n",
    "            x_train = x_disease_class[train_fold_index][id_tr]\n",
    "            y_train = y_disease_class[train_fold_index][id_tr].to(device)\n",
    "            x_val = x_disease_class[train_fold_index][id_val]\n",
    "            y_val = y_disease_class[train_fold_index][id_val].to(device)\n",
    "\n",
    "            # print(x_test.shape, x_train.shape, x_val.shape)\n",
    "            # print(y_test.shape, y_train.shape, y_val.shape)\n",
    "\n",
    "            for epoch in range(max_epochs):\n",
    "                model.train()\n",
    "\n",
    "                batch_size = 16\n",
    "                permutation = torch.randperm(x_train.shape[0])\n",
    "                # train\n",
    "                loss_items = []\n",
    "                for i in range(0, x_train.shape[0], batch_size):\n",
    "                        # print(f'doing batch {i//batch_size}/{x_train.shape[0]//batch_size}')\n",
    "                        batch_indices = permutation[i:i+batch_size]\n",
    "                        batch_x, batch_y = x_train[batch_indices].reshape(-1, 2), y_train[batch_indices]\n",
    "\n",
    "                        optimizer_disease_class.zero_grad()\n",
    "                        out = model(gene_net_data, disease_net_data, batch_x)\n",
    "                        loss = criterion_disease_class(out, batch_y)\n",
    "                        loss.backward()\n",
    "                        optimizer_disease_class.step()\n",
    "                        loss_items.append(loss.item())\n",
    "                losses[f'train_disease_class_{disease_class}_{fold}'].append(np.mean(loss_items))\n",
    "\n",
    "                # validation\n",
    "                with torch.no_grad():\n",
    "                    model.eval()\n",
    "                    out = model(gene_net_data, disease_net_data, x_val)\n",
    "                    loss = criterion_disease_class(out, y_val)\n",
    "                    losses[f'val_disease_class_{disease_class}_{fold}'].append(loss.item())\n",
    "\n",
    "                    if epoch % info_each_epoch == 0:\n",
    "                        print(\n",
    "                            'Epoch {}, train_loss: {:.4f}, val_loss: {:.4f}'.format(\n",
    "                                epoch, losses[f'train_disease_class_{disease_class}_{fold}'][epoch], losses[f'val_disease_class_{disease_class}_{fold}'][epoch]\n",
    "                            )\n",
    "                        )\n",
    "\n",
    "                # Early stopping\n",
    "                if epoch > early_stopping_window:\n",
    "                    # Stop if validation error did not decrease \n",
    "                    # w.r.t. the past early_stopping_window consecutive epochs.\n",
    "                    last_window_losses = losses[f'val_disease_class_{disease_class}_{fold}'][epoch - early_stopping_window:epoch]\n",
    "                    if losses[f'val_disease_class_{disease_class}_{fold}'][-1] > max(last_window_losses):\n",
    "                        print('Early Stopping!')\n",
    "                        break\n",
    "\n",
    "            # Test the disease classification model for current fold.\n",
    "            print(f'Test the model on fold {fold}:')\n",
    "            with torch.no_grad():\n",
    "                y_score = model(gene_net_data, disease_net_data, x_test)[:,1].cpu().detach().numpy()\n",
    "                y = y_test.cpu().detach().numpy()\n",
    "                final_disease_class_metrics[f'{disease_class}_{fold}']['roc_auc'] = sklearn.metrics.roc_auc_score(y, y_score)\n",
    "                precision, recall, thresholds = sklearn.metrics.precision_recall_curve(y, y_score)\n",
    "                final_disease_class_metrics[f'{disease_class}_{fold}']['pr_auc'] = sklearn.metrics.auc(recall, precision)\n",
    "                final_disease_class_metrics[f'{disease_class}_{fold}']['fmax'] = ((2*precision * recall) / (precision + recall + 0.00001)).max()\n",
    "                print(final_disease_class_metrics[f'{disease_class}_{fold}'])\n",
    "    return final_disease_class_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the classificatrion model.\n",
    "* Run the training for each pretrained model.\n",
    "* Store results in `disease_classification_results.gz`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############################################################\n",
      "# START CLASSIFICATION USING PRETRAINED MODEL FROM FOLD: 1 #\n",
      "############################################################\n",
      "Evaluate pretrained model on disease class Ophthamological (1/12)\n",
      "Starting Fold: 0\n",
      "Epoch 0, train_loss: 0.6949, val_loss: 0.6956\n",
      "Epoch 1, train_loss: 0.6803, val_loss: 0.6937\n",
      "Epoch 2, train_loss: 0.6741, val_loss: 0.6877\n",
      "Epoch 3, train_loss: 0.6623, val_loss: 0.6840\n",
      "Epoch 4, train_loss: 0.6527, val_loss: 0.6817\n",
      "Epoch 5, train_loss: 0.6518, val_loss: 0.6765\n",
      "Epoch 6, train_loss: 0.6384, val_loss: 0.6734\n",
      "Epoch 7, train_loss: 0.6347, val_loss: 0.6693\n",
      "Epoch 8, train_loss: 0.6261, val_loss: 0.6625\n",
      "Epoch 9, train_loss: 0.6247, val_loss: 0.6598\n",
      "Epoch 10, train_loss: 0.6184, val_loss: 0.6582\n",
      "Epoch 11, train_loss: 0.6069, val_loss: 0.6544\n",
      "Epoch 12, train_loss: 0.6068, val_loss: 0.6512\n",
      "Epoch 13, train_loss: 0.5905, val_loss: 0.6459\n",
      "Epoch 14, train_loss: 0.5921, val_loss: 0.6417\n",
      "Epoch 15, train_loss: 0.5803, val_loss: 0.6376\n",
      "Epoch 16, train_loss: 0.5750, val_loss: 0.6337\n",
      "Epoch 17, train_loss: 0.5735, val_loss: 0.6267\n",
      "Epoch 18, train_loss: 0.5612, val_loss: 0.6238\n",
      "Epoch 19, train_loss: 0.5475, val_loss: 0.6187\n",
      "Epoch 20, train_loss: 0.5451, val_loss: 0.6167\n",
      "Epoch 21, train_loss: 0.5374, val_loss: 0.6109\n",
      "Epoch 22, train_loss: 0.5277, val_loss: 0.6077\n",
      "Epoch 23, train_loss: 0.5232, val_loss: 0.6034\n",
      "Epoch 24, train_loss: 0.5124, val_loss: 0.5948\n",
      "Epoch 25, train_loss: 0.5087, val_loss: 0.5912\n",
      "Epoch 26, train_loss: 0.4968, val_loss: 0.5858\n",
      "Epoch 27, train_loss: 0.4936, val_loss: 0.5815\n",
      "Epoch 28, train_loss: 0.4746, val_loss: 0.5751\n",
      "Epoch 29, train_loss: 0.4623, val_loss: 0.5724\n",
      "Epoch 30, train_loss: 0.4643, val_loss: 0.5689\n",
      "Epoch 31, train_loss: 0.4495, val_loss: 0.5636\n",
      "Epoch 32, train_loss: 0.4452, val_loss: 0.5572\n",
      "Epoch 33, train_loss: 0.4361, val_loss: 0.5565\n",
      "Epoch 34, train_loss: 0.4367, val_loss: 0.5508\n",
      "Epoch 35, train_loss: 0.4211, val_loss: 0.5452\n",
      "Epoch 36, train_loss: 0.4046, val_loss: 0.5429\n",
      "Epoch 37, train_loss: 0.3993, val_loss: 0.5374\n",
      "Epoch 38, train_loss: 0.3902, val_loss: 0.5345\n",
      "Epoch 39, train_loss: 0.3813, val_loss: 0.5277\n",
      "Epoch 40, train_loss: 0.3729, val_loss: 0.5227\n",
      "Epoch 41, train_loss: 0.3706, val_loss: 0.5210\n",
      "Epoch 42, train_loss: 0.3682, val_loss: 0.5166\n",
      "Epoch 43, train_loss: 0.3539, val_loss: 0.5159\n",
      "Epoch 44, train_loss: 0.3520, val_loss: 0.5159\n",
      "Epoch 45, train_loss: 0.3416, val_loss: 0.5086\n",
      "Epoch 46, train_loss: 0.3235, val_loss: 0.5048\n",
      "Epoch 47, train_loss: 0.3199, val_loss: 0.5041\n",
      "Epoch 48, train_loss: 0.3189, val_loss: 0.5008\n",
      "Epoch 49, train_loss: 0.3127, val_loss: 0.4964\n",
      "Epoch 50, train_loss: 0.3096, val_loss: 0.4927\n",
      "Epoch 51, train_loss: 0.3078, val_loss: 0.4883\n",
      "Epoch 52, train_loss: 0.2899, val_loss: 0.4867\n",
      "Epoch 53, train_loss: 0.2854, val_loss: 0.4849\n",
      "Epoch 54, train_loss: 0.2798, val_loss: 0.4832\n",
      "Epoch 55, train_loss: 0.2729, val_loss: 0.4837\n",
      "Epoch 56, train_loss: 0.2663, val_loss: 0.4804\n",
      "Epoch 57, train_loss: 0.2646, val_loss: 0.4789\n",
      "Epoch 58, train_loss: 0.2608, val_loss: 0.4772\n",
      "Epoch 59, train_loss: 0.2478, val_loss: 0.4713\n",
      "Epoch 60, train_loss: 0.2433, val_loss: 0.4720\n",
      "Epoch 61, train_loss: 0.2472, val_loss: 0.4695\n",
      "Epoch 62, train_loss: 0.2449, val_loss: 0.4728\n",
      "Epoch 63, train_loss: 0.2304, val_loss: 0.4690\n",
      "Epoch 64, train_loss: 0.2351, val_loss: 0.4683\n",
      "Epoch 65, train_loss: 0.2295, val_loss: 0.4661\n",
      "Epoch 66, train_loss: 0.2242, val_loss: 0.4677\n",
      "Epoch 67, train_loss: 0.2300, val_loss: 0.4642\n",
      "Epoch 68, train_loss: 0.2251, val_loss: 0.4621\n",
      "Epoch 69, train_loss: 0.2199, val_loss: 0.4600\n",
      "Epoch 70, train_loss: 0.2166, val_loss: 0.4593\n",
      "Epoch 71, train_loss: 0.2083, val_loss: 0.4584\n",
      "Epoch 72, train_loss: 0.2006, val_loss: 0.4582\n",
      "Epoch 73, train_loss: 0.2025, val_loss: 0.4546\n",
      "Epoch 74, train_loss: 0.2056, val_loss: 0.4576\n",
      "Epoch 75, train_loss: 0.1921, val_loss: 0.4558\n",
      "Epoch 76, train_loss: 0.1928, val_loss: 0.4564\n",
      "Epoch 77, train_loss: 0.2007, val_loss: 0.4556\n",
      "Epoch 78, train_loss: 0.1949, val_loss: 0.4550\n",
      "Epoch 79, train_loss: 0.1922, val_loss: 0.4571\n",
      "Epoch 80, train_loss: 0.1942, val_loss: 0.4523\n",
      "Epoch 81, train_loss: 0.1874, val_loss: 0.4538\n",
      "Epoch 82, train_loss: 0.1808, val_loss: 0.4528\n",
      "Epoch 83, train_loss: 0.1798, val_loss: 0.4530\n",
      "Epoch 84, train_loss: 0.1795, val_loss: 0.4503\n",
      "Epoch 85, train_loss: 0.1790, val_loss: 0.4537\n",
      "Epoch 86, train_loss: 0.1715, val_loss: 0.4545\n",
      "Epoch 87, train_loss: 0.1701, val_loss: 0.4515\n",
      "Epoch 88, train_loss: 0.1693, val_loss: 0.4498\n",
      "Epoch 89, train_loss: 0.1650, val_loss: 0.4555\n",
      "Epoch 90, train_loss: 0.1702, val_loss: 0.4519\n",
      "Epoch 91, train_loss: 0.1619, val_loss: 0.4508\n",
      "Epoch 92, train_loss: 0.1625, val_loss: 0.4531\n",
      "Epoch 93, train_loss: 0.1695, val_loss: 0.4511\n",
      "Epoch 94, train_loss: 0.1597, val_loss: 0.4486\n",
      "Epoch 95, train_loss: 0.1585, val_loss: 0.4499\n",
      "Epoch 96, train_loss: 0.1553, val_loss: 0.4523\n",
      "Epoch 97, train_loss: 0.1609, val_loss: 0.4507\n",
      "Epoch 98, train_loss: 0.1576, val_loss: 0.4502\n",
      "Epoch 99, train_loss: 0.1532, val_loss: 0.4496\n",
      "Test the model on fold 0:\n",
      "{'roc_auc': 0.8940972222222222, 'pr_auc': 0.9323597666854869, 'fmax': 0.8571379592116616}\n",
      "Starting Fold: 1\n",
      "Epoch 0, train_loss: 0.6898, val_loss: 0.6833\n",
      "Epoch 1, train_loss: 0.6740, val_loss: 0.6684\n",
      "Epoch 2, train_loss: 0.6633, val_loss: 0.6609\n",
      "Epoch 3, train_loss: 0.6517, val_loss: 0.6543\n",
      "Epoch 4, train_loss: 0.6408, val_loss: 0.6483\n",
      "Epoch 5, train_loss: 0.6351, val_loss: 0.6449\n",
      "Epoch 6, train_loss: 0.6255, val_loss: 0.6416\n",
      "Epoch 7, train_loss: 0.6211, val_loss: 0.6387\n",
      "Epoch 8, train_loss: 0.6143, val_loss: 0.6360\n",
      "Epoch 9, train_loss: 0.6069, val_loss: 0.6326\n",
      "Epoch 10, train_loss: 0.5995, val_loss: 0.6301\n",
      "Epoch 11, train_loss: 0.5925, val_loss: 0.6280\n",
      "Epoch 12, train_loss: 0.5883, val_loss: 0.6226\n",
      "Epoch 13, train_loss: 0.5788, val_loss: 0.6209\n",
      "Epoch 14, train_loss: 0.5761, val_loss: 0.6172\n",
      "Epoch 15, train_loss: 0.5656, val_loss: 0.6137\n",
      "Epoch 16, train_loss: 0.5667, val_loss: 0.6113\n",
      "Epoch 17, train_loss: 0.5554, val_loss: 0.6071\n",
      "Epoch 18, train_loss: 0.5479, val_loss: 0.6033\n",
      "Epoch 19, train_loss: 0.5391, val_loss: 0.6008\n",
      "Epoch 20, train_loss: 0.5294, val_loss: 0.5972\n",
      "Epoch 21, train_loss: 0.5230, val_loss: 0.5937\n",
      "Epoch 22, train_loss: 0.5194, val_loss: 0.5898\n",
      "Epoch 23, train_loss: 0.5098, val_loss: 0.5853\n",
      "Epoch 24, train_loss: 0.4938, val_loss: 0.5820\n",
      "Epoch 25, train_loss: 0.5000, val_loss: 0.5770\n",
      "Epoch 26, train_loss: 0.4780, val_loss: 0.5749\n",
      "Epoch 27, train_loss: 0.4742, val_loss: 0.5710\n",
      "Epoch 28, train_loss: 0.4737, val_loss: 0.5691\n",
      "Epoch 29, train_loss: 0.4545, val_loss: 0.5643\n",
      "Epoch 30, train_loss: 0.4477, val_loss: 0.5608\n",
      "Epoch 31, train_loss: 0.4426, val_loss: 0.5564\n",
      "Epoch 32, train_loss: 0.4258, val_loss: 0.5542\n",
      "Epoch 33, train_loss: 0.4271, val_loss: 0.5515\n",
      "Epoch 34, train_loss: 0.4102, val_loss: 0.5490\n",
      "Epoch 35, train_loss: 0.4041, val_loss: 0.5455\n",
      "Epoch 36, train_loss: 0.4016, val_loss: 0.5434\n",
      "Epoch 37, train_loss: 0.3937, val_loss: 0.5409\n",
      "Epoch 38, train_loss: 0.3722, val_loss: 0.5367\n",
      "Epoch 39, train_loss: 0.3832, val_loss: 0.5354\n",
      "Epoch 40, train_loss: 0.3648, val_loss: 0.5342\n",
      "Epoch 41, train_loss: 0.3581, val_loss: 0.5301\n",
      "Epoch 42, train_loss: 0.3520, val_loss: 0.5288\n",
      "Epoch 43, train_loss: 0.3403, val_loss: 0.5271\n",
      "Epoch 44, train_loss: 0.3495, val_loss: 0.5249\n",
      "Epoch 45, train_loss: 0.3394, val_loss: 0.5230\n",
      "Epoch 46, train_loss: 0.3222, val_loss: 0.5196\n",
      "Epoch 47, train_loss: 0.3193, val_loss: 0.5187\n",
      "Epoch 48, train_loss: 0.3072, val_loss: 0.5190\n",
      "Epoch 49, train_loss: 0.3048, val_loss: 0.5168\n",
      "Epoch 50, train_loss: 0.2943, val_loss: 0.5153\n",
      "Epoch 51, train_loss: 0.2848, val_loss: 0.5139\n",
      "Epoch 52, train_loss: 0.2886, val_loss: 0.5095\n",
      "Epoch 53, train_loss: 0.2931, val_loss: 0.5118\n",
      "Epoch 54, train_loss: 0.2765, val_loss: 0.5106\n",
      "Epoch 55, train_loss: 0.2647, val_loss: 0.5101\n",
      "Epoch 56, train_loss: 0.2612, val_loss: 0.5121\n",
      "Epoch 57, train_loss: 0.2904, val_loss: 0.5109\n",
      "Epoch 58, train_loss: 0.2641, val_loss: 0.5113\n",
      "Epoch 59, train_loss: 0.2591, val_loss: 0.5120\n",
      "Epoch 60, train_loss: 0.2549, val_loss: 0.5117\n",
      "Epoch 61, train_loss: 0.2395, val_loss: 0.5093\n",
      "Epoch 62, train_loss: 0.2426, val_loss: 0.5072\n",
      "Epoch 63, train_loss: 0.2369, val_loss: 0.5077\n",
      "Epoch 64, train_loss: 0.2324, val_loss: 0.5064\n",
      "Epoch 65, train_loss: 0.2283, val_loss: 0.5051\n",
      "Epoch 66, train_loss: 0.2278, val_loss: 0.5030\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 67, train_loss: 0.2193, val_loss: 0.5061\n",
      "Epoch 68, train_loss: 0.2159, val_loss: 0.5066\n",
      "Epoch 69, train_loss: 0.2105, val_loss: 0.5069\n",
      "Epoch 70, train_loss: 0.2139, val_loss: 0.5042\n",
      "Epoch 71, train_loss: 0.2151, val_loss: 0.5051\n",
      "Epoch 72, train_loss: 0.2085, val_loss: 0.5054\n",
      "Epoch 73, train_loss: 0.2045, val_loss: 0.5026\n",
      "Epoch 74, train_loss: 0.2019, val_loss: 0.5018\n",
      "Epoch 75, train_loss: 0.1895, val_loss: 0.5011\n",
      "Epoch 76, train_loss: 0.1874, val_loss: 0.4993\n",
      "Epoch 77, train_loss: 0.1901, val_loss: 0.5009\n",
      "Epoch 78, train_loss: 0.1881, val_loss: 0.5035\n",
      "Epoch 79, train_loss: 0.1831, val_loss: 0.5033\n",
      "Epoch 80, train_loss: 0.1898, val_loss: 0.5019\n",
      "Epoch 81, train_loss: 0.1792, val_loss: 0.4986\n",
      "Epoch 82, train_loss: 0.1879, val_loss: 0.5006\n",
      "Epoch 83, train_loss: 0.1811, val_loss: 0.5015\n",
      "Epoch 84, train_loss: 0.1737, val_loss: 0.5020\n",
      "Epoch 85, train_loss: 0.1770, val_loss: 0.5018\n",
      "Epoch 86, train_loss: 0.1728, val_loss: 0.4987\n",
      "Epoch 87, train_loss: 0.1699, val_loss: 0.5007\n",
      "Epoch 88, train_loss: 0.1897, val_loss: 0.5050\n",
      "Early Stopping!\n",
      "Test the model on fold 1:\n",
      "{'roc_auc': 0.8787878787878788, 'pr_auc': 0.9171399855391591, 'fmax': 0.8450654235566494}\n",
      "Starting Fold: 2\n",
      "Epoch 0, train_loss: 0.6908, val_loss: 0.6872\n",
      "Epoch 1, train_loss: 0.6734, val_loss: 0.6819\n",
      "Epoch 2, train_loss: 0.6626, val_loss: 0.6796\n",
      "Epoch 3, train_loss: 0.6495, val_loss: 0.6776\n",
      "Epoch 4, train_loss: 0.6392, val_loss: 0.6760\n",
      "Epoch 5, train_loss: 0.6314, val_loss: 0.6738\n",
      "Epoch 6, train_loss: 0.6179, val_loss: 0.6719\n",
      "Epoch 7, train_loss: 0.6196, val_loss: 0.6696\n",
      "Epoch 8, train_loss: 0.6079, val_loss: 0.6667\n",
      "Epoch 9, train_loss: 0.6040, val_loss: 0.6644\n",
      "Epoch 10, train_loss: 0.5929, val_loss: 0.6619\n",
      "Epoch 11, train_loss: 0.5819, val_loss: 0.6587\n",
      "Epoch 12, train_loss: 0.5824, val_loss: 0.6563\n",
      "Epoch 13, train_loss: 0.5697, val_loss: 0.6533\n",
      "Epoch 14, train_loss: 0.5604, val_loss: 0.6512\n",
      "Epoch 15, train_loss: 0.5530, val_loss: 0.6484\n",
      "Epoch 16, train_loss: 0.5517, val_loss: 0.6467\n",
      "Epoch 17, train_loss: 0.5422, val_loss: 0.6448\n",
      "Epoch 18, train_loss: 0.5337, val_loss: 0.6419\n",
      "Epoch 19, train_loss: 0.5287, val_loss: 0.6387\n",
      "Epoch 20, train_loss: 0.5196, val_loss: 0.6365\n",
      "Epoch 21, train_loss: 0.5040, val_loss: 0.6319\n",
      "Epoch 22, train_loss: 0.4943, val_loss: 0.6298\n",
      "Epoch 23, train_loss: 0.4885, val_loss: 0.6266\n",
      "Epoch 24, train_loss: 0.4862, val_loss: 0.6228\n",
      "Epoch 25, train_loss: 0.4665, val_loss: 0.6193\n",
      "Epoch 26, train_loss: 0.4652, val_loss: 0.6159\n",
      "Epoch 27, train_loss: 0.4594, val_loss: 0.6150\n",
      "Epoch 28, train_loss: 0.4462, val_loss: 0.6130\n",
      "Epoch 29, train_loss: 0.4329, val_loss: 0.6106\n",
      "Epoch 30, train_loss: 0.4228, val_loss: 0.6050\n",
      "Epoch 31, train_loss: 0.4218, val_loss: 0.6017\n",
      "Epoch 32, train_loss: 0.4107, val_loss: 0.5977\n",
      "Epoch 33, train_loss: 0.4042, val_loss: 0.5958\n",
      "Epoch 34, train_loss: 0.3871, val_loss: 0.5914\n",
      "Epoch 35, train_loss: 0.3812, val_loss: 0.5886\n",
      "Epoch 36, train_loss: 0.3772, val_loss: 0.5912\n",
      "Epoch 37, train_loss: 0.3719, val_loss: 0.5880\n",
      "Epoch 38, train_loss: 0.3621, val_loss: 0.5822\n",
      "Epoch 39, train_loss: 0.3582, val_loss: 0.5803\n",
      "Epoch 40, train_loss: 0.3408, val_loss: 0.5759\n",
      "Epoch 41, train_loss: 0.3314, val_loss: 0.5746\n",
      "Epoch 42, train_loss: 0.3269, val_loss: 0.5695\n",
      "Epoch 43, train_loss: 0.3231, val_loss: 0.5674\n",
      "Epoch 44, train_loss: 0.3056, val_loss: 0.5672\n",
      "Epoch 45, train_loss: 0.3080, val_loss: 0.5662\n",
      "Epoch 46, train_loss: 0.3093, val_loss: 0.5635\n",
      "Epoch 47, train_loss: 0.2904, val_loss: 0.5613\n",
      "Epoch 48, train_loss: 0.2873, val_loss: 0.5601\n",
      "Epoch 49, train_loss: 0.2743, val_loss: 0.5592\n",
      "Epoch 50, train_loss: 0.2698, val_loss: 0.5561\n",
      "Epoch 51, train_loss: 0.2661, val_loss: 0.5540\n",
      "Epoch 52, train_loss: 0.2657, val_loss: 0.5543\n",
      "Epoch 53, train_loss: 0.2617, val_loss: 0.5508\n",
      "Epoch 54, train_loss: 0.2496, val_loss: 0.5469\n",
      "Epoch 55, train_loss: 0.2529, val_loss: 0.5456\n",
      "Epoch 56, train_loss: 0.2356, val_loss: 0.5457\n",
      "Epoch 57, train_loss: 0.2418, val_loss: 0.5439\n",
      "Epoch 58, train_loss: 0.2360, val_loss: 0.5439\n",
      "Epoch 59, train_loss: 0.2346, val_loss: 0.5436\n",
      "Epoch 60, train_loss: 0.2198, val_loss: 0.5409\n",
      "Epoch 61, train_loss: 0.2183, val_loss: 0.5400\n",
      "Epoch 62, train_loss: 0.2148, val_loss: 0.5396\n",
      "Epoch 63, train_loss: 0.2149, val_loss: 0.5394\n",
      "Epoch 64, train_loss: 0.2192, val_loss: 0.5380\n",
      "Epoch 65, train_loss: 0.2231, val_loss: 0.5356\n",
      "Epoch 66, train_loss: 0.2131, val_loss: 0.5349\n",
      "Epoch 67, train_loss: 0.1967, val_loss: 0.5367\n",
      "Epoch 68, train_loss: 0.1946, val_loss: 0.5346\n",
      "Epoch 69, train_loss: 0.1942, val_loss: 0.5343\n",
      "Epoch 70, train_loss: 0.1891, val_loss: 0.5338\n",
      "Epoch 71, train_loss: 0.1938, val_loss: 0.5334\n",
      "Epoch 72, train_loss: 0.1944, val_loss: 0.5305\n",
      "Epoch 73, train_loss: 0.1900, val_loss: 0.5304\n",
      "Epoch 74, train_loss: 0.1916, val_loss: 0.5282\n",
      "Epoch 75, train_loss: 0.1810, val_loss: 0.5298\n",
      "Epoch 76, train_loss: 0.1805, val_loss: 0.5276\n",
      "Epoch 77, train_loss: 0.1768, val_loss: 0.5278\n",
      "Epoch 78, train_loss: 0.1873, val_loss: 0.5278\n",
      "Epoch 79, train_loss: 0.1759, val_loss: 0.5269\n",
      "Epoch 80, train_loss: 0.1756, val_loss: 0.5264\n",
      "Epoch 81, train_loss: 0.1713, val_loss: 0.5283\n",
      "Epoch 82, train_loss: 0.1655, val_loss: 0.5267\n",
      "Epoch 83, train_loss: 0.1699, val_loss: 0.5257\n",
      "Epoch 84, train_loss: 0.1605, val_loss: 0.5274\n",
      "Epoch 85, train_loss: 0.1705, val_loss: 0.5269\n",
      "Epoch 86, train_loss: 0.1605, val_loss: 0.5258\n",
      "Epoch 87, train_loss: 0.1573, val_loss: 0.5262\n",
      "Epoch 88, train_loss: 0.1689, val_loss: 0.5291\n",
      "Epoch 89, train_loss: 0.1714, val_loss: 0.5270\n",
      "Epoch 90, train_loss: 0.1769, val_loss: 0.5217\n",
      "Epoch 91, train_loss: 0.1599, val_loss: 0.5207\n",
      "Epoch 92, train_loss: 0.1658, val_loss: 0.5227\n",
      "Epoch 93, train_loss: 0.1531, val_loss: 0.5207\n",
      "Epoch 94, train_loss: 0.1537, val_loss: 0.5221\n",
      "Epoch 95, train_loss: 0.1550, val_loss: 0.5252\n",
      "Epoch 96, train_loss: 0.1570, val_loss: 0.5221\n",
      "Epoch 97, train_loss: 0.1627, val_loss: 0.5239\n",
      "Epoch 98, train_loss: 0.1533, val_loss: 0.5257\n",
      "Epoch 99, train_loss: 0.1573, val_loss: 0.5239\n",
      "Test the model on fold 2:\n",
      "{'roc_auc': 0.8628472222222223, 'pr_auc': 0.8747652994492665, 'fmax': 0.7868802580267797}\n",
      "Starting Fold: 3\n",
      "Epoch 0, train_loss: 0.6953, val_loss: 0.6843\n",
      "Epoch 1, train_loss: 0.6647, val_loss: 0.6748\n",
      "Epoch 2, train_loss: 0.6557, val_loss: 0.6714\n",
      "Epoch 3, train_loss: 0.6453, val_loss: 0.6659\n",
      "Epoch 4, train_loss: 0.6325, val_loss: 0.6614\n",
      "Epoch 5, train_loss: 0.6224, val_loss: 0.6585\n",
      "Epoch 6, train_loss: 0.6180, val_loss: 0.6553\n",
      "Epoch 7, train_loss: 0.6020, val_loss: 0.6524\n",
      "Epoch 8, train_loss: 0.5925, val_loss: 0.6504\n",
      "Epoch 9, train_loss: 0.5901, val_loss: 0.6477\n",
      "Epoch 10, train_loss: 0.5770, val_loss: 0.6441\n",
      "Epoch 11, train_loss: 0.5696, val_loss: 0.6405\n",
      "Epoch 12, train_loss: 0.5642, val_loss: 0.6366\n",
      "Epoch 13, train_loss: 0.5550, val_loss: 0.6336\n",
      "Epoch 14, train_loss: 0.5435, val_loss: 0.6304\n",
      "Epoch 15, train_loss: 0.5280, val_loss: 0.6264\n",
      "Epoch 16, train_loss: 0.5247, val_loss: 0.6214\n",
      "Epoch 17, train_loss: 0.5063, val_loss: 0.6200\n",
      "Epoch 18, train_loss: 0.4926, val_loss: 0.6172\n",
      "Epoch 19, train_loss: 0.4970, val_loss: 0.6131\n",
      "Epoch 20, train_loss: 0.4843, val_loss: 0.6125\n",
      "Epoch 21, train_loss: 0.4723, val_loss: 0.6109\n",
      "Epoch 22, train_loss: 0.4520, val_loss: 0.6083\n",
      "Epoch 23, train_loss: 0.4588, val_loss: 0.6060\n",
      "Epoch 24, train_loss: 0.4405, val_loss: 0.6030\n",
      "Epoch 25, train_loss: 0.4367, val_loss: 0.6016\n",
      "Epoch 26, train_loss: 0.4175, val_loss: 0.6004\n",
      "Epoch 27, train_loss: 0.4100, val_loss: 0.5995\n",
      "Epoch 28, train_loss: 0.4027, val_loss: 0.5961\n",
      "Epoch 29, train_loss: 0.3915, val_loss: 0.5948\n",
      "Epoch 30, train_loss: 0.3889, val_loss: 0.5910\n",
      "Epoch 31, train_loss: 0.3828, val_loss: 0.5902\n",
      "Epoch 32, train_loss: 0.3712, val_loss: 0.5900\n",
      "Epoch 33, train_loss: 0.3707, val_loss: 0.5885\n",
      "Epoch 34, train_loss: 0.3509, val_loss: 0.5851\n",
      "Epoch 35, train_loss: 0.3487, val_loss: 0.5831\n",
      "Epoch 36, train_loss: 0.3407, val_loss: 0.5804\n",
      "Epoch 37, train_loss: 0.3180, val_loss: 0.5797\n",
      "Epoch 38, train_loss: 0.3222, val_loss: 0.5815\n",
      "Epoch 39, train_loss: 0.3080, val_loss: 0.5819\n",
      "Epoch 40, train_loss: 0.3012, val_loss: 0.5802\n",
      "Epoch 41, train_loss: 0.2945, val_loss: 0.5789\n",
      "Epoch 42, train_loss: 0.2780, val_loss: 0.5797\n",
      "Epoch 43, train_loss: 0.2879, val_loss: 0.5792\n",
      "Epoch 44, train_loss: 0.2685, val_loss: 0.5791\n",
      "Epoch 45, train_loss: 0.2803, val_loss: 0.5792\n",
      "Epoch 46, train_loss: 0.2609, val_loss: 0.5789\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47, train_loss: 0.2552, val_loss: 0.5770\n",
      "Epoch 48, train_loss: 0.2506, val_loss: 0.5773\n",
      "Epoch 49, train_loss: 0.2507, val_loss: 0.5784\n",
      "Epoch 50, train_loss: 0.2401, val_loss: 0.5779\n",
      "Epoch 51, train_loss: 0.2423, val_loss: 0.5768\n",
      "Epoch 52, train_loss: 0.2345, val_loss: 0.5769\n",
      "Epoch 53, train_loss: 0.2346, val_loss: 0.5748\n",
      "Epoch 54, train_loss: 0.2274, val_loss: 0.5760\n",
      "Epoch 55, train_loss: 0.2340, val_loss: 0.5747\n",
      "Epoch 56, train_loss: 0.2164, val_loss: 0.5752\n",
      "Epoch 57, train_loss: 0.2281, val_loss: 0.5757\n",
      "Epoch 58, train_loss: 0.2071, val_loss: 0.5743\n",
      "Epoch 59, train_loss: 0.2064, val_loss: 0.5733\n",
      "Epoch 60, train_loss: 0.2088, val_loss: 0.5743\n",
      "Epoch 61, train_loss: 0.2025, val_loss: 0.5758\n",
      "Epoch 62, train_loss: 0.2014, val_loss: 0.5780\n",
      "Epoch 63, train_loss: 0.1991, val_loss: 0.5762\n",
      "Epoch 64, train_loss: 0.1943, val_loss: 0.5759\n",
      "Epoch 65, train_loss: 0.1970, val_loss: 0.5738\n",
      "Epoch 66, train_loss: 0.1882, val_loss: 0.5750\n",
      "Epoch 67, train_loss: 0.1839, val_loss: 0.5755\n",
      "Epoch 68, train_loss: 0.1776, val_loss: 0.5736\n",
      "Epoch 69, train_loss: 0.1780, val_loss: 0.5754\n",
      "Epoch 70, train_loss: 0.1769, val_loss: 0.5762\n",
      "Epoch 71, train_loss: 0.1772, val_loss: 0.5747\n",
      "Epoch 72, train_loss: 0.1758, val_loss: 0.5753\n",
      "Epoch 73, train_loss: 0.1848, val_loss: 0.5768\n",
      "Epoch 74, train_loss: 0.1744, val_loss: 0.5757\n",
      "Epoch 75, train_loss: 0.1776, val_loss: 0.5787\n",
      "Early Stopping!\n",
      "Test the model on fold 3:\n",
      "{'roc_auc': 0.8763020833333333, 'pr_auc': 0.882896733926344, 'fmax': 0.8311638522814693}\n",
      "Starting Fold: 4\n",
      "Epoch 0, train_loss: 0.6929, val_loss: 0.6875\n",
      "Epoch 1, train_loss: 0.6696, val_loss: 0.6798\n",
      "Epoch 2, train_loss: 0.6569, val_loss: 0.6743\n",
      "Epoch 3, train_loss: 0.6417, val_loss: 0.6706\n",
      "Epoch 4, train_loss: 0.6367, val_loss: 0.6678\n",
      "Epoch 5, train_loss: 0.6201, val_loss: 0.6649\n",
      "Epoch 6, train_loss: 0.6227, val_loss: 0.6624\n",
      "Epoch 7, train_loss: 0.6140, val_loss: 0.6591\n",
      "Epoch 8, train_loss: 0.5995, val_loss: 0.6568\n",
      "Epoch 9, train_loss: 0.5918, val_loss: 0.6551\n",
      "Epoch 10, train_loss: 0.5775, val_loss: 0.6510\n",
      "Epoch 11, train_loss: 0.5711, val_loss: 0.6473\n",
      "Epoch 12, train_loss: 0.5688, val_loss: 0.6440\n",
      "Epoch 13, train_loss: 0.5546, val_loss: 0.6395\n",
      "Epoch 14, train_loss: 0.5468, val_loss: 0.6370\n",
      "Epoch 15, train_loss: 0.5401, val_loss: 0.6333\n",
      "Epoch 16, train_loss: 0.5268, val_loss: 0.6294\n",
      "Epoch 17, train_loss: 0.5215, val_loss: 0.6249\n",
      "Epoch 18, train_loss: 0.5184, val_loss: 0.6210\n",
      "Epoch 19, train_loss: 0.4930, val_loss: 0.6177\n",
      "Epoch 20, train_loss: 0.4995, val_loss: 0.6130\n",
      "Epoch 21, train_loss: 0.4878, val_loss: 0.6080\n",
      "Epoch 22, train_loss: 0.4709, val_loss: 0.6045\n",
      "Epoch 23, train_loss: 0.4603, val_loss: 0.6004\n",
      "Epoch 24, train_loss: 0.4472, val_loss: 0.5969\n",
      "Epoch 25, train_loss: 0.4378, val_loss: 0.5914\n",
      "Epoch 26, train_loss: 0.4329, val_loss: 0.5878\n",
      "Epoch 27, train_loss: 0.4232, val_loss: 0.5852\n",
      "Epoch 28, train_loss: 0.4078, val_loss: 0.5816\n",
      "Epoch 29, train_loss: 0.4016, val_loss: 0.5781\n",
      "Epoch 30, train_loss: 0.3949, val_loss: 0.5749\n",
      "Epoch 31, train_loss: 0.3833, val_loss: 0.5728\n",
      "Epoch 32, train_loss: 0.3695, val_loss: 0.5693\n",
      "Epoch 33, train_loss: 0.3616, val_loss: 0.5675\n",
      "Epoch 34, train_loss: 0.3575, val_loss: 0.5637\n",
      "Epoch 35, train_loss: 0.3408, val_loss: 0.5611\n",
      "Epoch 36, train_loss: 0.3374, val_loss: 0.5578\n",
      "Epoch 37, train_loss: 0.3307, val_loss: 0.5569\n",
      "Epoch 38, train_loss: 0.3143, val_loss: 0.5539\n",
      "Epoch 39, train_loss: 0.3153, val_loss: 0.5503\n",
      "Epoch 40, train_loss: 0.3092, val_loss: 0.5486\n",
      "Epoch 41, train_loss: 0.2925, val_loss: 0.5466\n",
      "Epoch 42, train_loss: 0.2933, val_loss: 0.5469\n",
      "Epoch 43, train_loss: 0.2907, val_loss: 0.5435\n",
      "Epoch 44, train_loss: 0.2792, val_loss: 0.5433\n",
      "Epoch 45, train_loss: 0.2605, val_loss: 0.5433\n",
      "Epoch 46, train_loss: 0.2684, val_loss: 0.5428\n",
      "Epoch 47, train_loss: 0.2571, val_loss: 0.5398\n",
      "Epoch 48, train_loss: 0.2494, val_loss: 0.5381\n",
      "Epoch 49, train_loss: 0.2402, val_loss: 0.5364\n",
      "Epoch 50, train_loss: 0.2490, val_loss: 0.5357\n",
      "Epoch 51, train_loss: 0.2308, val_loss: 0.5339\n",
      "Epoch 52, train_loss: 0.2354, val_loss: 0.5314\n",
      "Epoch 53, train_loss: 0.2261, val_loss: 0.5304\n",
      "Epoch 54, train_loss: 0.2171, val_loss: 0.5285\n",
      "Epoch 55, train_loss: 0.2164, val_loss: 0.5301\n",
      "Epoch 56, train_loss: 0.2227, val_loss: 0.5289\n",
      "Epoch 57, train_loss: 0.1984, val_loss: 0.5264\n",
      "Epoch 58, train_loss: 0.2011, val_loss: 0.5266\n",
      "Epoch 59, train_loss: 0.2012, val_loss: 0.5240\n",
      "Epoch 60, train_loss: 0.1996, val_loss: 0.5220\n",
      "Epoch 61, train_loss: 0.1840, val_loss: 0.5224\n",
      "Epoch 62, train_loss: 0.1933, val_loss: 0.5215\n",
      "Epoch 63, train_loss: 0.1835, val_loss: 0.5234\n",
      "Epoch 64, train_loss: 0.1848, val_loss: 0.5215\n",
      "Epoch 65, train_loss: 0.1752, val_loss: 0.5215\n",
      "Epoch 66, train_loss: 0.1711, val_loss: 0.5206\n",
      "Epoch 67, train_loss: 0.1707, val_loss: 0.5192\n",
      "Epoch 68, train_loss: 0.1734, val_loss: 0.5182\n",
      "Epoch 69, train_loss: 0.1627, val_loss: 0.5174\n",
      "Epoch 70, train_loss: 0.1660, val_loss: 0.5194\n",
      "Epoch 71, train_loss: 0.1705, val_loss: 0.5171\n",
      "Epoch 72, train_loss: 0.1685, val_loss: 0.5166\n",
      "Epoch 73, train_loss: 0.1574, val_loss: 0.5162\n",
      "Epoch 74, train_loss: 0.1527, val_loss: 0.5162\n",
      "Epoch 75, train_loss: 0.1542, val_loss: 0.5151\n",
      "Epoch 76, train_loss: 0.1530, val_loss: 0.5143\n",
      "Epoch 77, train_loss: 0.1433, val_loss: 0.5125\n",
      "Epoch 78, train_loss: 0.1464, val_loss: 0.5118\n",
      "Epoch 79, train_loss: 0.1425, val_loss: 0.5120\n",
      "Epoch 80, train_loss: 0.1467, val_loss: 0.5110\n",
      "Epoch 81, train_loss: 0.1422, val_loss: 0.5114\n",
      "Epoch 82, train_loss: 0.1425, val_loss: 0.5098\n",
      "Epoch 83, train_loss: 0.1375, val_loss: 0.5103\n",
      "Epoch 84, train_loss: 0.1440, val_loss: 0.5101\n",
      "Epoch 85, train_loss: 0.1394, val_loss: 0.5104\n",
      "Epoch 86, train_loss: 0.1416, val_loss: 0.5089\n",
      "Epoch 87, train_loss: 0.1382, val_loss: 0.5070\n",
      "Epoch 88, train_loss: 0.1342, val_loss: 0.5070\n",
      "Epoch 89, train_loss: 0.1321, val_loss: 0.5073\n",
      "Epoch 90, train_loss: 0.1289, val_loss: 0.5061\n",
      "Epoch 91, train_loss: 0.1253, val_loss: 0.5078\n",
      "Epoch 92, train_loss: 0.1351, val_loss: 0.5084\n",
      "Epoch 93, train_loss: 0.1258, val_loss: 0.5078\n",
      "Epoch 94, train_loss: 0.1242, val_loss: 0.5075\n",
      "Epoch 95, train_loss: 0.1253, val_loss: 0.5075\n",
      "Epoch 96, train_loss: 0.1255, val_loss: 0.5088\n",
      "Epoch 97, train_loss: 0.1179, val_loss: 0.5073\n",
      "Epoch 98, train_loss: 0.1189, val_loss: 0.5074\n",
      "Epoch 99, train_loss: 0.1284, val_loss: 0.5080\n",
      "Test the model on fold 4:\n",
      "{'roc_auc': 0.8151700087183957, 'pr_auc': 0.7800153907627874, 'fmax': 0.7777728875478939}\n",
      "Evaluate pretrained model on disease class Connective tissue (2/12)\n",
      "Starting Fold: 0\n",
      "Epoch 0, train_loss: 0.6879, val_loss: 0.6877\n",
      "Epoch 1, train_loss: 0.6929, val_loss: 0.6873\n",
      "Epoch 2, train_loss: 0.6752, val_loss: 0.6885\n",
      "Epoch 3, train_loss: 0.6780, val_loss: 0.6881\n",
      "Epoch 4, train_loss: 0.6600, val_loss: 0.6883\n",
      "Epoch 5, train_loss: 0.6586, val_loss: 0.6890\n",
      "Epoch 6, train_loss: 0.6585, val_loss: 0.6899\n",
      "Epoch 7, train_loss: 0.6378, val_loss: 0.6910\n",
      "Epoch 8, train_loss: 0.6350, val_loss: 0.6911\n",
      "Epoch 9, train_loss: 0.6335, val_loss: 0.6908\n",
      "Epoch 10, train_loss: 0.6222, val_loss: 0.6904\n",
      "Epoch 11, train_loss: 0.6242, val_loss: 0.6882\n",
      "Epoch 12, train_loss: 0.6158, val_loss: 0.6881\n",
      "Epoch 13, train_loss: 0.6223, val_loss: 0.6874\n",
      "Epoch 14, train_loss: 0.6009, val_loss: 0.6856\n",
      "Epoch 15, train_loss: 0.5941, val_loss: 0.6857\n",
      "Epoch 16, train_loss: 0.5982, val_loss: 0.6841\n",
      "Epoch 17, train_loss: 0.5897, val_loss: 0.6831\n",
      "Epoch 18, train_loss: 0.5862, val_loss: 0.6818\n",
      "Epoch 19, train_loss: 0.5811, val_loss: 0.6817\n",
      "Epoch 20, train_loss: 0.5660, val_loss: 0.6812\n",
      "Epoch 21, train_loss: 0.5643, val_loss: 0.6807\n",
      "Epoch 22, train_loss: 0.5585, val_loss: 0.6797\n",
      "Epoch 23, train_loss: 0.5488, val_loss: 0.6779\n",
      "Epoch 24, train_loss: 0.5512, val_loss: 0.6764\n",
      "Epoch 25, train_loss: 0.5409, val_loss: 0.6747\n",
      "Epoch 26, train_loss: 0.5348, val_loss: 0.6740\n",
      "Epoch 27, train_loss: 0.5177, val_loss: 0.6739\n",
      "Epoch 28, train_loss: 0.5200, val_loss: 0.6727\n",
      "Epoch 29, train_loss: 0.5187, val_loss: 0.6728\n",
      "Epoch 30, train_loss: 0.5129, val_loss: 0.6734\n",
      "Epoch 31, train_loss: 0.5144, val_loss: 0.6729\n",
      "Epoch 32, train_loss: 0.4909, val_loss: 0.6736\n",
      "Epoch 33, train_loss: 0.4892, val_loss: 0.6736\n",
      "Epoch 34, train_loss: 0.4897, val_loss: 0.6732\n",
      "Epoch 35, train_loss: 0.4793, val_loss: 0.6725\n",
      "Epoch 36, train_loss: 0.4820, val_loss: 0.6717\n",
      "Epoch 37, train_loss: 0.4763, val_loss: 0.6717\n",
      "Epoch 38, train_loss: 0.4543, val_loss: 0.6713\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39, train_loss: 0.4530, val_loss: 0.6712\n",
      "Epoch 40, train_loss: 0.4485, val_loss: 0.6706\n",
      "Epoch 41, train_loss: 0.4564, val_loss: 0.6705\n",
      "Epoch 42, train_loss: 0.4526, val_loss: 0.6705\n",
      "Epoch 43, train_loss: 0.4400, val_loss: 0.6708\n",
      "Epoch 44, train_loss: 0.4334, val_loss: 0.6713\n",
      "Epoch 45, train_loss: 0.4196, val_loss: 0.6716\n",
      "Epoch 46, train_loss: 0.4198, val_loss: 0.6711\n",
      "Epoch 47, train_loss: 0.4137, val_loss: 0.6701\n",
      "Epoch 48, train_loss: 0.4124, val_loss: 0.6701\n",
      "Epoch 49, train_loss: 0.4093, val_loss: 0.6693\n",
      "Epoch 50, train_loss: 0.3932, val_loss: 0.6678\n",
      "Epoch 51, train_loss: 0.3984, val_loss: 0.6665\n",
      "Epoch 52, train_loss: 0.3944, val_loss: 0.6656\n",
      "Epoch 53, train_loss: 0.3912, val_loss: 0.6658\n",
      "Epoch 54, train_loss: 0.3802, val_loss: 0.6648\n",
      "Epoch 55, train_loss: 0.3828, val_loss: 0.6641\n",
      "Epoch 56, train_loss: 0.3680, val_loss: 0.6634\n",
      "Epoch 57, train_loss: 0.3617, val_loss: 0.6635\n",
      "Epoch 58, train_loss: 0.3588, val_loss: 0.6635\n",
      "Epoch 59, train_loss: 0.3524, val_loss: 0.6634\n",
      "Epoch 60, train_loss: 0.3516, val_loss: 0.6627\n",
      "Epoch 61, train_loss: 0.3482, val_loss: 0.6632\n",
      "Epoch 62, train_loss: 0.3405, val_loss: 0.6624\n",
      "Epoch 63, train_loss: 0.3321, val_loss: 0.6610\n",
      "Epoch 64, train_loss: 0.3277, val_loss: 0.6602\n",
      "Epoch 65, train_loss: 0.3253, val_loss: 0.6586\n",
      "Epoch 66, train_loss: 0.3215, val_loss: 0.6580\n",
      "Epoch 67, train_loss: 0.3240, val_loss: 0.6586\n",
      "Epoch 68, train_loss: 0.3221, val_loss: 0.6577\n",
      "Epoch 69, train_loss: 0.3144, val_loss: 0.6568\n",
      "Epoch 70, train_loss: 0.3155, val_loss: 0.6573\n",
      "Epoch 71, train_loss: 0.3052, val_loss: 0.6575\n",
      "Epoch 72, train_loss: 0.2934, val_loss: 0.6574\n",
      "Epoch 73, train_loss: 0.2982, val_loss: 0.6571\n",
      "Epoch 74, train_loss: 0.2929, val_loss: 0.6566\n",
      "Epoch 75, train_loss: 0.2841, val_loss: 0.6558\n",
      "Epoch 76, train_loss: 0.2805, val_loss: 0.6543\n",
      "Epoch 77, train_loss: 0.2817, val_loss: 0.6541\n",
      "Epoch 78, train_loss: 0.2874, val_loss: 0.6520\n",
      "Epoch 79, train_loss: 0.2794, val_loss: 0.6511\n",
      "Epoch 80, train_loss: 0.2678, val_loss: 0.6509\n",
      "Epoch 81, train_loss: 0.2617, val_loss: 0.6496\n",
      "Epoch 82, train_loss: 0.2571, val_loss: 0.6492\n",
      "Epoch 83, train_loss: 0.2636, val_loss: 0.6488\n",
      "Epoch 84, train_loss: 0.2604, val_loss: 0.6484\n",
      "Epoch 85, train_loss: 0.2498, val_loss: 0.6480\n",
      "Epoch 86, train_loss: 0.2491, val_loss: 0.6475\n",
      "Epoch 87, train_loss: 0.2372, val_loss: 0.6464\n",
      "Epoch 88, train_loss: 0.2446, val_loss: 0.6461\n",
      "Epoch 89, train_loss: 0.2508, val_loss: 0.6457\n",
      "Epoch 90, train_loss: 0.2381, val_loss: 0.6433\n",
      "Epoch 91, train_loss: 0.2342, val_loss: 0.6431\n",
      "Epoch 92, train_loss: 0.2286, val_loss: 0.6432\n",
      "Epoch 93, train_loss: 0.2305, val_loss: 0.6434\n",
      "Epoch 94, train_loss: 0.2302, val_loss: 0.6435\n",
      "Epoch 95, train_loss: 0.2169, val_loss: 0.6424\n",
      "Epoch 96, train_loss: 0.2181, val_loss: 0.6432\n",
      "Epoch 97, train_loss: 0.2172, val_loss: 0.6442\n",
      "Epoch 98, train_loss: 0.2217, val_loss: 0.6422\n",
      "Epoch 99, train_loss: 0.2151, val_loss: 0.6410\n",
      "Test the model on fold 0:\n",
      "{'roc_auc': 0.7083333333333333, 'pr_auc': 0.8602375814448435, 'fmax': 0.7894689058460962}\n",
      "Starting Fold: 1\n",
      "Epoch 0, train_loss: 0.6939, val_loss: 0.6782\n",
      "Epoch 1, train_loss: 0.6932, val_loss: 0.6739\n",
      "Epoch 2, train_loss: 0.6660, val_loss: 0.6701\n",
      "Epoch 3, train_loss: 0.6689, val_loss: 0.6688\n",
      "Epoch 4, train_loss: 0.6535, val_loss: 0.6658\n",
      "Epoch 5, train_loss: 0.6537, val_loss: 0.6633\n",
      "Epoch 6, train_loss: 0.6419, val_loss: 0.6614\n",
      "Epoch 7, train_loss: 0.6294, val_loss: 0.6588\n",
      "Epoch 8, train_loss: 0.6268, val_loss: 0.6577\n",
      "Epoch 9, train_loss: 0.6237, val_loss: 0.6559\n",
      "Epoch 10, train_loss: 0.6157, val_loss: 0.6547\n",
      "Epoch 11, train_loss: 0.6022, val_loss: 0.6532\n",
      "Epoch 12, train_loss: 0.5967, val_loss: 0.6504\n",
      "Epoch 13, train_loss: 0.5898, val_loss: 0.6481\n",
      "Epoch 14, train_loss: 0.5847, val_loss: 0.6455\n",
      "Epoch 15, train_loss: 0.5773, val_loss: 0.6433\n",
      "Epoch 16, train_loss: 0.5733, val_loss: 0.6424\n",
      "Epoch 17, train_loss: 0.5764, val_loss: 0.6418\n",
      "Epoch 18, train_loss: 0.5666, val_loss: 0.6408\n",
      "Epoch 19, train_loss: 0.5586, val_loss: 0.6393\n",
      "Epoch 20, train_loss: 0.5451, val_loss: 0.6381\n",
      "Epoch 21, train_loss: 0.5527, val_loss: 0.6361\n",
      "Epoch 22, train_loss: 0.5474, val_loss: 0.6341\n",
      "Epoch 23, train_loss: 0.5380, val_loss: 0.6326\n",
      "Epoch 24, train_loss: 0.5288, val_loss: 0.6309\n",
      "Epoch 25, train_loss: 0.5323, val_loss: 0.6291\n",
      "Epoch 26, train_loss: 0.5326, val_loss: 0.6279\n",
      "Epoch 27, train_loss: 0.5138, val_loss: 0.6271\n",
      "Epoch 28, train_loss: 0.5184, val_loss: 0.6268\n",
      "Epoch 29, train_loss: 0.5109, val_loss: 0.6265\n",
      "Epoch 30, train_loss: 0.5138, val_loss: 0.6261\n",
      "Epoch 31, train_loss: 0.5084, val_loss: 0.6251\n",
      "Epoch 32, train_loss: 0.4980, val_loss: 0.6244\n",
      "Epoch 33, train_loss: 0.4928, val_loss: 0.6230\n",
      "Epoch 34, train_loss: 0.4895, val_loss: 0.6216\n",
      "Epoch 35, train_loss: 0.4821, val_loss: 0.6209\n",
      "Epoch 36, train_loss: 0.4824, val_loss: 0.6207\n",
      "Epoch 37, train_loss: 0.4656, val_loss: 0.6199\n",
      "Epoch 38, train_loss: 0.4612, val_loss: 0.6186\n",
      "Epoch 39, train_loss: 0.4615, val_loss: 0.6169\n",
      "Epoch 40, train_loss: 0.4569, val_loss: 0.6155\n",
      "Epoch 41, train_loss: 0.4612, val_loss: 0.6147\n",
      "Epoch 42, train_loss: 0.4558, val_loss: 0.6142\n",
      "Epoch 43, train_loss: 0.4414, val_loss: 0.6138\n",
      "Epoch 44, train_loss: 0.4427, val_loss: 0.6127\n",
      "Epoch 45, train_loss: 0.4342, val_loss: 0.6130\n",
      "Epoch 46, train_loss: 0.4299, val_loss: 0.6124\n",
      "Epoch 47, train_loss: 0.4346, val_loss: 0.6126\n",
      "Epoch 48, train_loss: 0.4152, val_loss: 0.6125\n",
      "Epoch 49, train_loss: 0.4254, val_loss: 0.6118\n",
      "Epoch 50, train_loss: 0.4157, val_loss: 0.6108\n",
      "Epoch 51, train_loss: 0.4039, val_loss: 0.6090\n",
      "Epoch 52, train_loss: 0.4085, val_loss: 0.6067\n",
      "Epoch 53, train_loss: 0.3944, val_loss: 0.6054\n",
      "Epoch 54, train_loss: 0.3976, val_loss: 0.6054\n",
      "Epoch 55, train_loss: 0.3884, val_loss: 0.6051\n",
      "Epoch 56, train_loss: 0.3824, val_loss: 0.6041\n",
      "Epoch 57, train_loss: 0.3737, val_loss: 0.6028\n",
      "Epoch 58, train_loss: 0.3791, val_loss: 0.6027\n",
      "Epoch 59, train_loss: 0.3774, val_loss: 0.6020\n",
      "Epoch 60, train_loss: 0.3667, val_loss: 0.6009\n",
      "Epoch 61, train_loss: 0.3671, val_loss: 0.5996\n",
      "Epoch 62, train_loss: 0.3654, val_loss: 0.5987\n",
      "Epoch 63, train_loss: 0.3678, val_loss: 0.5975\n",
      "Epoch 64, train_loss: 0.3536, val_loss: 0.5957\n",
      "Epoch 65, train_loss: 0.3570, val_loss: 0.5952\n",
      "Epoch 66, train_loss: 0.3475, val_loss: 0.5940\n",
      "Epoch 67, train_loss: 0.3384, val_loss: 0.5933\n",
      "Epoch 68, train_loss: 0.3439, val_loss: 0.5931\n",
      "Epoch 69, train_loss: 0.3337, val_loss: 0.5923\n",
      "Epoch 70, train_loss: 0.3349, val_loss: 0.5910\n",
      "Epoch 71, train_loss: 0.3329, val_loss: 0.5893\n",
      "Epoch 72, train_loss: 0.3261, val_loss: 0.5883\n",
      "Epoch 73, train_loss: 0.3340, val_loss: 0.5894\n",
      "Epoch 74, train_loss: 0.3168, val_loss: 0.5899\n",
      "Epoch 75, train_loss: 0.3169, val_loss: 0.5892\n",
      "Epoch 76, train_loss: 0.3185, val_loss: 0.5886\n",
      "Epoch 77, train_loss: 0.3100, val_loss: 0.5883\n",
      "Epoch 78, train_loss: 0.3064, val_loss: 0.5870\n",
      "Epoch 79, train_loss: 0.3023, val_loss: 0.5850\n",
      "Epoch 80, train_loss: 0.3047, val_loss: 0.5815\n",
      "Epoch 81, train_loss: 0.2973, val_loss: 0.5821\n",
      "Epoch 82, train_loss: 0.2944, val_loss: 0.5811\n",
      "Epoch 83, train_loss: 0.2859, val_loss: 0.5808\n",
      "Epoch 84, train_loss: 0.2899, val_loss: 0.5798\n",
      "Epoch 85, train_loss: 0.2752, val_loss: 0.5774\n",
      "Epoch 86, train_loss: 0.2847, val_loss: 0.5773\n",
      "Epoch 87, train_loss: 0.2688, val_loss: 0.5778\n",
      "Epoch 88, train_loss: 0.2737, val_loss: 0.5794\n",
      "Epoch 89, train_loss: 0.2631, val_loss: 0.5784\n",
      "Epoch 90, train_loss: 0.2733, val_loss: 0.5780\n",
      "Epoch 91, train_loss: 0.2581, val_loss: 0.5776\n",
      "Epoch 92, train_loss: 0.2603, val_loss: 0.5757\n",
      "Epoch 93, train_loss: 0.2549, val_loss: 0.5743\n",
      "Epoch 94, train_loss: 0.2570, val_loss: 0.5740\n",
      "Epoch 95, train_loss: 0.2449, val_loss: 0.5730\n",
      "Epoch 96, train_loss: 0.2430, val_loss: 0.5715\n",
      "Epoch 97, train_loss: 0.2401, val_loss: 0.5700\n",
      "Epoch 98, train_loss: 0.2484, val_loss: 0.5696\n",
      "Epoch 99, train_loss: 0.2401, val_loss: 0.5672\n",
      "Test the model on fold 1:\n",
      "{'roc_auc': 0.7272727272727273, 'pr_auc': 0.7795763466944587, 'fmax': 0.6999950500350033}\n",
      "Starting Fold: 2\n",
      "Epoch 0, train_loss: 0.6973, val_loss: 0.6745\n",
      "Epoch 1, train_loss: 0.6763, val_loss: 0.6733\n",
      "Epoch 2, train_loss: 0.6916, val_loss: 0.6714\n",
      "Epoch 3, train_loss: 0.6783, val_loss: 0.6699\n",
      "Epoch 4, train_loss: 0.6513, val_loss: 0.6678\n",
      "Epoch 5, train_loss: 0.6718, val_loss: 0.6648\n",
      "Epoch 6, train_loss: 0.6531, val_loss: 0.6635\n",
      "Epoch 7, train_loss: 0.6386, val_loss: 0.6630\n",
      "Epoch 8, train_loss: 0.6304, val_loss: 0.6630\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9, train_loss: 0.6579, val_loss: 0.6623\n",
      "Epoch 10, train_loss: 0.6240, val_loss: 0.6617\n",
      "Epoch 11, train_loss: 0.6243, val_loss: 0.6612\n",
      "Epoch 12, train_loss: 0.6176, val_loss: 0.6608\n",
      "Epoch 13, train_loss: 0.6108, val_loss: 0.6592\n",
      "Epoch 14, train_loss: 0.5801, val_loss: 0.6581\n",
      "Epoch 15, train_loss: 0.6009, val_loss: 0.6566\n",
      "Epoch 16, train_loss: 0.5989, val_loss: 0.6555\n",
      "Epoch 17, train_loss: 0.6029, val_loss: 0.6544\n",
      "Epoch 18, train_loss: 0.5912, val_loss: 0.6537\n",
      "Epoch 19, train_loss: 0.5852, val_loss: 0.6529\n",
      "Epoch 20, train_loss: 0.5849, val_loss: 0.6519\n",
      "Epoch 21, train_loss: 0.5824, val_loss: 0.6512\n",
      "Epoch 22, train_loss: 0.5605, val_loss: 0.6513\n",
      "Epoch 23, train_loss: 0.5809, val_loss: 0.6504\n",
      "Epoch 24, train_loss: 0.5523, val_loss: 0.6498\n",
      "Epoch 25, train_loss: 0.5690, val_loss: 0.6495\n",
      "Epoch 26, train_loss: 0.5574, val_loss: 0.6493\n",
      "Epoch 27, train_loss: 0.5455, val_loss: 0.6489\n",
      "Epoch 28, train_loss: 0.5434, val_loss: 0.6484\n",
      "Epoch 29, train_loss: 0.5496, val_loss: 0.6481\n",
      "Epoch 30, train_loss: 0.5018, val_loss: 0.6475\n",
      "Epoch 31, train_loss: 0.5330, val_loss: 0.6466\n",
      "Epoch 32, train_loss: 0.5147, val_loss: 0.6460\n",
      "Epoch 33, train_loss: 0.5272, val_loss: 0.6453\n",
      "Epoch 34, train_loss: 0.5005, val_loss: 0.6452\n",
      "Epoch 35, train_loss: 0.5310, val_loss: 0.6444\n",
      "Epoch 36, train_loss: 0.5297, val_loss: 0.6439\n",
      "Epoch 37, train_loss: 0.4975, val_loss: 0.6434\n",
      "Epoch 38, train_loss: 0.5224, val_loss: 0.6426\n",
      "Epoch 39, train_loss: 0.5104, val_loss: 0.6419\n",
      "Epoch 40, train_loss: 0.5079, val_loss: 0.6414\n",
      "Epoch 41, train_loss: 0.4568, val_loss: 0.6409\n",
      "Epoch 42, train_loss: 0.4891, val_loss: 0.6409\n",
      "Epoch 43, train_loss: 0.4827, val_loss: 0.6408\n",
      "Epoch 44, train_loss: 0.4399, val_loss: 0.6404\n",
      "Epoch 45, train_loss: 0.4434, val_loss: 0.6389\n",
      "Epoch 46, train_loss: 0.4610, val_loss: 0.6378\n",
      "Epoch 47, train_loss: 0.4786, val_loss: 0.6374\n",
      "Epoch 48, train_loss: 0.4836, val_loss: 0.6373\n",
      "Epoch 49, train_loss: 0.4476, val_loss: 0.6373\n",
      "Epoch 50, train_loss: 0.4661, val_loss: 0.6385\n",
      "Epoch 51, train_loss: 0.4350, val_loss: 0.6391\n",
      "Epoch 52, train_loss: 0.3939, val_loss: 0.6390\n",
      "Epoch 53, train_loss: 0.4113, val_loss: 0.6384\n",
      "Epoch 54, train_loss: 0.4632, val_loss: 0.6380\n",
      "Epoch 55, train_loss: 0.4176, val_loss: 0.6376\n",
      "Epoch 56, train_loss: 0.4507, val_loss: 0.6372\n",
      "Epoch 57, train_loss: 0.4097, val_loss: 0.6368\n",
      "Epoch 58, train_loss: 0.4380, val_loss: 0.6368\n",
      "Epoch 59, train_loss: 0.3949, val_loss: 0.6366\n",
      "Epoch 60, train_loss: 0.3772, val_loss: 0.6365\n",
      "Epoch 61, train_loss: 0.3736, val_loss: 0.6366\n",
      "Epoch 62, train_loss: 0.4220, val_loss: 0.6362\n",
      "Epoch 63, train_loss: 0.3596, val_loss: 0.6351\n",
      "Epoch 64, train_loss: 0.3920, val_loss: 0.6351\n",
      "Epoch 65, train_loss: 0.3607, val_loss: 0.6350\n",
      "Epoch 66, train_loss: 0.3404, val_loss: 0.6345\n",
      "Epoch 67, train_loss: 0.3806, val_loss: 0.6341\n",
      "Epoch 68, train_loss: 0.3527, val_loss: 0.6341\n",
      "Epoch 69, train_loss: 0.3289, val_loss: 0.6342\n",
      "Epoch 70, train_loss: 0.3778, val_loss: 0.6340\n",
      "Epoch 71, train_loss: 0.3467, val_loss: 0.6338\n",
      "Epoch 72, train_loss: 0.3735, val_loss: 0.6333\n",
      "Epoch 73, train_loss: 0.3657, val_loss: 0.6327\n",
      "Epoch 74, train_loss: 0.3336, val_loss: 0.6326\n",
      "Epoch 75, train_loss: 0.3511, val_loss: 0.6320\n",
      "Epoch 76, train_loss: 0.3291, val_loss: 0.6318\n",
      "Epoch 77, train_loss: 0.3401, val_loss: 0.6318\n",
      "Epoch 78, train_loss: 0.2901, val_loss: 0.6311\n",
      "Epoch 79, train_loss: 0.3402, val_loss: 0.6296\n",
      "Epoch 80, train_loss: 0.3203, val_loss: 0.6295\n",
      "Epoch 81, train_loss: 0.3012, val_loss: 0.6297\n",
      "Epoch 82, train_loss: 0.2934, val_loss: 0.6326\n",
      "Epoch 83, train_loss: 0.3049, val_loss: 0.6344\n",
      "Early Stopping!\n",
      "Test the model on fold 2:\n",
      "{'roc_auc': 0.8035714285714285, 'pr_auc': 0.907592188513241, 'fmax': 0.8148098217041229}\n",
      "Starting Fold: 3\n",
      "Epoch 0, train_loss: 0.6810, val_loss: 0.6753\n",
      "Epoch 1, train_loss: 0.6857, val_loss: 0.6709\n",
      "Epoch 2, train_loss: 0.6761, val_loss: 0.6659\n",
      "Epoch 3, train_loss: 0.6592, val_loss: 0.6613\n",
      "Epoch 4, train_loss: 0.6441, val_loss: 0.6572\n",
      "Epoch 5, train_loss: 0.6161, val_loss: 0.6514\n",
      "Epoch 6, train_loss: 0.6473, val_loss: 0.6449\n",
      "Epoch 7, train_loss: 0.6290, val_loss: 0.6404\n",
      "Epoch 8, train_loss: 0.6042, val_loss: 0.6383\n",
      "Epoch 9, train_loss: 0.6177, val_loss: 0.6371\n",
      "Epoch 10, train_loss: 0.6334, val_loss: 0.6373\n",
      "Epoch 11, train_loss: 0.6133, val_loss: 0.6364\n",
      "Epoch 12, train_loss: 0.5914, val_loss: 0.6346\n",
      "Epoch 13, train_loss: 0.6069, val_loss: 0.6327\n",
      "Epoch 14, train_loss: 0.5933, val_loss: 0.6311\n",
      "Epoch 15, train_loss: 0.5942, val_loss: 0.6293\n",
      "Epoch 16, train_loss: 0.5773, val_loss: 0.6278\n",
      "Epoch 17, train_loss: 0.5323, val_loss: 0.6256\n",
      "Epoch 18, train_loss: 0.5107, val_loss: 0.6220\n",
      "Epoch 19, train_loss: 0.5807, val_loss: 0.6177\n",
      "Epoch 20, train_loss: 0.4773, val_loss: 0.6147\n",
      "Epoch 21, train_loss: 0.5526, val_loss: 0.6120\n",
      "Epoch 22, train_loss: 0.5275, val_loss: 0.6097\n",
      "Epoch 23, train_loss: 0.5108, val_loss: 0.6074\n",
      "Epoch 24, train_loss: 0.5686, val_loss: 0.6060\n",
      "Epoch 25, train_loss: 0.4727, val_loss: 0.6054\n",
      "Epoch 26, train_loss: 0.4828, val_loss: 0.6031\n",
      "Epoch 27, train_loss: 0.4523, val_loss: 0.6004\n",
      "Epoch 28, train_loss: 0.4747, val_loss: 0.5980\n",
      "Epoch 29, train_loss: 0.5038, val_loss: 0.5957\n",
      "Epoch 30, train_loss: 0.4338, val_loss: 0.5937\n",
      "Epoch 31, train_loss: 0.5166, val_loss: 0.5925\n",
      "Epoch 32, train_loss: 0.5323, val_loss: 0.5922\n",
      "Epoch 33, train_loss: 0.4285, val_loss: 0.5924\n",
      "Epoch 34, train_loss: 0.4467, val_loss: 0.5914\n",
      "Epoch 35, train_loss: 0.4707, val_loss: 0.5897\n",
      "Epoch 36, train_loss: 0.5075, val_loss: 0.5867\n",
      "Epoch 37, train_loss: 0.4363, val_loss: 0.5842\n",
      "Epoch 38, train_loss: 0.4593, val_loss: 0.5826\n",
      "Epoch 39, train_loss: 0.4166, val_loss: 0.5816\n",
      "Epoch 40, train_loss: 0.4292, val_loss: 0.5804\n",
      "Epoch 41, train_loss: 0.5073, val_loss: 0.5788\n",
      "Epoch 42, train_loss: 0.4897, val_loss: 0.5779\n",
      "Epoch 43, train_loss: 0.4834, val_loss: 0.5788\n",
      "Epoch 44, train_loss: 0.4847, val_loss: 0.5792\n",
      "Epoch 45, train_loss: 0.4350, val_loss: 0.5790\n",
      "Epoch 46, train_loss: 0.4636, val_loss: 0.5779\n",
      "Epoch 47, train_loss: 0.4592, val_loss: 0.5779\n",
      "Epoch 48, train_loss: 0.4363, val_loss: 0.5789\n",
      "Epoch 49, train_loss: 0.4205, val_loss: 0.5797\n",
      "Epoch 50, train_loss: 0.3718, val_loss: 0.5794\n",
      "Epoch 51, train_loss: 0.4451, val_loss: 0.5791\n",
      "Epoch 52, train_loss: 0.3674, val_loss: 0.5792\n",
      "Epoch 53, train_loss: 0.4503, val_loss: 0.5791\n",
      "Epoch 54, train_loss: 0.4064, val_loss: 0.5788\n",
      "Epoch 55, train_loss: 0.4471, val_loss: 0.5773\n",
      "Epoch 56, train_loss: 0.4367, val_loss: 0.5770\n",
      "Epoch 57, train_loss: 0.4350, val_loss: 0.5765\n",
      "Epoch 58, train_loss: 0.3568, val_loss: 0.5759\n",
      "Epoch 59, train_loss: 0.3663, val_loss: 0.5744\n",
      "Epoch 60, train_loss: 0.4211, val_loss: 0.5737\n",
      "Epoch 61, train_loss: 0.3731, val_loss: 0.5732\n",
      "Epoch 62, train_loss: 0.4144, val_loss: 0.5726\n",
      "Epoch 63, train_loss: 0.4200, val_loss: 0.5713\n",
      "Epoch 64, train_loss: 0.3863, val_loss: 0.5698\n",
      "Epoch 65, train_loss: 0.3381, val_loss: 0.5675\n",
      "Epoch 66, train_loss: 0.4214, val_loss: 0.5660\n",
      "Epoch 67, train_loss: 0.4064, val_loss: 0.5659\n",
      "Epoch 68, train_loss: 0.3959, val_loss: 0.5662\n",
      "Epoch 69, train_loss: 0.3266, val_loss: 0.5673\n",
      "Epoch 70, train_loss: 0.4006, val_loss: 0.5682\n",
      "Epoch 71, train_loss: 0.3826, val_loss: 0.5691\n",
      "Epoch 72, train_loss: 0.3766, val_loss: 0.5703\n",
      "Epoch 73, train_loss: 0.3745, val_loss: 0.5696\n",
      "Epoch 74, train_loss: 0.3387, val_loss: 0.5689\n",
      "Epoch 75, train_loss: 0.3158, val_loss: 0.5677\n",
      "Epoch 76, train_loss: 0.3072, val_loss: 0.5661\n",
      "Epoch 77, train_loss: 0.3815, val_loss: 0.5646\n",
      "Epoch 78, train_loss: 0.3008, val_loss: 0.5642\n",
      "Epoch 79, train_loss: 0.3475, val_loss: 0.5638\n",
      "Epoch 80, train_loss: 0.3521, val_loss: 0.5649\n",
      "Epoch 81, train_loss: 0.4036, val_loss: 0.5672\n",
      "Epoch 82, train_loss: 0.3437, val_loss: 0.5689\n",
      "Epoch 83, train_loss: 0.3934, val_loss: 0.5686\n",
      "Epoch 84, train_loss: 0.3512, val_loss: 0.5688\n",
      "Epoch 85, train_loss: 0.3748, val_loss: 0.5695\n",
      "Epoch 86, train_loss: 0.3117, val_loss: 0.5701\n",
      "Epoch 87, train_loss: 0.3609, val_loss: 0.5708\n",
      "Early Stopping!\n",
      "Test the model on fold 3:\n",
      "{'roc_auc': 0.84375, 'pr_auc': 0.6656746031746031, 'fmax': 0.7692257988486843}\n",
      "Starting Fold: 4\n",
      "Epoch 0, train_loss: 0.7050, val_loss: 0.6824\n",
      "Epoch 1, train_loss: 0.6869, val_loss: 0.6787\n",
      "Epoch 2, train_loss: 0.6462, val_loss: 0.6731\n",
      "Epoch 3, train_loss: 0.6769, val_loss: 0.6666\n",
      "Epoch 4, train_loss: 0.6593, val_loss: 0.6623\n",
      "Epoch 5, train_loss: 0.6404, val_loss: 0.6576\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6, train_loss: 0.6426, val_loss: 0.6520\n",
      "Epoch 7, train_loss: 0.6298, val_loss: 0.6454\n",
      "Epoch 8, train_loss: 0.6469, val_loss: 0.6379\n",
      "Epoch 9, train_loss: 0.6119, val_loss: 0.6323\n",
      "Epoch 10, train_loss: 0.6245, val_loss: 0.6278\n",
      "Epoch 11, train_loss: 0.6115, val_loss: 0.6248\n",
      "Epoch 12, train_loss: 0.6110, val_loss: 0.6214\n",
      "Epoch 13, train_loss: 0.6087, val_loss: 0.6191\n",
      "Epoch 14, train_loss: 0.5992, val_loss: 0.6169\n",
      "Epoch 15, train_loss: 0.6094, val_loss: 0.6138\n",
      "Epoch 16, train_loss: 0.5717, val_loss: 0.6102\n",
      "Epoch 17, train_loss: 0.5832, val_loss: 0.6047\n",
      "Epoch 18, train_loss: 0.5298, val_loss: 0.5992\n",
      "Epoch 19, train_loss: 0.5656, val_loss: 0.5940\n",
      "Epoch 20, train_loss: 0.5760, val_loss: 0.5917\n",
      "Epoch 21, train_loss: 0.5657, val_loss: 0.5881\n",
      "Epoch 22, train_loss: 0.5486, val_loss: 0.5857\n",
      "Epoch 23, train_loss: 0.5411, val_loss: 0.5836\n",
      "Epoch 24, train_loss: 0.5233, val_loss: 0.5819\n",
      "Epoch 25, train_loss: 0.5536, val_loss: 0.5781\n",
      "Epoch 26, train_loss: 0.5261, val_loss: 0.5741\n",
      "Epoch 27, train_loss: 0.4852, val_loss: 0.5695\n",
      "Epoch 28, train_loss: 0.5108, val_loss: 0.5633\n",
      "Epoch 29, train_loss: 0.5053, val_loss: 0.5584\n",
      "Epoch 30, train_loss: 0.5212, val_loss: 0.5541\n",
      "Epoch 31, train_loss: 0.4848, val_loss: 0.5506\n",
      "Epoch 32, train_loss: 0.5118, val_loss: 0.5482\n",
      "Epoch 33, train_loss: 0.5020, val_loss: 0.5473\n",
      "Epoch 34, train_loss: 0.4670, val_loss: 0.5461\n",
      "Epoch 35, train_loss: 0.4409, val_loss: 0.5441\n",
      "Epoch 36, train_loss: 0.5188, val_loss: 0.5432\n",
      "Epoch 37, train_loss: 0.4282, val_loss: 0.5413\n",
      "Epoch 38, train_loss: 0.4649, val_loss: 0.5376\n",
      "Epoch 39, train_loss: 0.4944, val_loss: 0.5347\n",
      "Epoch 40, train_loss: 0.4319, val_loss: 0.5322\n",
      "Epoch 41, train_loss: 0.4796, val_loss: 0.5302\n",
      "Epoch 42, train_loss: 0.4563, val_loss: 0.5284\n",
      "Epoch 43, train_loss: 0.4302, val_loss: 0.5265\n",
      "Epoch 44, train_loss: 0.4747, val_loss: 0.5254\n",
      "Epoch 45, train_loss: 0.3912, val_loss: 0.5234\n",
      "Epoch 46, train_loss: 0.4057, val_loss: 0.5204\n",
      "Epoch 47, train_loss: 0.4351, val_loss: 0.5193\n",
      "Epoch 48, train_loss: 0.3956, val_loss: 0.5175\n",
      "Epoch 49, train_loss: 0.4126, val_loss: 0.5152\n",
      "Epoch 50, train_loss: 0.4420, val_loss: 0.5146\n",
      "Epoch 51, train_loss: 0.4278, val_loss: 0.5143\n",
      "Epoch 52, train_loss: 0.3712, val_loss: 0.5144\n",
      "Epoch 53, train_loss: 0.4300, val_loss: 0.5139\n",
      "Epoch 54, train_loss: 0.4173, val_loss: 0.5128\n",
      "Epoch 55, train_loss: 0.3635, val_loss: 0.5117\n",
      "Epoch 56, train_loss: 0.4153, val_loss: 0.5098\n",
      "Epoch 57, train_loss: 0.3704, val_loss: 0.5092\n",
      "Epoch 58, train_loss: 0.3659, val_loss: 0.5096\n",
      "Epoch 59, train_loss: 0.4148, val_loss: 0.5097\n",
      "Epoch 60, train_loss: 0.4080, val_loss: 0.5093\n",
      "Epoch 61, train_loss: 0.3711, val_loss: 0.5086\n",
      "Epoch 62, train_loss: 0.3784, val_loss: 0.5074\n",
      "Epoch 63, train_loss: 0.3424, val_loss: 0.5067\n",
      "Epoch 64, train_loss: 0.3180, val_loss: 0.5066\n",
      "Epoch 65, train_loss: 0.3183, val_loss: 0.5062\n",
      "Epoch 66, train_loss: 0.3275, val_loss: 0.5054\n",
      "Epoch 67, train_loss: 0.3596, val_loss: 0.5056\n",
      "Epoch 68, train_loss: 0.3265, val_loss: 0.5089\n",
      "Epoch 69, train_loss: 0.2978, val_loss: 0.5108\n",
      "Epoch 70, train_loss: 0.3199, val_loss: 0.5103\n",
      "Epoch 71, train_loss: 0.3121, val_loss: 0.5087\n",
      "Epoch 72, train_loss: 0.3016, val_loss: 0.5075\n",
      "Epoch 73, train_loss: 0.3134, val_loss: 0.5075\n",
      "Epoch 74, train_loss: 0.2802, val_loss: 0.5066\n",
      "Epoch 75, train_loss: 0.3422, val_loss: 0.5048\n",
      "Epoch 76, train_loss: 0.3582, val_loss: 0.5034\n",
      "Epoch 77, train_loss: 0.2979, val_loss: 0.5024\n",
      "Epoch 78, train_loss: 0.3474, val_loss: 0.5016\n",
      "Epoch 79, train_loss: 0.3475, val_loss: 0.5023\n",
      "Epoch 80, train_loss: 0.2907, val_loss: 0.5025\n",
      "Epoch 81, train_loss: 0.3059, val_loss: 0.5034\n",
      "Epoch 82, train_loss: 0.3035, val_loss: 0.5043\n",
      "Epoch 83, train_loss: 0.3049, val_loss: 0.5053\n",
      "Epoch 84, train_loss: 0.2659, val_loss: 0.5052\n",
      "Epoch 85, train_loss: 0.2981, val_loss: 0.5047\n",
      "Epoch 86, train_loss: 0.2840, val_loss: 0.5047\n",
      "Epoch 87, train_loss: 0.2643, val_loss: 0.5040\n",
      "Epoch 88, train_loss: 0.2342, val_loss: 0.5032\n",
      "Epoch 89, train_loss: 0.2520, val_loss: 0.5024\n",
      "Epoch 90, train_loss: 0.3308, val_loss: 0.5015\n",
      "Epoch 91, train_loss: 0.2705, val_loss: 0.5013\n",
      "Epoch 92, train_loss: 0.2342, val_loss: 0.5013\n",
      "Epoch 93, train_loss: 0.2493, val_loss: 0.5019\n",
      "Epoch 94, train_loss: 0.2281, val_loss: 0.5024\n",
      "Epoch 95, train_loss: 0.2813, val_loss: 0.5015\n",
      "Epoch 96, train_loss: 0.2633, val_loss: 0.5035\n",
      "Epoch 97, train_loss: 0.2515, val_loss: 0.5129\n",
      "Early Stopping!\n",
      "Test the model on fold 4:\n",
      "{'roc_auc': 0.5916666666666666, 'pr_auc': 0.6357156064180057, 'fmax': 0.6249957031545408}\n",
      "Evaluate pretrained model on disease class Endocrine (3/12)\n",
      "Starting Fold: 0\n",
      "Epoch 0, train_loss: 0.6978, val_loss: 0.6835\n",
      "Epoch 1, train_loss: 0.6876, val_loss: 0.6800\n",
      "Epoch 2, train_loss: 0.6831, val_loss: 0.6784\n",
      "Epoch 3, train_loss: 0.6747, val_loss: 0.6753\n",
      "Epoch 4, train_loss: 0.6684, val_loss: 0.6728\n",
      "Epoch 5, train_loss: 0.6544, val_loss: 0.6690\n",
      "Epoch 6, train_loss: 0.6500, val_loss: 0.6655\n",
      "Epoch 7, train_loss: 0.6458, val_loss: 0.6606\n",
      "Epoch 8, train_loss: 0.6363, val_loss: 0.6564\n",
      "Epoch 9, train_loss: 0.6372, val_loss: 0.6531\n",
      "Epoch 10, train_loss: 0.6259, val_loss: 0.6519\n",
      "Epoch 11, train_loss: 0.6219, val_loss: 0.6489\n",
      "Epoch 12, train_loss: 0.6153, val_loss: 0.6461\n",
      "Epoch 13, train_loss: 0.6061, val_loss: 0.6435\n",
      "Epoch 14, train_loss: 0.5919, val_loss: 0.6402\n",
      "Epoch 15, train_loss: 0.5935, val_loss: 0.6360\n",
      "Epoch 16, train_loss: 0.5904, val_loss: 0.6323\n",
      "Epoch 17, train_loss: 0.5893, val_loss: 0.6283\n",
      "Epoch 18, train_loss: 0.5718, val_loss: 0.6238\n",
      "Epoch 19, train_loss: 0.5679, val_loss: 0.6208\n",
      "Epoch 20, train_loss: 0.5619, val_loss: 0.6173\n",
      "Epoch 21, train_loss: 0.5535, val_loss: 0.6138\n",
      "Epoch 22, train_loss: 0.5495, val_loss: 0.6090\n",
      "Epoch 23, train_loss: 0.5480, val_loss: 0.6070\n",
      "Epoch 24, train_loss: 0.5440, val_loss: 0.6043\n",
      "Epoch 25, train_loss: 0.5402, val_loss: 0.6011\n",
      "Epoch 26, train_loss: 0.5187, val_loss: 0.5977\n",
      "Epoch 27, train_loss: 0.4958, val_loss: 0.5944\n",
      "Epoch 28, train_loss: 0.4882, val_loss: 0.5909\n",
      "Epoch 29, train_loss: 0.5004, val_loss: 0.5882\n",
      "Epoch 30, train_loss: 0.4882, val_loss: 0.5841\n",
      "Epoch 31, train_loss: 0.4794, val_loss: 0.5798\n",
      "Epoch 32, train_loss: 0.4720, val_loss: 0.5767\n",
      "Epoch 33, train_loss: 0.4684, val_loss: 0.5724\n",
      "Epoch 34, train_loss: 0.4496, val_loss: 0.5707\n",
      "Epoch 35, train_loss: 0.4406, val_loss: 0.5663\n",
      "Epoch 36, train_loss: 0.4331, val_loss: 0.5602\n",
      "Epoch 37, train_loss: 0.4347, val_loss: 0.5573\n",
      "Epoch 38, train_loss: 0.4248, val_loss: 0.5539\n",
      "Epoch 39, train_loss: 0.4113, val_loss: 0.5500\n",
      "Epoch 40, train_loss: 0.4146, val_loss: 0.5465\n",
      "Epoch 41, train_loss: 0.4063, val_loss: 0.5418\n",
      "Epoch 42, train_loss: 0.4030, val_loss: 0.5375\n",
      "Epoch 43, train_loss: 0.3718, val_loss: 0.5326\n",
      "Epoch 44, train_loss: 0.3726, val_loss: 0.5282\n",
      "Epoch 45, train_loss: 0.3717, val_loss: 0.5267\n",
      "Epoch 46, train_loss: 0.3594, val_loss: 0.5244\n",
      "Epoch 47, train_loss: 0.3645, val_loss: 0.5227\n",
      "Epoch 48, train_loss: 0.3250, val_loss: 0.5145\n",
      "Epoch 49, train_loss: 0.3210, val_loss: 0.5114\n",
      "Epoch 50, train_loss: 0.3215, val_loss: 0.5081\n",
      "Epoch 51, train_loss: 0.3226, val_loss: 0.5051\n",
      "Epoch 52, train_loss: 0.3280, val_loss: 0.5015\n",
      "Epoch 53, train_loss: 0.3269, val_loss: 0.4977\n",
      "Epoch 54, train_loss: 0.2905, val_loss: 0.4971\n",
      "Epoch 55, train_loss: 0.2962, val_loss: 0.4952\n",
      "Epoch 56, train_loss: 0.2969, val_loss: 0.4938\n",
      "Epoch 57, train_loss: 0.2828, val_loss: 0.4907\n",
      "Epoch 58, train_loss: 0.2721, val_loss: 0.4844\n",
      "Epoch 59, train_loss: 0.2646, val_loss: 0.4823\n",
      "Epoch 60, train_loss: 0.2746, val_loss: 0.4800\n",
      "Epoch 61, train_loss: 0.2595, val_loss: 0.4783\n",
      "Epoch 62, train_loss: 0.2583, val_loss: 0.4786\n",
      "Epoch 63, train_loss: 0.2417, val_loss: 0.4783\n",
      "Epoch 64, train_loss: 0.2350, val_loss: 0.4773\n",
      "Epoch 65, train_loss: 0.2378, val_loss: 0.4750\n",
      "Epoch 66, train_loss: 0.2335, val_loss: 0.4745\n",
      "Epoch 67, train_loss: 0.2312, val_loss: 0.4710\n",
      "Epoch 68, train_loss: 0.2433, val_loss: 0.4700\n",
      "Epoch 69, train_loss: 0.2513, val_loss: 0.4679\n",
      "Epoch 70, train_loss: 0.2279, val_loss: 0.4644\n",
      "Epoch 71, train_loss: 0.2101, val_loss: 0.4611\n",
      "Epoch 72, train_loss: 0.2219, val_loss: 0.4597\n",
      "Epoch 73, train_loss: 0.1940, val_loss: 0.4596\n",
      "Epoch 74, train_loss: 0.1932, val_loss: 0.4582\n",
      "Epoch 75, train_loss: 0.2125, val_loss: 0.4566\n",
      "Epoch 76, train_loss: 0.1884, val_loss: 0.4552\n",
      "Epoch 77, train_loss: 0.1837, val_loss: 0.4566\n",
      "Epoch 78, train_loss: 0.1908, val_loss: 0.4566\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 79, train_loss: 0.1899, val_loss: 0.4540\n",
      "Epoch 80, train_loss: 0.1871, val_loss: 0.4557\n",
      "Epoch 81, train_loss: 0.1807, val_loss: 0.4565\n",
      "Epoch 82, train_loss: 0.1788, val_loss: 0.4566\n",
      "Epoch 83, train_loss: 0.1830, val_loss: 0.4546\n",
      "Epoch 84, train_loss: 0.1728, val_loss: 0.4609\n",
      "Epoch 85, train_loss: 0.1733, val_loss: 0.4582\n",
      "Epoch 86, train_loss: 0.1616, val_loss: 0.4535\n",
      "Epoch 87, train_loss: 0.1786, val_loss: 0.4515\n",
      "Epoch 88, train_loss: 0.1661, val_loss: 0.4514\n",
      "Epoch 89, train_loss: 0.1573, val_loss: 0.4524\n",
      "Epoch 90, train_loss: 0.1515, val_loss: 0.4502\n",
      "Epoch 91, train_loss: 0.1601, val_loss: 0.4462\n",
      "Epoch 92, train_loss: 0.1546, val_loss: 0.4457\n",
      "Epoch 93, train_loss: 0.1692, val_loss: 0.4442\n",
      "Epoch 94, train_loss: 0.1549, val_loss: 0.4466\n",
      "Epoch 95, train_loss: 0.1605, val_loss: 0.4485\n",
      "Epoch 96, train_loss: 0.1482, val_loss: 0.4503\n",
      "Epoch 97, train_loss: 0.1400, val_loss: 0.4520\n",
      "Epoch 98, train_loss: 0.1503, val_loss: 0.4501\n",
      "Epoch 99, train_loss: 0.1371, val_loss: 0.4493\n",
      "Test the model on fold 0:\n",
      "{'roc_auc': 0.8814102564102565, 'pr_auc': 0.7963498732081419, 'fmax': 0.8474526975293935}\n",
      "Starting Fold: 1\n",
      "Epoch 0, train_loss: 0.6837, val_loss: 0.6883\n",
      "Epoch 1, train_loss: 0.6859, val_loss: 0.6805\n",
      "Epoch 2, train_loss: 0.6594, val_loss: 0.6765\n",
      "Epoch 3, train_loss: 0.6587, val_loss: 0.6697\n",
      "Epoch 4, train_loss: 0.6532, val_loss: 0.6635\n",
      "Epoch 5, train_loss: 0.6430, val_loss: 0.6577\n",
      "Epoch 6, train_loss: 0.6317, val_loss: 0.6530\n",
      "Epoch 7, train_loss: 0.6257, val_loss: 0.6500\n",
      "Epoch 8, train_loss: 0.6240, val_loss: 0.6471\n",
      "Epoch 9, train_loss: 0.6130, val_loss: 0.6428\n",
      "Epoch 10, train_loss: 0.6018, val_loss: 0.6400\n",
      "Epoch 11, train_loss: 0.6159, val_loss: 0.6358\n",
      "Epoch 12, train_loss: 0.5941, val_loss: 0.6334\n",
      "Epoch 13, train_loss: 0.6073, val_loss: 0.6295\n",
      "Epoch 14, train_loss: 0.5833, val_loss: 0.6275\n",
      "Epoch 15, train_loss: 0.5935, val_loss: 0.6225\n",
      "Epoch 16, train_loss: 0.5712, val_loss: 0.6203\n",
      "Epoch 17, train_loss: 0.5650, val_loss: 0.6169\n",
      "Epoch 18, train_loss: 0.5572, val_loss: 0.6147\n",
      "Epoch 19, train_loss: 0.5681, val_loss: 0.6125\n",
      "Epoch 20, train_loss: 0.5475, val_loss: 0.6089\n",
      "Epoch 21, train_loss: 0.5515, val_loss: 0.6056\n",
      "Epoch 22, train_loss: 0.5502, val_loss: 0.6028\n",
      "Epoch 23, train_loss: 0.5434, val_loss: 0.6000\n",
      "Epoch 24, train_loss: 0.5520, val_loss: 0.5968\n",
      "Epoch 25, train_loss: 0.5241, val_loss: 0.5938\n",
      "Epoch 26, train_loss: 0.5314, val_loss: 0.5923\n",
      "Epoch 27, train_loss: 0.5148, val_loss: 0.5891\n",
      "Epoch 28, train_loss: 0.4956, val_loss: 0.5860\n",
      "Epoch 29, train_loss: 0.5101, val_loss: 0.5817\n",
      "Epoch 30, train_loss: 0.4939, val_loss: 0.5792\n",
      "Epoch 31, train_loss: 0.4907, val_loss: 0.5760\n",
      "Epoch 32, train_loss: 0.4849, val_loss: 0.5707\n",
      "Epoch 33, train_loss: 0.4611, val_loss: 0.5672\n",
      "Epoch 34, train_loss: 0.4584, val_loss: 0.5643\n",
      "Epoch 35, train_loss: 0.4852, val_loss: 0.5608\n",
      "Epoch 36, train_loss: 0.4546, val_loss: 0.5570\n",
      "Epoch 37, train_loss: 0.4506, val_loss: 0.5525\n",
      "Epoch 38, train_loss: 0.4330, val_loss: 0.5485\n",
      "Epoch 39, train_loss: 0.4342, val_loss: 0.5451\n",
      "Epoch 40, train_loss: 0.4195, val_loss: 0.5420\n",
      "Epoch 41, train_loss: 0.4141, val_loss: 0.5392\n",
      "Epoch 42, train_loss: 0.4157, val_loss: 0.5368\n",
      "Epoch 43, train_loss: 0.4087, val_loss: 0.5333\n",
      "Epoch 44, train_loss: 0.4019, val_loss: 0.5308\n",
      "Epoch 45, train_loss: 0.3840, val_loss: 0.5281\n",
      "Epoch 46, train_loss: 0.3844, val_loss: 0.5249\n",
      "Epoch 47, train_loss: 0.4037, val_loss: 0.5232\n",
      "Epoch 48, train_loss: 0.3671, val_loss: 0.5179\n",
      "Epoch 49, train_loss: 0.3588, val_loss: 0.5152\n",
      "Epoch 50, train_loss: 0.3546, val_loss: 0.5134\n",
      "Epoch 51, train_loss: 0.3654, val_loss: 0.5113\n",
      "Epoch 52, train_loss: 0.3266, val_loss: 0.5094\n",
      "Epoch 53, train_loss: 0.3226, val_loss: 0.5062\n",
      "Epoch 54, train_loss: 0.3300, val_loss: 0.5052\n",
      "Epoch 55, train_loss: 0.3175, val_loss: 0.5034\n",
      "Epoch 56, train_loss: 0.3236, val_loss: 0.5023\n",
      "Epoch 57, train_loss: 0.3046, val_loss: 0.5009\n",
      "Epoch 58, train_loss: 0.2996, val_loss: 0.4984\n",
      "Epoch 59, train_loss: 0.2805, val_loss: 0.4950\n",
      "Epoch 60, train_loss: 0.2809, val_loss: 0.4916\n",
      "Epoch 61, train_loss: 0.2700, val_loss: 0.4886\n",
      "Epoch 62, train_loss: 0.2638, val_loss: 0.4874\n",
      "Epoch 63, train_loss: 0.2582, val_loss: 0.4889\n",
      "Epoch 64, train_loss: 0.2546, val_loss: 0.4865\n",
      "Epoch 65, train_loss: 0.2464, val_loss: 0.4829\n",
      "Epoch 66, train_loss: 0.2580, val_loss: 0.4816\n",
      "Epoch 67, train_loss: 0.2306, val_loss: 0.4790\n",
      "Epoch 68, train_loss: 0.2381, val_loss: 0.4780\n",
      "Epoch 69, train_loss: 0.2393, val_loss: 0.4757\n",
      "Epoch 70, train_loss: 0.2431, val_loss: 0.4738\n",
      "Epoch 71, train_loss: 0.2303, val_loss: 0.4718\n",
      "Epoch 72, train_loss: 0.2191, val_loss: 0.4702\n",
      "Epoch 73, train_loss: 0.2161, val_loss: 0.4732\n",
      "Epoch 74, train_loss: 0.2099, val_loss: 0.4723\n",
      "Epoch 75, train_loss: 0.2070, val_loss: 0.4724\n",
      "Epoch 76, train_loss: 0.2158, val_loss: 0.4712\n",
      "Epoch 77, train_loss: 0.2098, val_loss: 0.4683\n",
      "Epoch 78, train_loss: 0.2041, val_loss: 0.4687\n",
      "Epoch 79, train_loss: 0.1960, val_loss: 0.4662\n",
      "Epoch 80, train_loss: 0.1904, val_loss: 0.4654\n",
      "Epoch 81, train_loss: 0.2111, val_loss: 0.4634\n",
      "Epoch 82, train_loss: 0.1972, val_loss: 0.4610\n",
      "Epoch 83, train_loss: 0.1869, val_loss: 0.4611\n",
      "Epoch 84, train_loss: 0.1853, val_loss: 0.4611\n",
      "Epoch 85, train_loss: 0.2039, val_loss: 0.4608\n",
      "Epoch 86, train_loss: 0.2045, val_loss: 0.4472\n",
      "Epoch 87, train_loss: 0.1728, val_loss: 0.4459\n",
      "Epoch 88, train_loss: 0.1821, val_loss: 0.4450\n",
      "Epoch 89, train_loss: 0.1841, val_loss: 0.4488\n",
      "Epoch 90, train_loss: 0.1739, val_loss: 0.4498\n",
      "Epoch 91, train_loss: 0.1591, val_loss: 0.4505\n",
      "Epoch 92, train_loss: 0.1614, val_loss: 0.4505\n",
      "Epoch 93, train_loss: 0.1610, val_loss: 0.4539\n",
      "Epoch 94, train_loss: 0.1561, val_loss: 0.4550\n",
      "Epoch 95, train_loss: 0.1681, val_loss: 0.4551\n",
      "Epoch 96, train_loss: 0.1757, val_loss: 0.4564\n",
      "Epoch 97, train_loss: 0.1587, val_loss: 0.4548\n",
      "Epoch 98, train_loss: 0.1520, val_loss: 0.4566\n",
      "Epoch 99, train_loss: 0.1584, val_loss: 0.4556\n",
      "Test the model on fold 1:\n",
      "{'roc_auc': 0.7419871794871795, 'pr_auc': 0.8037486050834394, 'fmax': 0.7142807398305618}\n",
      "Starting Fold: 2\n",
      "Epoch 0, train_loss: 0.6981, val_loss: 0.6941\n",
      "Epoch 1, train_loss: 0.6811, val_loss: 0.6900\n",
      "Epoch 2, train_loss: 0.6650, val_loss: 0.6870\n",
      "Epoch 3, train_loss: 0.6522, val_loss: 0.6846\n",
      "Epoch 4, train_loss: 0.6517, val_loss: 0.6814\n",
      "Epoch 5, train_loss: 0.6443, val_loss: 0.6821\n",
      "Epoch 6, train_loss: 0.6313, val_loss: 0.6810\n",
      "Epoch 7, train_loss: 0.6289, val_loss: 0.6793\n",
      "Epoch 8, train_loss: 0.6194, val_loss: 0.6789\n",
      "Epoch 9, train_loss: 0.6212, val_loss: 0.6786\n",
      "Epoch 10, train_loss: 0.6168, val_loss: 0.6778\n",
      "Epoch 11, train_loss: 0.6115, val_loss: 0.6756\n",
      "Epoch 12, train_loss: 0.5904, val_loss: 0.6744\n",
      "Epoch 13, train_loss: 0.6043, val_loss: 0.6729\n",
      "Epoch 14, train_loss: 0.5896, val_loss: 0.6702\n",
      "Epoch 15, train_loss: 0.5838, val_loss: 0.6697\n",
      "Epoch 16, train_loss: 0.5758, val_loss: 0.6664\n",
      "Epoch 17, train_loss: 0.5748, val_loss: 0.6639\n",
      "Epoch 18, train_loss: 0.5605, val_loss: 0.6616\n",
      "Epoch 19, train_loss: 0.5546, val_loss: 0.6602\n",
      "Epoch 20, train_loss: 0.5438, val_loss: 0.6592\n",
      "Epoch 21, train_loss: 0.5525, val_loss: 0.6571\n",
      "Epoch 22, train_loss: 0.5422, val_loss: 0.6548\n",
      "Epoch 23, train_loss: 0.5412, val_loss: 0.6529\n",
      "Epoch 24, train_loss: 0.5257, val_loss: 0.6500\n",
      "Epoch 25, train_loss: 0.5107, val_loss: 0.6481\n",
      "Epoch 26, train_loss: 0.5056, val_loss: 0.6447\n",
      "Epoch 27, train_loss: 0.5057, val_loss: 0.6432\n",
      "Epoch 28, train_loss: 0.4896, val_loss: 0.6406\n",
      "Epoch 29, train_loss: 0.4996, val_loss: 0.6363\n",
      "Epoch 30, train_loss: 0.4739, val_loss: 0.6336\n",
      "Epoch 31, train_loss: 0.4741, val_loss: 0.6308\n",
      "Epoch 32, train_loss: 0.4688, val_loss: 0.6286\n",
      "Epoch 33, train_loss: 0.4724, val_loss: 0.6273\n",
      "Epoch 34, train_loss: 0.4696, val_loss: 0.6236\n",
      "Epoch 35, train_loss: 0.4489, val_loss: 0.6207\n",
      "Epoch 36, train_loss: 0.4396, val_loss: 0.6183\n",
      "Epoch 37, train_loss: 0.4364, val_loss: 0.6142\n",
      "Epoch 38, train_loss: 0.4233, val_loss: 0.6124\n",
      "Epoch 39, train_loss: 0.4122, val_loss: 0.6099\n",
      "Epoch 40, train_loss: 0.4121, val_loss: 0.6083\n",
      "Epoch 41, train_loss: 0.4020, val_loss: 0.6056\n",
      "Epoch 42, train_loss: 0.3797, val_loss: 0.6032\n",
      "Epoch 43, train_loss: 0.3774, val_loss: 0.6026\n",
      "Epoch 44, train_loss: 0.3822, val_loss: 0.5989\n",
      "Epoch 45, train_loss: 0.3591, val_loss: 0.5974\n",
      "Epoch 46, train_loss: 0.3642, val_loss: 0.5953\n",
      "Epoch 47, train_loss: 0.3553, val_loss: 0.5938\n",
      "Epoch 48, train_loss: 0.3528, val_loss: 0.5864\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49, train_loss: 0.3405, val_loss: 0.5825\n",
      "Epoch 50, train_loss: 0.3352, val_loss: 0.5830\n",
      "Epoch 51, train_loss: 0.3068, val_loss: 0.5814\n",
      "Epoch 52, train_loss: 0.3147, val_loss: 0.5792\n",
      "Epoch 53, train_loss: 0.2930, val_loss: 0.5769\n",
      "Epoch 54, train_loss: 0.3012, val_loss: 0.5749\n",
      "Epoch 55, train_loss: 0.2937, val_loss: 0.5732\n",
      "Epoch 56, train_loss: 0.2890, val_loss: 0.5717\n",
      "Epoch 57, train_loss: 0.2812, val_loss: 0.5700\n",
      "Epoch 58, train_loss: 0.2817, val_loss: 0.5678\n",
      "Epoch 59, train_loss: 0.2705, val_loss: 0.5657\n",
      "Epoch 60, train_loss: 0.2564, val_loss: 0.5630\n",
      "Epoch 61, train_loss: 0.2446, val_loss: 0.5606\n",
      "Epoch 62, train_loss: 0.2488, val_loss: 0.5579\n",
      "Epoch 63, train_loss: 0.2608, val_loss: 0.5562\n",
      "Epoch 64, train_loss: 0.2268, val_loss: 0.5572\n",
      "Epoch 65, train_loss: 0.2149, val_loss: 0.5572\n",
      "Epoch 66, train_loss: 0.2247, val_loss: 0.5544\n",
      "Epoch 67, train_loss: 0.2335, val_loss: 0.5523\n",
      "Epoch 68, train_loss: 0.2438, val_loss: 0.5532\n",
      "Epoch 69, train_loss: 0.2040, val_loss: 0.5511\n",
      "Epoch 70, train_loss: 0.2162, val_loss: 0.5525\n",
      "Epoch 71, train_loss: 0.2008, val_loss: 0.5489\n",
      "Epoch 72, train_loss: 0.2024, val_loss: 0.5464\n",
      "Epoch 73, train_loss: 0.2200, val_loss: 0.5462\n",
      "Epoch 74, train_loss: 0.2038, val_loss: 0.5472\n",
      "Epoch 75, train_loss: 0.1838, val_loss: 0.5472\n",
      "Epoch 76, train_loss: 0.1898, val_loss: 0.5473\n",
      "Epoch 77, train_loss: 0.1839, val_loss: 0.5441\n",
      "Epoch 78, train_loss: 0.1799, val_loss: 0.5434\n",
      "Epoch 79, train_loss: 0.1815, val_loss: 0.5415\n",
      "Epoch 80, train_loss: 0.1769, val_loss: 0.5397\n",
      "Epoch 81, train_loss: 0.1635, val_loss: 0.5391\n",
      "Epoch 82, train_loss: 0.1787, val_loss: 0.5395\n",
      "Epoch 83, train_loss: 0.1564, val_loss: 0.5404\n",
      "Epoch 84, train_loss: 0.1534, val_loss: 0.5393\n",
      "Epoch 85, train_loss: 0.1621, val_loss: 0.5380\n",
      "Epoch 86, train_loss: 0.1610, val_loss: 0.5383\n",
      "Epoch 87, train_loss: 0.1542, val_loss: 0.5384\n",
      "Epoch 88, train_loss: 0.1489, val_loss: 0.5383\n",
      "Epoch 89, train_loss: 0.1498, val_loss: 0.5355\n",
      "Epoch 90, train_loss: 0.1601, val_loss: 0.5339\n",
      "Epoch 91, train_loss: 0.1470, val_loss: 0.5348\n",
      "Epoch 92, train_loss: 0.1523, val_loss: 0.5344\n",
      "Epoch 93, train_loss: 0.1447, val_loss: 0.5335\n",
      "Epoch 94, train_loss: 0.1456, val_loss: 0.5331\n",
      "Epoch 95, train_loss: 0.1329, val_loss: 0.5342\n",
      "Epoch 96, train_loss: 0.1422, val_loss: 0.5335\n",
      "Epoch 97, train_loss: 0.1367, val_loss: 0.5332\n",
      "Epoch 98, train_loss: 0.1322, val_loss: 0.5322\n",
      "Epoch 99, train_loss: 0.1321, val_loss: 0.5315\n",
      "Test the model on fold 2:\n",
      "{'roc_auc': 0.7980769230769231, 'pr_auc': 0.8505879622070736, 'fmax': 0.7727223140814069}\n",
      "Starting Fold: 3\n",
      "Epoch 0, train_loss: 0.6945, val_loss: 0.6802\n",
      "Epoch 1, train_loss: 0.6749, val_loss: 0.6732\n",
      "Epoch 2, train_loss: 0.6649, val_loss: 0.6684\n",
      "Epoch 3, train_loss: 0.6536, val_loss: 0.6628\n",
      "Epoch 4, train_loss: 0.6461, val_loss: 0.6568\n",
      "Epoch 5, train_loss: 0.6439, val_loss: 0.6541\n",
      "Epoch 6, train_loss: 0.6319, val_loss: 0.6504\n",
      "Epoch 7, train_loss: 0.6350, val_loss: 0.6466\n",
      "Epoch 8, train_loss: 0.6170, val_loss: 0.6436\n",
      "Epoch 9, train_loss: 0.6298, val_loss: 0.6427\n",
      "Epoch 10, train_loss: 0.6240, val_loss: 0.6402\n",
      "Epoch 11, train_loss: 0.6078, val_loss: 0.6379\n",
      "Epoch 12, train_loss: 0.6037, val_loss: 0.6348\n",
      "Epoch 13, train_loss: 0.5986, val_loss: 0.6326\n",
      "Epoch 14, train_loss: 0.6014, val_loss: 0.6263\n",
      "Epoch 15, train_loss: 0.5933, val_loss: 0.6239\n",
      "Epoch 16, train_loss: 0.5853, val_loss: 0.6226\n",
      "Epoch 17, train_loss: 0.5813, val_loss: 0.6194\n",
      "Epoch 18, train_loss: 0.5802, val_loss: 0.6171\n",
      "Epoch 19, train_loss: 0.5767, val_loss: 0.6147\n",
      "Epoch 20, train_loss: 0.5692, val_loss: 0.6141\n",
      "Epoch 21, train_loss: 0.5628, val_loss: 0.6134\n",
      "Epoch 22, train_loss: 0.5540, val_loss: 0.6112\n",
      "Epoch 23, train_loss: 0.5459, val_loss: 0.6087\n",
      "Epoch 24, train_loss: 0.5340, val_loss: 0.6061\n",
      "Epoch 25, train_loss: 0.5388, val_loss: 0.6018\n",
      "Epoch 26, train_loss: 0.5336, val_loss: 0.5986\n",
      "Epoch 27, train_loss: 0.5274, val_loss: 0.5953\n",
      "Epoch 28, train_loss: 0.5211, val_loss: 0.5926\n",
      "Epoch 29, train_loss: 0.5084, val_loss: 0.5903\n",
      "Epoch 30, train_loss: 0.4998, val_loss: 0.5874\n",
      "Epoch 31, train_loss: 0.5062, val_loss: 0.5849\n",
      "Epoch 32, train_loss: 0.4940, val_loss: 0.5818\n",
      "Epoch 33, train_loss: 0.4918, val_loss: 0.5786\n",
      "Epoch 34, train_loss: 0.4734, val_loss: 0.5760\n",
      "Epoch 35, train_loss: 0.4655, val_loss: 0.5731\n",
      "Epoch 36, train_loss: 0.4594, val_loss: 0.5694\n",
      "Epoch 37, train_loss: 0.4579, val_loss: 0.5664\n",
      "Epoch 38, train_loss: 0.4566, val_loss: 0.5629\n",
      "Epoch 39, train_loss: 0.4372, val_loss: 0.5604\n",
      "Epoch 40, train_loss: 0.4246, val_loss: 0.5578\n",
      "Epoch 41, train_loss: 0.4349, val_loss: 0.5556\n",
      "Epoch 42, train_loss: 0.4161, val_loss: 0.5527\n",
      "Epoch 43, train_loss: 0.3988, val_loss: 0.5489\n",
      "Epoch 44, train_loss: 0.4064, val_loss: 0.5444\n",
      "Epoch 45, train_loss: 0.4129, val_loss: 0.5402\n",
      "Epoch 46, train_loss: 0.3888, val_loss: 0.5377\n",
      "Epoch 47, train_loss: 0.3925, val_loss: 0.5353\n",
      "Epoch 48, train_loss: 0.3854, val_loss: 0.5305\n",
      "Epoch 49, train_loss: 0.3658, val_loss: 0.5282\n",
      "Epoch 50, train_loss: 0.3555, val_loss: 0.5259\n",
      "Epoch 51, train_loss: 0.3581, val_loss: 0.5230\n",
      "Epoch 52, train_loss: 0.3414, val_loss: 0.5182\n",
      "Epoch 53, train_loss: 0.3390, val_loss: 0.5152\n",
      "Epoch 54, train_loss: 0.3332, val_loss: 0.5131\n",
      "Epoch 55, train_loss: 0.3273, val_loss: 0.5104\n",
      "Epoch 56, train_loss: 0.3134, val_loss: 0.5063\n",
      "Epoch 57, train_loss: 0.2997, val_loss: 0.5020\n",
      "Epoch 58, train_loss: 0.3013, val_loss: 0.4990\n",
      "Epoch 59, train_loss: 0.3023, val_loss: 0.4963\n",
      "Epoch 60, train_loss: 0.2939, val_loss: 0.4918\n",
      "Epoch 61, train_loss: 0.2789, val_loss: 0.4892\n",
      "Epoch 62, train_loss: 0.2836, val_loss: 0.4885\n",
      "Epoch 63, train_loss: 0.2599, val_loss: 0.4873\n",
      "Epoch 64, train_loss: 0.2511, val_loss: 0.4828\n",
      "Epoch 65, train_loss: 0.2531, val_loss: 0.4816\n",
      "Epoch 66, train_loss: 0.2386, val_loss: 0.4764\n",
      "Epoch 67, train_loss: 0.2433, val_loss: 0.4757\n",
      "Epoch 68, train_loss: 0.2394, val_loss: 0.4744\n",
      "Epoch 69, train_loss: 0.2482, val_loss: 0.4699\n",
      "Epoch 70, train_loss: 0.2340, val_loss: 0.4694\n",
      "Epoch 71, train_loss: 0.2412, val_loss: 0.4675\n",
      "Epoch 72, train_loss: 0.2291, val_loss: 0.4651\n",
      "Epoch 73, train_loss: 0.2113, val_loss: 0.4649\n",
      "Epoch 74, train_loss: 0.2126, val_loss: 0.4657\n",
      "Epoch 75, train_loss: 0.2137, val_loss: 0.4618\n",
      "Epoch 76, train_loss: 0.2004, val_loss: 0.4594\n",
      "Epoch 77, train_loss: 0.1912, val_loss: 0.4589\n",
      "Epoch 78, train_loss: 0.1864, val_loss: 0.4586\n",
      "Epoch 79, train_loss: 0.1911, val_loss: 0.4565\n",
      "Epoch 80, train_loss: 0.1886, val_loss: 0.4553\n",
      "Epoch 81, train_loss: 0.1819, val_loss: 0.4557\n",
      "Epoch 82, train_loss: 0.1865, val_loss: 0.4563\n",
      "Epoch 83, train_loss: 0.1677, val_loss: 0.4516\n",
      "Epoch 84, train_loss: 0.1760, val_loss: 0.4507\n",
      "Epoch 85, train_loss: 0.1774, val_loss: 0.4492\n",
      "Epoch 86, train_loss: 0.1596, val_loss: 0.4474\n",
      "Epoch 87, train_loss: 0.1857, val_loss: 0.4479\n",
      "Epoch 88, train_loss: 0.1696, val_loss: 0.4463\n",
      "Epoch 89, train_loss: 0.1667, val_loss: 0.4445\n",
      "Epoch 90, train_loss: 0.1605, val_loss: 0.4459\n",
      "Epoch 91, train_loss: 0.1677, val_loss: 0.4423\n",
      "Epoch 92, train_loss: 0.1511, val_loss: 0.4398\n",
      "Epoch 93, train_loss: 0.1527, val_loss: 0.4395\n",
      "Epoch 94, train_loss: 0.1578, val_loss: 0.4361\n",
      "Epoch 95, train_loss: 0.1578, val_loss: 0.4363\n",
      "Epoch 96, train_loss: 0.1423, val_loss: 0.4375\n",
      "Epoch 97, train_loss: 0.1491, val_loss: 0.4401\n",
      "Epoch 98, train_loss: 0.1457, val_loss: 0.4317\n",
      "Epoch 99, train_loss: 0.1503, val_loss: 0.4325\n",
      "Test the model on fold 3:\n",
      "{'roc_auc': 0.8616666666666666, 'pr_auc': 0.8538636061436368, 'fmax': 0.7999952000287999}\n",
      "Starting Fold: 4\n",
      "Epoch 0, train_loss: 0.6938, val_loss: 0.6818\n",
      "Epoch 1, train_loss: 0.6885, val_loss: 0.6766\n",
      "Epoch 2, train_loss: 0.6746, val_loss: 0.6687\n",
      "Epoch 3, train_loss: 0.6659, val_loss: 0.6628\n",
      "Epoch 4, train_loss: 0.6556, val_loss: 0.6576\n",
      "Epoch 5, train_loss: 0.6491, val_loss: 0.6542\n",
      "Epoch 6, train_loss: 0.6346, val_loss: 0.6525\n",
      "Epoch 7, train_loss: 0.6386, val_loss: 0.6497\n",
      "Epoch 8, train_loss: 0.6298, val_loss: 0.6463\n",
      "Epoch 9, train_loss: 0.6264, val_loss: 0.6439\n",
      "Epoch 10, train_loss: 0.6175, val_loss: 0.6409\n",
      "Epoch 11, train_loss: 0.6198, val_loss: 0.6373\n",
      "Epoch 12, train_loss: 0.6042, val_loss: 0.6341\n",
      "Epoch 13, train_loss: 0.6028, val_loss: 0.6299\n",
      "Epoch 14, train_loss: 0.5891, val_loss: 0.6266\n",
      "Epoch 15, train_loss: 0.5914, val_loss: 0.6238\n",
      "Epoch 16, train_loss: 0.5803, val_loss: 0.6206\n",
      "Epoch 17, train_loss: 0.5801, val_loss: 0.6177\n",
      "Epoch 18, train_loss: 0.5833, val_loss: 0.6139\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19, train_loss: 0.5755, val_loss: 0.6109\n",
      "Epoch 20, train_loss: 0.5698, val_loss: 0.6083\n",
      "Epoch 21, train_loss: 0.5681, val_loss: 0.6044\n",
      "Epoch 22, train_loss: 0.5577, val_loss: 0.6002\n",
      "Epoch 23, train_loss: 0.5448, val_loss: 0.5955\n",
      "Epoch 24, train_loss: 0.5444, val_loss: 0.5920\n",
      "Epoch 25, train_loss: 0.5509, val_loss: 0.5899\n",
      "Epoch 26, train_loss: 0.5294, val_loss: 0.5869\n",
      "Epoch 27, train_loss: 0.5322, val_loss: 0.5839\n",
      "Epoch 28, train_loss: 0.5148, val_loss: 0.5800\n",
      "Epoch 29, train_loss: 0.5199, val_loss: 0.5743\n",
      "Epoch 30, train_loss: 0.5104, val_loss: 0.5699\n",
      "Epoch 31, train_loss: 0.4916, val_loss: 0.5656\n",
      "Epoch 32, train_loss: 0.4891, val_loss: 0.5601\n",
      "Epoch 33, train_loss: 0.4647, val_loss: 0.5563\n",
      "Epoch 34, train_loss: 0.4710, val_loss: 0.5521\n",
      "Epoch 35, train_loss: 0.4650, val_loss: 0.5466\n",
      "Epoch 36, train_loss: 0.4689, val_loss: 0.5440\n",
      "Epoch 37, train_loss: 0.4494, val_loss: 0.5392\n",
      "Epoch 38, train_loss: 0.4435, val_loss: 0.5338\n",
      "Epoch 39, train_loss: 0.4235, val_loss: 0.5289\n",
      "Epoch 40, train_loss: 0.4276, val_loss: 0.5256\n",
      "Epoch 41, train_loss: 0.4303, val_loss: 0.5222\n",
      "Epoch 42, train_loss: 0.4109, val_loss: 0.5179\n",
      "Epoch 43, train_loss: 0.4028, val_loss: 0.5144\n",
      "Epoch 44, train_loss: 0.4086, val_loss: 0.5094\n",
      "Epoch 45, train_loss: 0.3810, val_loss: 0.5062\n",
      "Epoch 46, train_loss: 0.3829, val_loss: 0.5011\n",
      "Epoch 47, train_loss: 0.3775, val_loss: 0.4976\n",
      "Epoch 48, train_loss: 0.3484, val_loss: 0.4933\n",
      "Epoch 49, train_loss: 0.3584, val_loss: 0.4871\n",
      "Epoch 50, train_loss: 0.3523, val_loss: 0.4832\n",
      "Epoch 51, train_loss: 0.3386, val_loss: 0.4785\n",
      "Epoch 52, train_loss: 0.3220, val_loss: 0.4727\n",
      "Epoch 53, train_loss: 0.3288, val_loss: 0.4677\n",
      "Epoch 54, train_loss: 0.3068, val_loss: 0.4646\n",
      "Epoch 55, train_loss: 0.3109, val_loss: 0.4612\n",
      "Epoch 56, train_loss: 0.2949, val_loss: 0.4583\n",
      "Epoch 57, train_loss: 0.2874, val_loss: 0.4554\n",
      "Epoch 58, train_loss: 0.2776, val_loss: 0.4530\n",
      "Epoch 59, train_loss: 0.2795, val_loss: 0.4510\n",
      "Epoch 60, train_loss: 0.2706, val_loss: 0.4474\n",
      "Epoch 61, train_loss: 0.2615, val_loss: 0.4439\n",
      "Epoch 62, train_loss: 0.2731, val_loss: 0.4412\n",
      "Epoch 63, train_loss: 0.2521, val_loss: 0.4397\n",
      "Epoch 64, train_loss: 0.2396, val_loss: 0.4364\n",
      "Epoch 65, train_loss: 0.2472, val_loss: 0.4345\n",
      "Epoch 66, train_loss: 0.2442, val_loss: 0.4328\n",
      "Epoch 67, train_loss: 0.2364, val_loss: 0.4293\n",
      "Epoch 68, train_loss: 0.2264, val_loss: 0.4275\n",
      "Epoch 69, train_loss: 0.2147, val_loss: 0.4265\n",
      "Epoch 70, train_loss: 0.2125, val_loss: 0.4239\n",
      "Epoch 71, train_loss: 0.2055, val_loss: 0.4239\n",
      "Epoch 72, train_loss: 0.2082, val_loss: 0.4222\n",
      "Epoch 73, train_loss: 0.2019, val_loss: 0.4208\n",
      "Epoch 74, train_loss: 0.2068, val_loss: 0.4190\n",
      "Epoch 75, train_loss: 0.1922, val_loss: 0.4184\n",
      "Epoch 76, train_loss: 0.1901, val_loss: 0.4177\n",
      "Epoch 77, train_loss: 0.1911, val_loss: 0.4163\n",
      "Epoch 78, train_loss: 0.1973, val_loss: 0.4152\n",
      "Epoch 79, train_loss: 0.1814, val_loss: 0.4146\n",
      "Epoch 80, train_loss: 0.1727, val_loss: 0.4128\n",
      "Epoch 81, train_loss: 0.1738, val_loss: 0.4133\n",
      "Epoch 82, train_loss: 0.1867, val_loss: 0.4131\n",
      "Epoch 83, train_loss: 0.1746, val_loss: 0.4111\n",
      "Epoch 84, train_loss: 0.1683, val_loss: 0.4100\n",
      "Epoch 85, train_loss: 0.1666, val_loss: 0.4102\n",
      "Epoch 86, train_loss: 0.1550, val_loss: 0.4085\n",
      "Epoch 87, train_loss: 0.1559, val_loss: 0.4074\n",
      "Epoch 88, train_loss: 0.1577, val_loss: 0.4062\n",
      "Epoch 89, train_loss: 0.1684, val_loss: 0.4059\n",
      "Epoch 90, train_loss: 0.1644, val_loss: 0.4051\n",
      "Epoch 91, train_loss: 0.1534, val_loss: 0.4041\n",
      "Epoch 92, train_loss: 0.1486, val_loss: 0.4038\n",
      "Epoch 93, train_loss: 0.1626, val_loss: 0.4038\n",
      "Epoch 94, train_loss: 0.1493, val_loss: 0.4038\n",
      "Epoch 95, train_loss: 0.1649, val_loss: 0.4039\n",
      "Epoch 96, train_loss: 0.1543, val_loss: 0.4031\n",
      "Epoch 97, train_loss: 0.1428, val_loss: 0.4019\n",
      "Epoch 98, train_loss: 0.1421, val_loss: 0.4032\n",
      "Epoch 99, train_loss: 0.1426, val_loss: 0.4036\n",
      "Test the model on fold 4:\n",
      "{'roc_auc': 0.8099999999999999, 'pr_auc': 0.8562124807319175, 'fmax': 0.8181768595341847}\n",
      "Evaluate pretrained model on disease class Skeletal (4/12)\n",
      "Starting Fold: 0\n",
      "Epoch 0, train_loss: 0.6864, val_loss: 0.6827\n",
      "Epoch 1, train_loss: 0.6720, val_loss: 0.6696\n",
      "Epoch 2, train_loss: 0.6694, val_loss: 0.6569\n",
      "Epoch 3, train_loss: 0.6577, val_loss: 0.6456\n",
      "Epoch 4, train_loss: 0.6412, val_loss: 0.6350\n",
      "Epoch 5, train_loss: 0.6333, val_loss: 0.6251\n",
      "Epoch 6, train_loss: 0.6195, val_loss: 0.6226\n",
      "Epoch 7, train_loss: 0.6142, val_loss: 0.6189\n",
      "Epoch 8, train_loss: 0.5945, val_loss: 0.6140\n",
      "Epoch 9, train_loss: 0.5874, val_loss: 0.6088\n",
      "Epoch 10, train_loss: 0.5825, val_loss: 0.5998\n",
      "Epoch 11, train_loss: 0.5776, val_loss: 0.5934\n",
      "Epoch 12, train_loss: 0.5691, val_loss: 0.5862\n",
      "Epoch 13, train_loss: 0.5604, val_loss: 0.5826\n",
      "Epoch 14, train_loss: 0.5473, val_loss: 0.5803\n",
      "Epoch 15, train_loss: 0.5442, val_loss: 0.5766\n",
      "Epoch 16, train_loss: 0.5285, val_loss: 0.5723\n",
      "Epoch 17, train_loss: 0.5273, val_loss: 0.5703\n",
      "Epoch 18, train_loss: 0.5193, val_loss: 0.5668\n",
      "Epoch 19, train_loss: 0.5064, val_loss: 0.5664\n",
      "Epoch 20, train_loss: 0.5045, val_loss: 0.5644\n",
      "Epoch 21, train_loss: 0.5038, val_loss: 0.5635\n",
      "Epoch 22, train_loss: 0.4955, val_loss: 0.5632\n",
      "Epoch 23, train_loss: 0.4758, val_loss: 0.5610\n",
      "Epoch 24, train_loss: 0.4610, val_loss: 0.5584\n",
      "Epoch 25, train_loss: 0.4654, val_loss: 0.5543\n",
      "Epoch 26, train_loss: 0.4566, val_loss: 0.5522\n",
      "Epoch 27, train_loss: 0.4514, val_loss: 0.5531\n",
      "Epoch 28, train_loss: 0.4467, val_loss: 0.5494\n",
      "Epoch 29, train_loss: 0.4337, val_loss: 0.5459\n",
      "Epoch 30, train_loss: 0.4305, val_loss: 0.5442\n",
      "Epoch 31, train_loss: 0.4161, val_loss: 0.5429\n",
      "Epoch 32, train_loss: 0.4091, val_loss: 0.5422\n",
      "Epoch 33, train_loss: 0.4060, val_loss: 0.5405\n",
      "Epoch 34, train_loss: 0.4025, val_loss: 0.5387\n",
      "Epoch 35, train_loss: 0.3947, val_loss: 0.5387\n",
      "Epoch 36, train_loss: 0.3857, val_loss: 0.5402\n",
      "Epoch 37, train_loss: 0.3775, val_loss: 0.5420\n",
      "Epoch 38, train_loss: 0.3649, val_loss: 0.5404\n",
      "Epoch 39, train_loss: 0.3672, val_loss: 0.5429\n",
      "Epoch 40, train_loss: 0.3565, val_loss: 0.5432\n",
      "Epoch 41, train_loss: 0.3453, val_loss: 0.5443\n",
      "Epoch 42, train_loss: 0.3420, val_loss: 0.5422\n",
      "Epoch 43, train_loss: 0.3361, val_loss: 0.5422\n",
      "Epoch 44, train_loss: 0.3257, val_loss: 0.5419\n",
      "Epoch 45, train_loss: 0.3158, val_loss: 0.5427\n",
      "Epoch 46, train_loss: 0.3202, val_loss: 0.5404\n",
      "Epoch 47, train_loss: 0.3127, val_loss: 0.5437\n",
      "Epoch 48, train_loss: 0.3080, val_loss: 0.5418\n",
      "Epoch 49, train_loss: 0.3010, val_loss: 0.5443\n",
      "Epoch 50, train_loss: 0.2905, val_loss: 0.5441\n",
      "Epoch 51, train_loss: 0.2759, val_loss: 0.5449\n",
      "Early Stopping!\n",
      "Test the model on fold 0:\n",
      "{'roc_auc': 0.8606811145510835, 'pr_auc': 0.9127230955766104, 'fmax': 0.8484799632971809}\n",
      "Starting Fold: 1\n",
      "Epoch 0, train_loss: 0.6830, val_loss: 0.6879\n",
      "Epoch 1, train_loss: 0.6706, val_loss: 0.6729\n",
      "Epoch 2, train_loss: 0.6580, val_loss: 0.6591\n",
      "Epoch 3, train_loss: 0.6426, val_loss: 0.6472\n",
      "Epoch 4, train_loss: 0.6236, val_loss: 0.6344\n",
      "Epoch 5, train_loss: 0.6102, val_loss: 0.6257\n",
      "Epoch 6, train_loss: 0.6060, val_loss: 0.6154\n",
      "Epoch 7, train_loss: 0.5919, val_loss: 0.6068\n",
      "Epoch 8, train_loss: 0.5871, val_loss: 0.6007\n",
      "Epoch 9, train_loss: 0.5810, val_loss: 0.5946\n",
      "Epoch 10, train_loss: 0.5728, val_loss: 0.5894\n",
      "Epoch 11, train_loss: 0.5596, val_loss: 0.5831\n",
      "Epoch 12, train_loss: 0.5525, val_loss: 0.5774\n",
      "Epoch 13, train_loss: 0.5551, val_loss: 0.5721\n",
      "Epoch 14, train_loss: 0.5428, val_loss: 0.5664\n",
      "Epoch 15, train_loss: 0.5338, val_loss: 0.5603\n",
      "Epoch 16, train_loss: 0.5327, val_loss: 0.5559\n",
      "Epoch 17, train_loss: 0.5338, val_loss: 0.5513\n",
      "Epoch 18, train_loss: 0.5132, val_loss: 0.5469\n",
      "Epoch 19, train_loss: 0.5185, val_loss: 0.5426\n",
      "Epoch 20, train_loss: 0.5106, val_loss: 0.5381\n",
      "Epoch 21, train_loss: 0.4975, val_loss: 0.5340\n",
      "Epoch 22, train_loss: 0.4959, val_loss: 0.5284\n",
      "Epoch 23, train_loss: 0.4923, val_loss: 0.5244\n",
      "Epoch 24, train_loss: 0.4892, val_loss: 0.5202\n",
      "Epoch 25, train_loss: 0.4818, val_loss: 0.5160\n",
      "Epoch 26, train_loss: 0.4791, val_loss: 0.5126\n",
      "Epoch 27, train_loss: 0.4733, val_loss: 0.5098\n",
      "Epoch 28, train_loss: 0.4616, val_loss: 0.5073\n",
      "Epoch 29, train_loss: 0.4626, val_loss: 0.5045\n",
      "Epoch 30, train_loss: 0.4556, val_loss: 0.5008\n",
      "Epoch 31, train_loss: 0.4456, val_loss: 0.4963\n",
      "Epoch 32, train_loss: 0.4406, val_loss: 0.4907\n",
      "Epoch 33, train_loss: 0.4352, val_loss: 0.4857\n",
      "Epoch 34, train_loss: 0.4347, val_loss: 0.4840\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35, train_loss: 0.4340, val_loss: 0.4821\n",
      "Epoch 36, train_loss: 0.4222, val_loss: 0.4780\n",
      "Epoch 37, train_loss: 0.4094, val_loss: 0.4760\n",
      "Epoch 38, train_loss: 0.4146, val_loss: 0.4725\n",
      "Epoch 39, train_loss: 0.4024, val_loss: 0.4684\n",
      "Epoch 40, train_loss: 0.3971, val_loss: 0.4643\n",
      "Epoch 41, train_loss: 0.3904, val_loss: 0.4607\n",
      "Epoch 42, train_loss: 0.3867, val_loss: 0.4574\n",
      "Epoch 43, train_loss: 0.3840, val_loss: 0.4535\n",
      "Epoch 44, train_loss: 0.3685, val_loss: 0.4489\n",
      "Epoch 45, train_loss: 0.3701, val_loss: 0.4451\n",
      "Epoch 46, train_loss: 0.3709, val_loss: 0.4428\n",
      "Epoch 47, train_loss: 0.3604, val_loss: 0.4423\n",
      "Epoch 48, train_loss: 0.3515, val_loss: 0.4422\n",
      "Epoch 49, train_loss: 0.3469, val_loss: 0.4411\n",
      "Epoch 50, train_loss: 0.3494, val_loss: 0.4399\n",
      "Epoch 51, train_loss: 0.3386, val_loss: 0.4376\n",
      "Epoch 52, train_loss: 0.3389, val_loss: 0.4357\n",
      "Epoch 53, train_loss: 0.3240, val_loss: 0.4327\n",
      "Epoch 54, train_loss: 0.3230, val_loss: 0.4287\n",
      "Epoch 55, train_loss: 0.3111, val_loss: 0.4268\n",
      "Epoch 56, train_loss: 0.3114, val_loss: 0.4244\n",
      "Epoch 57, train_loss: 0.2987, val_loss: 0.4220\n",
      "Epoch 58, train_loss: 0.3052, val_loss: 0.4195\n",
      "Epoch 59, train_loss: 0.2924, val_loss: 0.4184\n",
      "Epoch 60, train_loss: 0.2905, val_loss: 0.4173\n",
      "Epoch 61, train_loss: 0.2853, val_loss: 0.4160\n",
      "Epoch 62, train_loss: 0.2826, val_loss: 0.4136\n",
      "Epoch 63, train_loss: 0.2821, val_loss: 0.4111\n",
      "Epoch 64, train_loss: 0.2780, val_loss: 0.4099\n",
      "Epoch 65, train_loss: 0.2690, val_loss: 0.4095\n",
      "Epoch 66, train_loss: 0.2641, val_loss: 0.4083\n",
      "Epoch 67, train_loss: 0.2657, val_loss: 0.4070\n",
      "Epoch 68, train_loss: 0.2557, val_loss: 0.4050\n",
      "Epoch 69, train_loss: 0.2573, val_loss: 0.4043\n",
      "Epoch 70, train_loss: 0.2491, val_loss: 0.4042\n",
      "Epoch 71, train_loss: 0.2513, val_loss: 0.4021\n",
      "Epoch 72, train_loss: 0.2474, val_loss: 0.4016\n",
      "Epoch 73, train_loss: 0.2428, val_loss: 0.4000\n",
      "Epoch 74, train_loss: 0.2382, val_loss: 0.3980\n",
      "Epoch 75, train_loss: 0.2330, val_loss: 0.3971\n",
      "Epoch 76, train_loss: 0.2352, val_loss: 0.3978\n",
      "Epoch 77, train_loss: 0.2263, val_loss: 0.3955\n",
      "Epoch 78, train_loss: 0.2284, val_loss: 0.3935\n",
      "Epoch 79, train_loss: 0.2278, val_loss: 0.3921\n",
      "Epoch 80, train_loss: 0.2251, val_loss: 0.3910\n",
      "Epoch 81, train_loss: 0.2198, val_loss: 0.3923\n",
      "Epoch 82, train_loss: 0.2163, val_loss: 0.3940\n",
      "Epoch 83, train_loss: 0.2099, val_loss: 0.3940\n",
      "Epoch 84, train_loss: 0.2079, val_loss: 0.3923\n",
      "Epoch 85, train_loss: 0.2153, val_loss: 0.3914\n",
      "Epoch 86, train_loss: 0.2083, val_loss: 0.3904\n",
      "Epoch 87, train_loss: 0.2012, val_loss: 0.3890\n",
      "Epoch 88, train_loss: 0.2029, val_loss: 0.3878\n",
      "Epoch 89, train_loss: 0.2008, val_loss: 0.3894\n",
      "Epoch 90, train_loss: 0.1982, val_loss: 0.3883\n",
      "Epoch 91, train_loss: 0.1932, val_loss: 0.3871\n",
      "Epoch 92, train_loss: 0.1917, val_loss: 0.3860\n",
      "Epoch 93, train_loss: 0.1937, val_loss: 0.3856\n",
      "Epoch 94, train_loss: 0.1886, val_loss: 0.3863\n",
      "Epoch 95, train_loss: 0.1817, val_loss: 0.3860\n",
      "Epoch 96, train_loss: 0.1826, val_loss: 0.3852\n",
      "Epoch 97, train_loss: 0.1830, val_loss: 0.3857\n",
      "Epoch 98, train_loss: 0.1763, val_loss: 0.3854\n",
      "Epoch 99, train_loss: 0.1779, val_loss: 0.3847\n",
      "Test the model on fold 1:\n",
      "{'roc_auc': 0.8594771241830065, 'pr_auc': 0.9023870521059103, 'fmax': 0.8387047242748293}\n",
      "Starting Fold: 2\n",
      "Epoch 0, train_loss: 0.6837, val_loss: 0.6944\n",
      "Epoch 1, train_loss: 0.6748, val_loss: 0.6848\n",
      "Epoch 2, train_loss: 0.6449, val_loss: 0.6756\n",
      "Epoch 3, train_loss: 0.6307, val_loss: 0.6669\n",
      "Epoch 4, train_loss: 0.6195, val_loss: 0.6578\n",
      "Epoch 5, train_loss: 0.6014, val_loss: 0.6502\n",
      "Epoch 6, train_loss: 0.5954, val_loss: 0.6458\n",
      "Epoch 7, train_loss: 0.5766, val_loss: 0.6436\n",
      "Epoch 8, train_loss: 0.5703, val_loss: 0.6421\n",
      "Epoch 9, train_loss: 0.5566, val_loss: 0.6409\n",
      "Epoch 10, train_loss: 0.5515, val_loss: 0.6409\n",
      "Epoch 11, train_loss: 0.5374, val_loss: 0.6405\n",
      "Epoch 12, train_loss: 0.5374, val_loss: 0.6383\n",
      "Epoch 13, train_loss: 0.5237, val_loss: 0.6379\n",
      "Epoch 14, train_loss: 0.5099, val_loss: 0.6380\n",
      "Epoch 15, train_loss: 0.5233, val_loss: 0.6392\n",
      "Epoch 16, train_loss: 0.4997, val_loss: 0.6386\n",
      "Epoch 17, train_loss: 0.4981, val_loss: 0.6400\n",
      "Epoch 18, train_loss: 0.4958, val_loss: 0.6397\n",
      "Epoch 19, train_loss: 0.4882, val_loss: 0.6400\n",
      "Epoch 20, train_loss: 0.4755, val_loss: 0.6405\n",
      "Epoch 21, train_loss: 0.4755, val_loss: 0.6416\n",
      "Epoch 22, train_loss: 0.4703, val_loss: 0.6428\n",
      "Epoch 23, train_loss: 0.4657, val_loss: 0.6446\n",
      "Early Stopping!\n",
      "Test the model on fold 2:\n",
      "{'roc_auc': 0.8954248366013071, 'pr_auc': 0.9197324410871823, 'fmax': 0.8648598685460297}\n",
      "Starting Fold: 3\n",
      "Epoch 0, train_loss: 0.6897, val_loss: 0.6810\n",
      "Epoch 1, train_loss: 0.6722, val_loss: 0.6602\n",
      "Epoch 2, train_loss: 0.6549, val_loss: 0.6371\n",
      "Epoch 3, train_loss: 0.6420, val_loss: 0.6214\n",
      "Epoch 4, train_loss: 0.6182, val_loss: 0.6095\n",
      "Epoch 5, train_loss: 0.5993, val_loss: 0.6008\n",
      "Epoch 6, train_loss: 0.6020, val_loss: 0.5937\n",
      "Epoch 7, train_loss: 0.5742, val_loss: 0.5874\n",
      "Epoch 8, train_loss: 0.5651, val_loss: 0.5826\n",
      "Epoch 9, train_loss: 0.5776, val_loss: 0.5774\n",
      "Epoch 10, train_loss: 0.5501, val_loss: 0.5729\n",
      "Epoch 11, train_loss: 0.5527, val_loss: 0.5683\n",
      "Epoch 12, train_loss: 0.5381, val_loss: 0.5653\n",
      "Epoch 13, train_loss: 0.5352, val_loss: 0.5629\n",
      "Epoch 14, train_loss: 0.5233, val_loss: 0.5596\n",
      "Epoch 15, train_loss: 0.5152, val_loss: 0.5587\n",
      "Epoch 16, train_loss: 0.5112, val_loss: 0.5576\n",
      "Epoch 17, train_loss: 0.5037, val_loss: 0.5558\n",
      "Epoch 18, train_loss: 0.4918, val_loss: 0.5549\n",
      "Epoch 19, train_loss: 0.4855, val_loss: 0.5534\n",
      "Epoch 20, train_loss: 0.4833, val_loss: 0.5516\n",
      "Epoch 21, train_loss: 0.4788, val_loss: 0.5505\n",
      "Epoch 22, train_loss: 0.4733, val_loss: 0.5497\n",
      "Epoch 23, train_loss: 0.4643, val_loss: 0.5476\n",
      "Epoch 24, train_loss: 0.4558, val_loss: 0.5473\n",
      "Epoch 25, train_loss: 0.4427, val_loss: 0.5462\n",
      "Epoch 26, train_loss: 0.4339, val_loss: 0.5459\n",
      "Epoch 27, train_loss: 0.4341, val_loss: 0.5457\n",
      "Epoch 28, train_loss: 0.4295, val_loss: 0.5468\n",
      "Epoch 29, train_loss: 0.4135, val_loss: 0.5468\n",
      "Epoch 30, train_loss: 0.4118, val_loss: 0.5463\n",
      "Epoch 31, train_loss: 0.3963, val_loss: 0.5466\n",
      "Epoch 32, train_loss: 0.4002, val_loss: 0.5466\n",
      "Epoch 33, train_loss: 0.3844, val_loss: 0.5477\n",
      "Epoch 34, train_loss: 0.3813, val_loss: 0.5479\n",
      "Epoch 35, train_loss: 0.3725, val_loss: 0.5477\n",
      "Epoch 36, train_loss: 0.3636, val_loss: 0.5480\n",
      "Epoch 37, train_loss: 0.3675, val_loss: 0.5479\n",
      "Epoch 38, train_loss: 0.3530, val_loss: 0.5491\n",
      "Early Stopping!\n",
      "Test the model on fold 3:\n",
      "{'roc_auc': 0.9342105263157894, 'pr_auc': 0.9600139776803863, 'fmax': 0.9189139225983859}\n",
      "Starting Fold: 4\n",
      "Epoch 0, train_loss: 0.6845, val_loss: 0.6876\n",
      "Epoch 1, train_loss: 0.6728, val_loss: 0.6780\n",
      "Epoch 2, train_loss: 0.6446, val_loss: 0.6681\n",
      "Epoch 3, train_loss: 0.6261, val_loss: 0.6590\n",
      "Epoch 4, train_loss: 0.6129, val_loss: 0.6518\n",
      "Epoch 5, train_loss: 0.5956, val_loss: 0.6443\n",
      "Epoch 6, train_loss: 0.5838, val_loss: 0.6369\n",
      "Epoch 7, train_loss: 0.5693, val_loss: 0.6313\n",
      "Epoch 8, train_loss: 0.5602, val_loss: 0.6265\n",
      "Epoch 9, train_loss: 0.5520, val_loss: 0.6211\n",
      "Epoch 10, train_loss: 0.5450, val_loss: 0.6151\n",
      "Epoch 11, train_loss: 0.5339, val_loss: 0.6099\n",
      "Epoch 12, train_loss: 0.5347, val_loss: 0.6055\n",
      "Epoch 13, train_loss: 0.5287, val_loss: 0.6005\n",
      "Epoch 14, train_loss: 0.5153, val_loss: 0.5959\n",
      "Epoch 15, train_loss: 0.5116, val_loss: 0.5905\n",
      "Epoch 16, train_loss: 0.5013, val_loss: 0.5849\n",
      "Epoch 17, train_loss: 0.4963, val_loss: 0.5797\n",
      "Epoch 18, train_loss: 0.4877, val_loss: 0.5747\n",
      "Epoch 19, train_loss: 0.4821, val_loss: 0.5704\n",
      "Epoch 20, train_loss: 0.4772, val_loss: 0.5655\n",
      "Epoch 21, train_loss: 0.4769, val_loss: 0.5604\n",
      "Epoch 22, train_loss: 0.4652, val_loss: 0.5560\n",
      "Epoch 23, train_loss: 0.4666, val_loss: 0.5515\n",
      "Epoch 24, train_loss: 0.4634, val_loss: 0.5478\n",
      "Epoch 25, train_loss: 0.4593, val_loss: 0.5433\n",
      "Epoch 26, train_loss: 0.4444, val_loss: 0.5391\n",
      "Epoch 27, train_loss: 0.4403, val_loss: 0.5350\n",
      "Epoch 28, train_loss: 0.4362, val_loss: 0.5310\n",
      "Epoch 29, train_loss: 0.4315, val_loss: 0.5267\n",
      "Epoch 30, train_loss: 0.4240, val_loss: 0.5224\n",
      "Epoch 31, train_loss: 0.4244, val_loss: 0.5181\n",
      "Epoch 32, train_loss: 0.4155, val_loss: 0.5136\n",
      "Epoch 33, train_loss: 0.4159, val_loss: 0.5096\n",
      "Epoch 34, train_loss: 0.4045, val_loss: 0.5057\n",
      "Epoch 35, train_loss: 0.4076, val_loss: 0.5020\n",
      "Epoch 36, train_loss: 0.3947, val_loss: 0.4989\n",
      "Epoch 37, train_loss: 0.3926, val_loss: 0.4956\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38, train_loss: 0.3851, val_loss: 0.4925\n",
      "Epoch 39, train_loss: 0.3843, val_loss: 0.4894\n",
      "Epoch 40, train_loss: 0.3717, val_loss: 0.4859\n",
      "Epoch 41, train_loss: 0.3721, val_loss: 0.4826\n",
      "Epoch 42, train_loss: 0.3678, val_loss: 0.4791\n",
      "Epoch 43, train_loss: 0.3655, val_loss: 0.4751\n",
      "Epoch 44, train_loss: 0.3544, val_loss: 0.4715\n",
      "Epoch 45, train_loss: 0.3522, val_loss: 0.4680\n",
      "Epoch 46, train_loss: 0.3475, val_loss: 0.4646\n",
      "Epoch 47, train_loss: 0.3374, val_loss: 0.4614\n",
      "Epoch 48, train_loss: 0.3351, val_loss: 0.4579\n",
      "Epoch 49, train_loss: 0.3332, val_loss: 0.4549\n",
      "Epoch 50, train_loss: 0.3291, val_loss: 0.4520\n",
      "Epoch 51, train_loss: 0.3219, val_loss: 0.4487\n",
      "Epoch 52, train_loss: 0.3162, val_loss: 0.4458\n",
      "Epoch 53, train_loss: 0.3114, val_loss: 0.4429\n",
      "Epoch 54, train_loss: 0.3048, val_loss: 0.4406\n",
      "Epoch 55, train_loss: 0.2978, val_loss: 0.4377\n",
      "Epoch 56, train_loss: 0.2990, val_loss: 0.4349\n",
      "Epoch 57, train_loss: 0.2940, val_loss: 0.4316\n",
      "Epoch 58, train_loss: 0.2885, val_loss: 0.4287\n",
      "Epoch 59, train_loss: 0.2805, val_loss: 0.4262\n",
      "Epoch 60, train_loss: 0.2870, val_loss: 0.4234\n",
      "Epoch 61, train_loss: 0.2793, val_loss: 0.4207\n",
      "Epoch 62, train_loss: 0.2721, val_loss: 0.4184\n",
      "Epoch 63, train_loss: 0.2681, val_loss: 0.4162\n",
      "Epoch 64, train_loss: 0.2591, val_loss: 0.4138\n",
      "Epoch 65, train_loss: 0.2511, val_loss: 0.4110\n",
      "Epoch 66, train_loss: 0.2538, val_loss: 0.4091\n",
      "Epoch 67, train_loss: 0.2432, val_loss: 0.4070\n",
      "Epoch 68, train_loss: 0.2396, val_loss: 0.4050\n",
      "Epoch 69, train_loss: 0.2397, val_loss: 0.4031\n",
      "Epoch 70, train_loss: 0.2335, val_loss: 0.4012\n",
      "Epoch 71, train_loss: 0.2348, val_loss: 0.3996\n",
      "Epoch 72, train_loss: 0.2294, val_loss: 0.3979\n",
      "Epoch 73, train_loss: 0.2198, val_loss: 0.3958\n",
      "Epoch 74, train_loss: 0.2216, val_loss: 0.3945\n",
      "Epoch 75, train_loss: 0.2262, val_loss: 0.3926\n",
      "Epoch 76, train_loss: 0.2165, val_loss: 0.3905\n",
      "Epoch 77, train_loss: 0.2153, val_loss: 0.3890\n",
      "Epoch 78, train_loss: 0.2073, val_loss: 0.3877\n",
      "Epoch 79, train_loss: 0.2090, val_loss: 0.3861\n",
      "Epoch 80, train_loss: 0.2002, val_loss: 0.3845\n",
      "Epoch 81, train_loss: 0.2017, val_loss: 0.3837\n",
      "Epoch 82, train_loss: 0.1938, val_loss: 0.3828\n",
      "Epoch 83, train_loss: 0.1970, val_loss: 0.3818\n",
      "Epoch 84, train_loss: 0.1890, val_loss: 0.3803\n",
      "Epoch 85, train_loss: 0.1922, val_loss: 0.3789\n",
      "Epoch 86, train_loss: 0.1876, val_loss: 0.3773\n",
      "Epoch 87, train_loss: 0.1876, val_loss: 0.3761\n",
      "Epoch 88, train_loss: 0.1857, val_loss: 0.3754\n",
      "Epoch 89, train_loss: 0.1754, val_loss: 0.3746\n",
      "Epoch 90, train_loss: 0.1807, val_loss: 0.3737\n",
      "Epoch 91, train_loss: 0.1733, val_loss: 0.3729\n",
      "Epoch 92, train_loss: 0.1727, val_loss: 0.3719\n",
      "Epoch 93, train_loss: 0.1749, val_loss: 0.3712\n",
      "Epoch 94, train_loss: 0.1695, val_loss: 0.3704\n",
      "Epoch 95, train_loss: 0.1652, val_loss: 0.3697\n",
      "Epoch 96, train_loss: 0.1635, val_loss: 0.3693\n",
      "Epoch 97, train_loss: 0.1616, val_loss: 0.3681\n",
      "Epoch 98, train_loss: 0.1700, val_loss: 0.3676\n",
      "Epoch 99, train_loss: 0.1602, val_loss: 0.3667\n",
      "Test the model on fold 4:\n",
      "{'roc_auc': 0.8216666666666667, 'pr_auc': 0.7629618265934885, 'fmax': 0.8275812128719913}\n",
      "Evaluate pretrained model on disease class Metabolic (5/12)\n",
      "Starting Fold: 0\n",
      "Epoch 0, train_loss: 0.6909, val_loss: 0.6696\n",
      "Epoch 1, train_loss: 0.6699, val_loss: 0.6541\n",
      "Epoch 2, train_loss: 0.6595, val_loss: 0.6385\n",
      "Epoch 3, train_loss: 0.6398, val_loss: 0.6255\n",
      "Epoch 4, train_loss: 0.6341, val_loss: 0.6130\n",
      "Epoch 5, train_loss: 0.6193, val_loss: 0.6017\n",
      "Epoch 6, train_loss: 0.6072, val_loss: 0.5906\n",
      "Epoch 7, train_loss: 0.5935, val_loss: 0.5771\n",
      "Epoch 8, train_loss: 0.5815, val_loss: 0.5660\n",
      "Epoch 9, train_loss: 0.5747, val_loss: 0.5553\n",
      "Epoch 10, train_loss: 0.5588, val_loss: 0.5437\n",
      "Epoch 11, train_loss: 0.5474, val_loss: 0.5321\n",
      "Epoch 12, train_loss: 0.5355, val_loss: 0.5228\n",
      "Epoch 13, train_loss: 0.5196, val_loss: 0.5128\n",
      "Epoch 14, train_loss: 0.5080, val_loss: 0.5019\n",
      "Epoch 15, train_loss: 0.4991, val_loss: 0.4914\n",
      "Epoch 16, train_loss: 0.4800, val_loss: 0.4822\n",
      "Epoch 17, train_loss: 0.4777, val_loss: 0.4733\n",
      "Epoch 18, train_loss: 0.4572, val_loss: 0.4665\n",
      "Epoch 19, train_loss: 0.4415, val_loss: 0.4570\n",
      "Epoch 20, train_loss: 0.4291, val_loss: 0.4501\n",
      "Epoch 21, train_loss: 0.4179, val_loss: 0.4415\n",
      "Epoch 22, train_loss: 0.4063, val_loss: 0.4351\n",
      "Epoch 23, train_loss: 0.3878, val_loss: 0.4273\n",
      "Epoch 24, train_loss: 0.3814, val_loss: 0.4212\n",
      "Epoch 25, train_loss: 0.3579, val_loss: 0.4152\n",
      "Epoch 26, train_loss: 0.3575, val_loss: 0.4108\n",
      "Epoch 27, train_loss: 0.3442, val_loss: 0.4061\n",
      "Epoch 28, train_loss: 0.3365, val_loss: 0.3992\n",
      "Epoch 29, train_loss: 0.3229, val_loss: 0.3971\n",
      "Epoch 30, train_loss: 0.3100, val_loss: 0.3915\n",
      "Epoch 31, train_loss: 0.3045, val_loss: 0.3884\n",
      "Epoch 32, train_loss: 0.2867, val_loss: 0.3851\n",
      "Epoch 33, train_loss: 0.2913, val_loss: 0.3810\n",
      "Epoch 34, train_loss: 0.2823, val_loss: 0.3773\n",
      "Epoch 35, train_loss: 0.2710, val_loss: 0.3783\n",
      "Epoch 36, train_loss: 0.2660, val_loss: 0.3761\n",
      "Epoch 37, train_loss: 0.2574, val_loss: 0.3707\n",
      "Epoch 38, train_loss: 0.2499, val_loss: 0.3712\n",
      "Epoch 39, train_loss: 0.2417, val_loss: 0.3676\n",
      "Epoch 40, train_loss: 0.2384, val_loss: 0.3653\n",
      "Epoch 41, train_loss: 0.2360, val_loss: 0.3630\n",
      "Epoch 42, train_loss: 0.2300, val_loss: 0.3631\n",
      "Epoch 43, train_loss: 0.2229, val_loss: 0.3605\n",
      "Epoch 44, train_loss: 0.2208, val_loss: 0.3591\n",
      "Epoch 45, train_loss: 0.2164, val_loss: 0.3596\n",
      "Epoch 46, train_loss: 0.2107, val_loss: 0.3564\n",
      "Epoch 47, train_loss: 0.2100, val_loss: 0.3564\n",
      "Epoch 48, train_loss: 0.2048, val_loss: 0.3541\n",
      "Epoch 49, train_loss: 0.2001, val_loss: 0.3536\n",
      "Epoch 50, train_loss: 0.1968, val_loss: 0.3515\n",
      "Epoch 51, train_loss: 0.1937, val_loss: 0.3499\n",
      "Epoch 52, train_loss: 0.1920, val_loss: 0.3500\n",
      "Epoch 53, train_loss: 0.1864, val_loss: 0.3505\n",
      "Epoch 54, train_loss: 0.1907, val_loss: 0.3498\n",
      "Epoch 55, train_loss: 0.1848, val_loss: 0.3501\n",
      "Epoch 56, train_loss: 0.1792, val_loss: 0.3499\n",
      "Epoch 57, train_loss: 0.1804, val_loss: 0.3471\n",
      "Epoch 58, train_loss: 0.1769, val_loss: 0.3488\n",
      "Epoch 59, train_loss: 0.1754, val_loss: 0.3458\n",
      "Epoch 60, train_loss: 0.1740, val_loss: 0.3450\n",
      "Epoch 61, train_loss: 0.1729, val_loss: 0.3464\n",
      "Epoch 62, train_loss: 0.1720, val_loss: 0.3456\n",
      "Epoch 63, train_loss: 0.1711, val_loss: 0.3467\n",
      "Epoch 64, train_loss: 0.1692, val_loss: 0.3459\n",
      "Epoch 65, train_loss: 0.1708, val_loss: 0.3457\n",
      "Epoch 66, train_loss: 0.1673, val_loss: 0.3450\n",
      "Epoch 67, train_loss: 0.1698, val_loss: 0.3422\n",
      "Epoch 68, train_loss: 0.1685, val_loss: 0.3413\n",
      "Epoch 69, train_loss: 0.1636, val_loss: 0.3417\n",
      "Epoch 70, train_loss: 0.1624, val_loss: 0.3433\n",
      "Epoch 71, train_loss: 0.1586, val_loss: 0.3435\n",
      "Epoch 72, train_loss: 0.1557, val_loss: 0.3427\n",
      "Epoch 73, train_loss: 0.1594, val_loss: 0.3453\n",
      "Epoch 74, train_loss: 0.1627, val_loss: 0.3423\n",
      "Epoch 75, train_loss: 0.1571, val_loss: 0.3388\n",
      "Epoch 76, train_loss: 0.1505, val_loss: 0.3415\n",
      "Epoch 77, train_loss: 0.1548, val_loss: 0.3436\n",
      "Epoch 78, train_loss: 0.1575, val_loss: 0.3420\n",
      "Epoch 79, train_loss: 0.1520, val_loss: 0.3434\n",
      "Epoch 80, train_loss: 0.1511, val_loss: 0.3429\n",
      "Epoch 81, train_loss: 0.1500, val_loss: 0.3415\n",
      "Epoch 82, train_loss: 0.1510, val_loss: 0.3405\n",
      "Epoch 83, train_loss: 0.1511, val_loss: 0.3397\n",
      "Epoch 84, train_loss: 0.1512, val_loss: 0.3429\n",
      "Epoch 85, train_loss: 0.1535, val_loss: 0.3432\n",
      "Epoch 86, train_loss: 0.1474, val_loss: 0.3451\n",
      "Epoch 87, train_loss: 0.1495, val_loss: 0.3421\n",
      "Epoch 88, train_loss: 0.1432, val_loss: 0.3440\n",
      "Epoch 89, train_loss: 0.1443, val_loss: 0.3458\n",
      "Early Stopping!\n",
      "Test the model on fold 0:\n",
      "{'roc_auc': 0.886150234741784, 'pr_auc': 0.852013297014666, 'fmax': 0.8571378875309216}\n",
      "Starting Fold: 1\n",
      "Epoch 0, train_loss: 0.6880, val_loss: 0.6581\n",
      "Epoch 1, train_loss: 0.6573, val_loss: 0.6361\n",
      "Epoch 2, train_loss: 0.6446, val_loss: 0.6271\n",
      "Epoch 3, train_loss: 0.6308, val_loss: 0.6165\n",
      "Epoch 4, train_loss: 0.6229, val_loss: 0.6056\n",
      "Epoch 5, train_loss: 0.6156, val_loss: 0.5975\n",
      "Epoch 6, train_loss: 0.6027, val_loss: 0.5878\n",
      "Epoch 7, train_loss: 0.5943, val_loss: 0.5773\n",
      "Epoch 8, train_loss: 0.5835, val_loss: 0.5665\n",
      "Epoch 9, train_loss: 0.5766, val_loss: 0.5577\n",
      "Epoch 10, train_loss: 0.5617, val_loss: 0.5492\n",
      "Epoch 11, train_loss: 0.5552, val_loss: 0.5375\n",
      "Epoch 12, train_loss: 0.5428, val_loss: 0.5300\n",
      "Epoch 13, train_loss: 0.5292, val_loss: 0.5188\n",
      "Epoch 14, train_loss: 0.5195, val_loss: 0.5087\n",
      "Epoch 15, train_loss: 0.5057, val_loss: 0.4973\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16, train_loss: 0.4936, val_loss: 0.4851\n",
      "Epoch 17, train_loss: 0.4855, val_loss: 0.4771\n",
      "Epoch 18, train_loss: 0.4657, val_loss: 0.4673\n",
      "Epoch 19, train_loss: 0.4553, val_loss: 0.4568\n",
      "Epoch 20, train_loss: 0.4420, val_loss: 0.4460\n",
      "Epoch 21, train_loss: 0.4257, val_loss: 0.4384\n",
      "Epoch 22, train_loss: 0.4136, val_loss: 0.4270\n",
      "Epoch 23, train_loss: 0.3953, val_loss: 0.4197\n",
      "Epoch 24, train_loss: 0.3798, val_loss: 0.4118\n",
      "Epoch 25, train_loss: 0.3725, val_loss: 0.4064\n",
      "Epoch 26, train_loss: 0.3592, val_loss: 0.3977\n",
      "Epoch 27, train_loss: 0.3366, val_loss: 0.3970\n",
      "Epoch 28, train_loss: 0.3371, val_loss: 0.3880\n",
      "Epoch 29, train_loss: 0.3235, val_loss: 0.3857\n",
      "Epoch 30, train_loss: 0.3183, val_loss: 0.3789\n",
      "Epoch 31, train_loss: 0.3055, val_loss: 0.3741\n",
      "Epoch 32, train_loss: 0.2913, val_loss: 0.3714\n",
      "Epoch 33, train_loss: 0.2865, val_loss: 0.3691\n",
      "Epoch 34, train_loss: 0.2752, val_loss: 0.3669\n",
      "Epoch 35, train_loss: 0.2672, val_loss: 0.3683\n",
      "Epoch 36, train_loss: 0.2555, val_loss: 0.3631\n",
      "Epoch 37, train_loss: 0.2540, val_loss: 0.3598\n",
      "Epoch 38, train_loss: 0.2474, val_loss: 0.3599\n",
      "Epoch 39, train_loss: 0.2418, val_loss: 0.3584\n",
      "Epoch 40, train_loss: 0.2359, val_loss: 0.3559\n",
      "Epoch 41, train_loss: 0.2287, val_loss: 0.3554\n",
      "Epoch 42, train_loss: 0.2214, val_loss: 0.3552\n",
      "Epoch 43, train_loss: 0.2162, val_loss: 0.3528\n",
      "Epoch 44, train_loss: 0.2124, val_loss: 0.3530\n",
      "Epoch 45, train_loss: 0.2089, val_loss: 0.3549\n",
      "Epoch 46, train_loss: 0.2046, val_loss: 0.3525\n",
      "Epoch 47, train_loss: 0.2036, val_loss: 0.3497\n",
      "Epoch 48, train_loss: 0.1962, val_loss: 0.3497\n",
      "Epoch 49, train_loss: 0.1954, val_loss: 0.3510\n",
      "Epoch 50, train_loss: 0.1941, val_loss: 0.3578\n",
      "Epoch 51, train_loss: 0.1941, val_loss: 0.3562\n",
      "Epoch 52, train_loss: 0.1905, val_loss: 0.3533\n",
      "Epoch 53, train_loss: 0.1854, val_loss: 0.3562\n",
      "Epoch 54, train_loss: 0.1810, val_loss: 0.3550\n",
      "Epoch 55, train_loss: 0.1803, val_loss: 0.3576\n",
      "Epoch 56, train_loss: 0.1782, val_loss: 0.3564\n",
      "Epoch 57, train_loss: 0.1770, val_loss: 0.3583\n",
      "Early Stopping!\n",
      "Test the model on fold 1:\n",
      "{'roc_auc': 0.8877217553688143, 'pr_auc': 0.9179723975121457, 'fmax': 0.8264413578600764}\n",
      "Starting Fold: 2\n",
      "Epoch 0, train_loss: 0.6884, val_loss: 0.6750\n",
      "Epoch 1, train_loss: 0.6626, val_loss: 0.6617\n",
      "Epoch 2, train_loss: 0.6444, val_loss: 0.6540\n",
      "Epoch 3, train_loss: 0.6305, val_loss: 0.6451\n",
      "Epoch 4, train_loss: 0.6196, val_loss: 0.6381\n",
      "Epoch 5, train_loss: 0.6062, val_loss: 0.6322\n",
      "Epoch 6, train_loss: 0.5987, val_loss: 0.6250\n",
      "Epoch 7, train_loss: 0.5817, val_loss: 0.6192\n",
      "Epoch 8, train_loss: 0.5760, val_loss: 0.6114\n",
      "Epoch 9, train_loss: 0.5605, val_loss: 0.6043\n",
      "Epoch 10, train_loss: 0.5503, val_loss: 0.5991\n",
      "Epoch 11, train_loss: 0.5348, val_loss: 0.5920\n",
      "Epoch 12, train_loss: 0.5213, val_loss: 0.5861\n",
      "Epoch 13, train_loss: 0.5086, val_loss: 0.5788\n",
      "Epoch 14, train_loss: 0.4867, val_loss: 0.5720\n",
      "Epoch 15, train_loss: 0.4740, val_loss: 0.5647\n",
      "Epoch 16, train_loss: 0.4635, val_loss: 0.5592\n",
      "Epoch 17, train_loss: 0.4498, val_loss: 0.5516\n",
      "Epoch 18, train_loss: 0.4301, val_loss: 0.5456\n",
      "Epoch 19, train_loss: 0.4198, val_loss: 0.5392\n",
      "Epoch 20, train_loss: 0.4056, val_loss: 0.5335\n",
      "Epoch 21, train_loss: 0.3884, val_loss: 0.5271\n",
      "Epoch 22, train_loss: 0.3780, val_loss: 0.5212\n",
      "Epoch 23, train_loss: 0.3586, val_loss: 0.5161\n",
      "Epoch 24, train_loss: 0.3457, val_loss: 0.5127\n",
      "Epoch 25, train_loss: 0.3357, val_loss: 0.5074\n",
      "Epoch 26, train_loss: 0.3213, val_loss: 0.5011\n",
      "Epoch 27, train_loss: 0.3103, val_loss: 0.4965\n",
      "Epoch 28, train_loss: 0.3008, val_loss: 0.4941\n",
      "Epoch 29, train_loss: 0.2921, val_loss: 0.4899\n",
      "Epoch 30, train_loss: 0.2806, val_loss: 0.4875\n",
      "Epoch 31, train_loss: 0.2786, val_loss: 0.4843\n",
      "Epoch 32, train_loss: 0.2665, val_loss: 0.4838\n",
      "Epoch 33, train_loss: 0.2535, val_loss: 0.4802\n",
      "Epoch 34, train_loss: 0.2443, val_loss: 0.4780\n",
      "Epoch 35, train_loss: 0.2442, val_loss: 0.4758\n",
      "Epoch 36, train_loss: 0.2359, val_loss: 0.4770\n",
      "Epoch 37, train_loss: 0.2311, val_loss: 0.4753\n",
      "Epoch 38, train_loss: 0.2263, val_loss: 0.4718\n",
      "Epoch 39, train_loss: 0.2198, val_loss: 0.4708\n",
      "Epoch 40, train_loss: 0.2108, val_loss: 0.4705\n",
      "Epoch 41, train_loss: 0.2095, val_loss: 0.4725\n",
      "Epoch 42, train_loss: 0.2012, val_loss: 0.4709\n",
      "Epoch 43, train_loss: 0.1998, val_loss: 0.4675\n",
      "Epoch 44, train_loss: 0.1990, val_loss: 0.4675\n",
      "Epoch 45, train_loss: 0.1942, val_loss: 0.4695\n",
      "Epoch 46, train_loss: 0.1909, val_loss: 0.4674\n",
      "Epoch 47, train_loss: 0.1866, val_loss: 0.4668\n",
      "Epoch 48, train_loss: 0.1867, val_loss: 0.4658\n",
      "Epoch 49, train_loss: 0.1811, val_loss: 0.4683\n",
      "Epoch 50, train_loss: 0.1767, val_loss: 0.4683\n",
      "Epoch 51, train_loss: 0.1740, val_loss: 0.4686\n",
      "Epoch 52, train_loss: 0.1753, val_loss: 0.4656\n",
      "Epoch 53, train_loss: 0.1709, val_loss: 0.4705\n",
      "Epoch 54, train_loss: 0.1761, val_loss: 0.4660\n",
      "Epoch 55, train_loss: 0.1746, val_loss: 0.4677\n",
      "Epoch 56, train_loss: 0.1709, val_loss: 0.4692\n",
      "Epoch 57, train_loss: 0.1670, val_loss: 0.4661\n",
      "Epoch 58, train_loss: 0.1673, val_loss: 0.4687\n",
      "Epoch 59, train_loss: 0.1600, val_loss: 0.4658\n",
      "Epoch 60, train_loss: 0.1655, val_loss: 0.4661\n",
      "Epoch 61, train_loss: 0.1648, val_loss: 0.4701\n",
      "Epoch 62, train_loss: 0.1592, val_loss: 0.4688\n",
      "Epoch 63, train_loss: 0.1565, val_loss: 0.4662\n",
      "Epoch 64, train_loss: 0.1593, val_loss: 0.4693\n",
      "Epoch 65, train_loss: 0.1570, val_loss: 0.4675\n",
      "Epoch 66, train_loss: 0.1552, val_loss: 0.4678\n",
      "Epoch 67, train_loss: 0.1539, val_loss: 0.4665\n",
      "Epoch 68, train_loss: 0.1534, val_loss: 0.4658\n",
      "Epoch 69, train_loss: 0.1567, val_loss: 0.4730\n",
      "Early Stopping!\n",
      "Test the model on fold 2:\n",
      "{'roc_auc': 0.8941761363636364, 'pr_auc': 0.9037485029671882, 'fmax': 0.8769180781350039}\n",
      "Starting Fold: 3\n",
      "Epoch 0, train_loss: 0.6888, val_loss: 0.6750\n",
      "Epoch 1, train_loss: 0.6579, val_loss: 0.6629\n",
      "Epoch 2, train_loss: 0.6425, val_loss: 0.6521\n",
      "Epoch 3, train_loss: 0.6293, val_loss: 0.6437\n",
      "Epoch 4, train_loss: 0.6178, val_loss: 0.6347\n",
      "Epoch 5, train_loss: 0.6059, val_loss: 0.6262\n",
      "Epoch 6, train_loss: 0.5933, val_loss: 0.6180\n",
      "Epoch 7, train_loss: 0.5801, val_loss: 0.6101\n",
      "Epoch 8, train_loss: 0.5659, val_loss: 0.6033\n",
      "Epoch 9, train_loss: 0.5508, val_loss: 0.5941\n",
      "Epoch 10, train_loss: 0.5397, val_loss: 0.5862\n",
      "Epoch 11, train_loss: 0.5241, val_loss: 0.5784\n",
      "Epoch 12, train_loss: 0.5111, val_loss: 0.5707\n",
      "Epoch 13, train_loss: 0.4934, val_loss: 0.5643\n",
      "Epoch 14, train_loss: 0.4791, val_loss: 0.5569\n",
      "Epoch 15, train_loss: 0.4631, val_loss: 0.5496\n",
      "Epoch 16, train_loss: 0.4523, val_loss: 0.5445\n",
      "Epoch 17, train_loss: 0.4406, val_loss: 0.5403\n",
      "Epoch 18, train_loss: 0.4195, val_loss: 0.5343\n",
      "Epoch 19, train_loss: 0.4041, val_loss: 0.5264\n",
      "Epoch 20, train_loss: 0.3904, val_loss: 0.5259\n",
      "Epoch 21, train_loss: 0.3750, val_loss: 0.5221\n",
      "Epoch 22, train_loss: 0.3604, val_loss: 0.5193\n",
      "Epoch 23, train_loss: 0.3492, val_loss: 0.5162\n",
      "Epoch 24, train_loss: 0.3353, val_loss: 0.5160\n",
      "Epoch 25, train_loss: 0.3202, val_loss: 0.5132\n",
      "Epoch 26, train_loss: 0.3091, val_loss: 0.5090\n",
      "Epoch 27, train_loss: 0.2979, val_loss: 0.5090\n",
      "Epoch 28, train_loss: 0.2846, val_loss: 0.5100\n",
      "Epoch 29, train_loss: 0.2800, val_loss: 0.5072\n",
      "Epoch 30, train_loss: 0.2741, val_loss: 0.5091\n",
      "Epoch 31, train_loss: 0.2584, val_loss: 0.5070\n",
      "Epoch 32, train_loss: 0.2522, val_loss: 0.5078\n",
      "Epoch 33, train_loss: 0.2458, val_loss: 0.5077\n",
      "Epoch 34, train_loss: 0.2403, val_loss: 0.5069\n",
      "Epoch 35, train_loss: 0.2332, val_loss: 0.5053\n",
      "Epoch 36, train_loss: 0.2279, val_loss: 0.5048\n",
      "Epoch 37, train_loss: 0.2234, val_loss: 0.5089\n",
      "Epoch 38, train_loss: 0.2177, val_loss: 0.5111\n",
      "Epoch 39, train_loss: 0.2128, val_loss: 0.5107\n",
      "Epoch 40, train_loss: 0.2072, val_loss: 0.5095\n",
      "Epoch 41, train_loss: 0.2042, val_loss: 0.5072\n",
      "Epoch 42, train_loss: 0.1994, val_loss: 0.5093\n",
      "Epoch 43, train_loss: 0.1950, val_loss: 0.5105\n",
      "Epoch 44, train_loss: 0.1963, val_loss: 0.5077\n",
      "Epoch 45, train_loss: 0.1884, val_loss: 0.5126\n",
      "Early Stopping!\n",
      "Test the model on fold 3:\n",
      "{'roc_auc': 0.8894724277870615, 'pr_auc': 0.8568000310598556, 'fmax': 0.8448225877227964}\n",
      "Starting Fold: 4\n",
      "Epoch 0, train_loss: 0.6847, val_loss: 0.6638\n",
      "Epoch 1, train_loss: 0.6626, val_loss: 0.6450\n",
      "Epoch 2, train_loss: 0.6530, val_loss: 0.6345\n",
      "Epoch 3, train_loss: 0.6394, val_loss: 0.6235\n",
      "Epoch 4, train_loss: 0.6248, val_loss: 0.6128\n",
      "Epoch 5, train_loss: 0.6129, val_loss: 0.6022\n",
      "Epoch 6, train_loss: 0.6039, val_loss: 0.5933\n",
      "Epoch 7, train_loss: 0.5941, val_loss: 0.5832\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8, train_loss: 0.5825, val_loss: 0.5721\n",
      "Epoch 9, train_loss: 0.5732, val_loss: 0.5621\n",
      "Epoch 10, train_loss: 0.5584, val_loss: 0.5520\n",
      "Epoch 11, train_loss: 0.5484, val_loss: 0.5389\n",
      "Epoch 12, train_loss: 0.5400, val_loss: 0.5276\n",
      "Epoch 13, train_loss: 0.5198, val_loss: 0.5188\n",
      "Epoch 14, train_loss: 0.5148, val_loss: 0.5079\n",
      "Epoch 15, train_loss: 0.4968, val_loss: 0.4996\n",
      "Epoch 16, train_loss: 0.4788, val_loss: 0.4893\n",
      "Epoch 17, train_loss: 0.4686, val_loss: 0.4802\n",
      "Epoch 18, train_loss: 0.4518, val_loss: 0.4713\n",
      "Epoch 19, train_loss: 0.4399, val_loss: 0.4646\n",
      "Epoch 20, train_loss: 0.4226, val_loss: 0.4552\n",
      "Epoch 21, train_loss: 0.4067, val_loss: 0.4485\n",
      "Epoch 22, train_loss: 0.3878, val_loss: 0.4422\n",
      "Epoch 23, train_loss: 0.3775, val_loss: 0.4353\n",
      "Epoch 24, train_loss: 0.3718, val_loss: 0.4288\n",
      "Epoch 25, train_loss: 0.3525, val_loss: 0.4261\n",
      "Epoch 26, train_loss: 0.3427, val_loss: 0.4214\n",
      "Epoch 27, train_loss: 0.3271, val_loss: 0.4156\n",
      "Epoch 28, train_loss: 0.3198, val_loss: 0.4151\n",
      "Epoch 29, train_loss: 0.3086, val_loss: 0.4116\n",
      "Epoch 30, train_loss: 0.3021, val_loss: 0.4106\n",
      "Epoch 31, train_loss: 0.2895, val_loss: 0.4071\n",
      "Epoch 32, train_loss: 0.2806, val_loss: 0.4061\n",
      "Epoch 33, train_loss: 0.2673, val_loss: 0.4024\n",
      "Epoch 34, train_loss: 0.2590, val_loss: 0.4041\n",
      "Epoch 35, train_loss: 0.2555, val_loss: 0.4010\n",
      "Epoch 36, train_loss: 0.2516, val_loss: 0.4022\n",
      "Epoch 37, train_loss: 0.2416, val_loss: 0.4010\n",
      "Epoch 38, train_loss: 0.2362, val_loss: 0.4022\n",
      "Epoch 39, train_loss: 0.2274, val_loss: 0.3992\n",
      "Epoch 40, train_loss: 0.2287, val_loss: 0.3999\n",
      "Epoch 41, train_loss: 0.2240, val_loss: 0.3980\n",
      "Epoch 42, train_loss: 0.2185, val_loss: 0.4001\n",
      "Epoch 43, train_loss: 0.2144, val_loss: 0.3970\n",
      "Epoch 44, train_loss: 0.2084, val_loss: 0.3964\n",
      "Epoch 45, train_loss: 0.2097, val_loss: 0.3948\n",
      "Epoch 46, train_loss: 0.2065, val_loss: 0.3966\n",
      "Epoch 47, train_loss: 0.2024, val_loss: 0.3989\n",
      "Epoch 48, train_loss: 0.1999, val_loss: 0.3974\n",
      "Epoch 49, train_loss: 0.1942, val_loss: 0.3978\n",
      "Epoch 50, train_loss: 0.1975, val_loss: 0.3983\n",
      "Epoch 51, train_loss: 0.1871, val_loss: 0.3989\n",
      "Epoch 52, train_loss: 0.1890, val_loss: 0.3983\n",
      "Epoch 53, train_loss: 0.1844, val_loss: 0.3966\n",
      "Epoch 54, train_loss: 0.1857, val_loss: 0.3984\n",
      "Epoch 55, train_loss: 0.1821, val_loss: 0.3999\n",
      "Epoch 56, train_loss: 0.1790, val_loss: 0.3957\n",
      "Epoch 57, train_loss: 0.1778, val_loss: 0.3998\n",
      "Epoch 58, train_loss: 0.1762, val_loss: 0.3990\n",
      "Epoch 59, train_loss: 0.1756, val_loss: 0.4036\n",
      "Early Stopping!\n",
      "Test the model on fold 4:\n",
      "{'roc_auc': 0.9095643939393939, 'pr_auc': 0.906072135005006, 'fmax': 0.8484798531056267}\n",
      "Evaluate pretrained model on disease class Cardiovascular (6/12)\n",
      "Starting Fold: 0\n",
      "Epoch 0, train_loss: 0.6900, val_loss: 0.6853\n",
      "Epoch 1, train_loss: 0.6868, val_loss: 0.6795\n",
      "Epoch 2, train_loss: 0.6719, val_loss: 0.6752\n",
      "Epoch 3, train_loss: 0.6657, val_loss: 0.6708\n",
      "Epoch 4, train_loss: 0.6588, val_loss: 0.6664\n",
      "Epoch 5, train_loss: 0.6518, val_loss: 0.6610\n",
      "Epoch 6, train_loss: 0.6432, val_loss: 0.6583\n",
      "Epoch 7, train_loss: 0.6396, val_loss: 0.6551\n",
      "Epoch 8, train_loss: 0.6267, val_loss: 0.6518\n",
      "Epoch 9, train_loss: 0.6222, val_loss: 0.6482\n",
      "Epoch 10, train_loss: 0.6219, val_loss: 0.6450\n",
      "Epoch 11, train_loss: 0.6115, val_loss: 0.6423\n",
      "Epoch 12, train_loss: 0.6064, val_loss: 0.6387\n",
      "Epoch 13, train_loss: 0.5964, val_loss: 0.6375\n",
      "Epoch 14, train_loss: 0.5942, val_loss: 0.6347\n",
      "Epoch 15, train_loss: 0.5814, val_loss: 0.6322\n",
      "Epoch 16, train_loss: 0.5822, val_loss: 0.6301\n",
      "Epoch 17, train_loss: 0.5720, val_loss: 0.6270\n",
      "Epoch 18, train_loss: 0.5607, val_loss: 0.6262\n",
      "Epoch 19, train_loss: 0.5565, val_loss: 0.6232\n",
      "Epoch 20, train_loss: 0.5505, val_loss: 0.6217\n",
      "Epoch 21, train_loss: 0.5496, val_loss: 0.6200\n",
      "Epoch 22, train_loss: 0.5352, val_loss: 0.6187\n",
      "Epoch 23, train_loss: 0.5389, val_loss: 0.6170\n",
      "Epoch 24, train_loss: 0.5341, val_loss: 0.6155\n",
      "Epoch 25, train_loss: 0.5282, val_loss: 0.6144\n",
      "Epoch 26, train_loss: 0.5109, val_loss: 0.6119\n",
      "Epoch 27, train_loss: 0.5179, val_loss: 0.6111\n",
      "Epoch 28, train_loss: 0.5028, val_loss: 0.6101\n",
      "Epoch 29, train_loss: 0.5002, val_loss: 0.6085\n",
      "Epoch 30, train_loss: 0.4991, val_loss: 0.6082\n",
      "Epoch 31, train_loss: 0.4821, val_loss: 0.6090\n",
      "Epoch 32, train_loss: 0.4861, val_loss: 0.6082\n",
      "Epoch 33, train_loss: 0.4737, val_loss: 0.6064\n",
      "Epoch 34, train_loss: 0.4665, val_loss: 0.6062\n",
      "Epoch 35, train_loss: 0.4760, val_loss: 0.6070\n",
      "Epoch 36, train_loss: 0.4529, val_loss: 0.6049\n",
      "Epoch 37, train_loss: 0.4511, val_loss: 0.6038\n",
      "Epoch 38, train_loss: 0.4459, val_loss: 0.6039\n",
      "Epoch 39, train_loss: 0.4340, val_loss: 0.6039\n",
      "Epoch 40, train_loss: 0.4312, val_loss: 0.6040\n",
      "Epoch 41, train_loss: 0.4296, val_loss: 0.6030\n",
      "Epoch 42, train_loss: 0.4277, val_loss: 0.6057\n",
      "Epoch 43, train_loss: 0.4155, val_loss: 0.6071\n",
      "Epoch 44, train_loss: 0.4053, val_loss: 0.6066\n",
      "Epoch 45, train_loss: 0.4053, val_loss: 0.6072\n",
      "Epoch 46, train_loss: 0.3990, val_loss: 0.6086\n",
      "Epoch 47, train_loss: 0.3884, val_loss: 0.6096\n",
      "Early Stopping!\n",
      "Test the model on fold 0:\n",
      "{'roc_auc': 0.8113553113553114, 'pr_auc': 0.7328698421230617, 'fmax': 0.7692257988486843}\n",
      "Starting Fold: 1\n",
      "Epoch 0, train_loss: 0.6965, val_loss: 0.6947\n",
      "Epoch 1, train_loss: 0.6791, val_loss: 0.6896\n",
      "Epoch 2, train_loss: 0.6716, val_loss: 0.6835\n",
      "Epoch 3, train_loss: 0.6563, val_loss: 0.6802\n",
      "Epoch 4, train_loss: 0.6512, val_loss: 0.6764\n",
      "Epoch 5, train_loss: 0.6373, val_loss: 0.6733\n",
      "Epoch 6, train_loss: 0.6311, val_loss: 0.6710\n",
      "Epoch 7, train_loss: 0.6141, val_loss: 0.6703\n",
      "Epoch 8, train_loss: 0.6127, val_loss: 0.6666\n",
      "Epoch 9, train_loss: 0.6114, val_loss: 0.6645\n",
      "Epoch 10, train_loss: 0.6002, val_loss: 0.6622\n",
      "Epoch 11, train_loss: 0.5958, val_loss: 0.6599\n",
      "Epoch 12, train_loss: 0.5862, val_loss: 0.6588\n",
      "Epoch 13, train_loss: 0.5908, val_loss: 0.6566\n",
      "Epoch 14, train_loss: 0.5776, val_loss: 0.6544\n",
      "Epoch 15, train_loss: 0.5678, val_loss: 0.6524\n",
      "Epoch 16, train_loss: 0.5698, val_loss: 0.6521\n",
      "Epoch 17, train_loss: 0.5537, val_loss: 0.6501\n",
      "Epoch 18, train_loss: 0.5574, val_loss: 0.6489\n",
      "Epoch 19, train_loss: 0.5476, val_loss: 0.6480\n",
      "Epoch 20, train_loss: 0.5377, val_loss: 0.6469\n",
      "Epoch 21, train_loss: 0.5464, val_loss: 0.6456\n",
      "Epoch 22, train_loss: 0.5268, val_loss: 0.6442\n",
      "Epoch 23, train_loss: 0.5210, val_loss: 0.6426\n",
      "Epoch 24, train_loss: 0.5157, val_loss: 0.6416\n",
      "Epoch 25, train_loss: 0.5151, val_loss: 0.6412\n",
      "Epoch 26, train_loss: 0.5086, val_loss: 0.6402\n",
      "Epoch 27, train_loss: 0.5014, val_loss: 0.6405\n",
      "Epoch 28, train_loss: 0.4948, val_loss: 0.6412\n",
      "Epoch 29, train_loss: 0.4871, val_loss: 0.6400\n",
      "Epoch 30, train_loss: 0.4814, val_loss: 0.6396\n",
      "Epoch 31, train_loss: 0.4785, val_loss: 0.6406\n",
      "Epoch 32, train_loss: 0.4697, val_loss: 0.6403\n",
      "Epoch 33, train_loss: 0.4611, val_loss: 0.6387\n",
      "Epoch 34, train_loss: 0.4528, val_loss: 0.6394\n",
      "Epoch 35, train_loss: 0.4571, val_loss: 0.6402\n",
      "Epoch 36, train_loss: 0.4448, val_loss: 0.6400\n",
      "Epoch 37, train_loss: 0.4461, val_loss: 0.6390\n",
      "Epoch 38, train_loss: 0.4436, val_loss: 0.6421\n",
      "Epoch 39, train_loss: 0.4326, val_loss: 0.6424\n",
      "Early Stopping!\n",
      "Test the model on fold 1:\n",
      "{'roc_auc': 0.6521739130434783, 'pr_auc': 0.7096580890582613, 'fmax': 0.6857097796214481}\n",
      "Starting Fold: 2\n",
      "Epoch 0, train_loss: 0.6963, val_loss: 0.6903\n",
      "Epoch 1, train_loss: 0.6733, val_loss: 0.6888\n",
      "Epoch 2, train_loss: 0.6758, val_loss: 0.6842\n",
      "Epoch 3, train_loss: 0.6652, val_loss: 0.6802\n",
      "Epoch 4, train_loss: 0.6475, val_loss: 0.6774\n",
      "Epoch 5, train_loss: 0.6426, val_loss: 0.6767\n",
      "Epoch 6, train_loss: 0.6438, val_loss: 0.6760\n",
      "Epoch 7, train_loss: 0.6294, val_loss: 0.6764\n",
      "Epoch 8, train_loss: 0.6230, val_loss: 0.6758\n",
      "Epoch 9, train_loss: 0.6185, val_loss: 0.6741\n",
      "Epoch 10, train_loss: 0.6119, val_loss: 0.6743\n",
      "Epoch 11, train_loss: 0.6028, val_loss: 0.6745\n",
      "Epoch 12, train_loss: 0.5976, val_loss: 0.6748\n",
      "Epoch 13, train_loss: 0.6010, val_loss: 0.6752\n",
      "Epoch 14, train_loss: 0.5905, val_loss: 0.6760\n",
      "Epoch 15, train_loss: 0.5854, val_loss: 0.6774\n",
      "Epoch 16, train_loss: 0.5681, val_loss: 0.6777\n",
      "Epoch 17, train_loss: 0.5733, val_loss: 0.6779\n",
      "Epoch 18, train_loss: 0.5688, val_loss: 0.6784\n",
      "Epoch 19, train_loss: 0.5723, val_loss: 0.6782\n",
      "Epoch 20, train_loss: 0.5550, val_loss: 0.6789\n",
      "Early Stopping!\n",
      "Test the model on fold 2:\n",
      "{'roc_auc': 0.6818181818181818, 'pr_auc': 0.7824657341548585, 'fmax': 0.7142811224784984}\n",
      "Starting Fold: 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, train_loss: 0.6980, val_loss: 0.6919\n",
      "Epoch 1, train_loss: 0.6827, val_loss: 0.6852\n",
      "Epoch 2, train_loss: 0.6785, val_loss: 0.6794\n",
      "Epoch 3, train_loss: 0.6683, val_loss: 0.6759\n",
      "Epoch 4, train_loss: 0.6574, val_loss: 0.6727\n",
      "Epoch 5, train_loss: 0.6534, val_loss: 0.6718\n",
      "Epoch 6, train_loss: 0.6415, val_loss: 0.6684\n",
      "Epoch 7, train_loss: 0.6445, val_loss: 0.6683\n",
      "Epoch 8, train_loss: 0.6361, val_loss: 0.6664\n",
      "Epoch 9, train_loss: 0.6276, val_loss: 0.6644\n",
      "Epoch 10, train_loss: 0.6260, val_loss: 0.6629\n",
      "Epoch 11, train_loss: 0.6236, val_loss: 0.6618\n",
      "Epoch 12, train_loss: 0.6163, val_loss: 0.6583\n",
      "Epoch 13, train_loss: 0.6123, val_loss: 0.6569\n",
      "Epoch 14, train_loss: 0.6045, val_loss: 0.6556\n",
      "Epoch 15, train_loss: 0.6016, val_loss: 0.6536\n",
      "Epoch 16, train_loss: 0.5998, val_loss: 0.6522\n",
      "Epoch 17, train_loss: 0.5938, val_loss: 0.6518\n",
      "Epoch 18, train_loss: 0.5848, val_loss: 0.6505\n",
      "Epoch 19, train_loss: 0.5803, val_loss: 0.6494\n",
      "Epoch 20, train_loss: 0.5743, val_loss: 0.6490\n",
      "Epoch 21, train_loss: 0.5735, val_loss: 0.6479\n",
      "Epoch 22, train_loss: 0.5732, val_loss: 0.6476\n",
      "Epoch 23, train_loss: 0.5628, val_loss: 0.6445\n",
      "Epoch 24, train_loss: 0.5639, val_loss: 0.6436\n",
      "Epoch 25, train_loss: 0.5512, val_loss: 0.6425\n",
      "Epoch 26, train_loss: 0.5512, val_loss: 0.6412\n",
      "Epoch 27, train_loss: 0.5396, val_loss: 0.6411\n",
      "Epoch 28, train_loss: 0.5363, val_loss: 0.6383\n",
      "Epoch 29, train_loss: 0.5336, val_loss: 0.6366\n",
      "Epoch 30, train_loss: 0.5272, val_loss: 0.6367\n",
      "Epoch 31, train_loss: 0.5204, val_loss: 0.6352\n",
      "Epoch 32, train_loss: 0.5187, val_loss: 0.6344\n",
      "Epoch 33, train_loss: 0.5037, val_loss: 0.6337\n",
      "Epoch 34, train_loss: 0.5035, val_loss: 0.6335\n",
      "Epoch 35, train_loss: 0.5013, val_loss: 0.6320\n",
      "Epoch 36, train_loss: 0.4986, val_loss: 0.6330\n",
      "Epoch 37, train_loss: 0.4883, val_loss: 0.6330\n",
      "Epoch 38, train_loss: 0.4743, val_loss: 0.6316\n",
      "Epoch 39, train_loss: 0.4790, val_loss: 0.6296\n",
      "Epoch 40, train_loss: 0.4645, val_loss: 0.6288\n",
      "Epoch 41, train_loss: 0.4639, val_loss: 0.6285\n",
      "Epoch 42, train_loss: 0.4527, val_loss: 0.6272\n",
      "Epoch 43, train_loss: 0.4429, val_loss: 0.6254\n",
      "Epoch 44, train_loss: 0.4521, val_loss: 0.6256\n",
      "Epoch 45, train_loss: 0.4395, val_loss: 0.6242\n",
      "Epoch 46, train_loss: 0.4287, val_loss: 0.6221\n",
      "Epoch 47, train_loss: 0.4295, val_loss: 0.6240\n",
      "Epoch 48, train_loss: 0.4199, val_loss: 0.6201\n",
      "Epoch 49, train_loss: 0.4144, val_loss: 0.6184\n",
      "Epoch 50, train_loss: 0.4045, val_loss: 0.6153\n",
      "Epoch 51, train_loss: 0.4052, val_loss: 0.6156\n",
      "Epoch 52, train_loss: 0.3975, val_loss: 0.6147\n",
      "Epoch 53, train_loss: 0.3821, val_loss: 0.6138\n",
      "Epoch 54, train_loss: 0.3860, val_loss: 0.6157\n",
      "Epoch 55, train_loss: 0.3743, val_loss: 0.6130\n",
      "Epoch 56, train_loss: 0.3707, val_loss: 0.6139\n",
      "Epoch 57, train_loss: 0.3583, val_loss: 0.6134\n",
      "Epoch 58, train_loss: 0.3561, val_loss: 0.6144\n",
      "Epoch 59, train_loss: 0.3535, val_loss: 0.6148\n",
      "Epoch 60, train_loss: 0.3402, val_loss: 0.6126\n",
      "Epoch 61, train_loss: 0.3411, val_loss: 0.6175\n",
      "Epoch 62, train_loss: 0.3347, val_loss: 0.6168\n",
      "Epoch 63, train_loss: 0.3295, val_loss: 0.6151\n",
      "Epoch 64, train_loss: 0.3109, val_loss: 0.6152\n",
      "Epoch 65, train_loss: 0.3170, val_loss: 0.6139\n",
      "Epoch 66, train_loss: 0.3116, val_loss: 0.6134\n",
      "Epoch 67, train_loss: 0.3057, val_loss: 0.6130\n",
      "Epoch 68, train_loss: 0.2989, val_loss: 0.6111\n",
      "Epoch 69, train_loss: 0.3013, val_loss: 0.6121\n",
      "Epoch 70, train_loss: 0.2930, val_loss: 0.6132\n",
      "Epoch 71, train_loss: 0.2905, val_loss: 0.6118\n",
      "Epoch 72, train_loss: 0.2961, val_loss: 0.6109\n",
      "Epoch 73, train_loss: 0.2782, val_loss: 0.6129\n",
      "Epoch 74, train_loss: 0.2722, val_loss: 0.6132\n",
      "Epoch 75, train_loss: 0.2712, val_loss: 0.6102\n",
      "Epoch 76, train_loss: 0.2649, val_loss: 0.6105\n",
      "Epoch 77, train_loss: 0.2584, val_loss: 0.6129\n",
      "Epoch 78, train_loss: 0.2582, val_loss: 0.6138\n",
      "Epoch 79, train_loss: 0.2706, val_loss: 0.6132\n",
      "Epoch 80, train_loss: 0.2541, val_loss: 0.6184\n",
      "Early Stopping!\n",
      "Test the model on fold 3:\n",
      "{'roc_auc': 0.8454545454545455, 'pr_auc': 0.8965552082735154, 'fmax': 0.8085056586997504}\n",
      "Starting Fold: 4\n",
      "Epoch 0, train_loss: 0.6993, val_loss: 0.6841\n",
      "Epoch 1, train_loss: 0.6790, val_loss: 0.6749\n",
      "Epoch 2, train_loss: 0.6701, val_loss: 0.6668\n",
      "Epoch 3, train_loss: 0.6652, val_loss: 0.6623\n",
      "Epoch 4, train_loss: 0.6461, val_loss: 0.6595\n",
      "Epoch 5, train_loss: 0.6502, val_loss: 0.6556\n",
      "Epoch 6, train_loss: 0.6353, val_loss: 0.6524\n",
      "Epoch 7, train_loss: 0.6304, val_loss: 0.6494\n",
      "Epoch 8, train_loss: 0.6266, val_loss: 0.6468\n",
      "Epoch 9, train_loss: 0.6160, val_loss: 0.6445\n",
      "Epoch 10, train_loss: 0.6203, val_loss: 0.6422\n",
      "Epoch 11, train_loss: 0.6058, val_loss: 0.6419\n",
      "Epoch 12, train_loss: 0.6006, val_loss: 0.6415\n",
      "Epoch 13, train_loss: 0.5987, val_loss: 0.6400\n",
      "Epoch 14, train_loss: 0.5954, val_loss: 0.6389\n",
      "Epoch 15, train_loss: 0.5860, val_loss: 0.6374\n",
      "Epoch 16, train_loss: 0.5741, val_loss: 0.6363\n",
      "Epoch 17, train_loss: 0.5754, val_loss: 0.6356\n",
      "Epoch 18, train_loss: 0.5699, val_loss: 0.6345\n",
      "Epoch 19, train_loss: 0.5628, val_loss: 0.6353\n",
      "Epoch 20, train_loss: 0.5660, val_loss: 0.6355\n",
      "Epoch 21, train_loss: 0.5528, val_loss: 0.6348\n",
      "Epoch 22, train_loss: 0.5531, val_loss: 0.6347\n",
      "Epoch 23, train_loss: 0.5482, val_loss: 0.6338\n",
      "Epoch 24, train_loss: 0.5401, val_loss: 0.6335\n",
      "Epoch 25, train_loss: 0.5374, val_loss: 0.6334\n",
      "Epoch 26, train_loss: 0.5355, val_loss: 0.6333\n",
      "Epoch 27, train_loss: 0.5267, val_loss: 0.6327\n",
      "Epoch 28, train_loss: 0.5141, val_loss: 0.6325\n",
      "Epoch 29, train_loss: 0.5055, val_loss: 0.6324\n",
      "Epoch 30, train_loss: 0.5092, val_loss: 0.6324\n",
      "Epoch 31, train_loss: 0.5068, val_loss: 0.6324\n",
      "Epoch 32, train_loss: 0.4925, val_loss: 0.6327\n",
      "Epoch 33, train_loss: 0.4911, val_loss: 0.6345\n",
      "Epoch 34, train_loss: 0.4871, val_loss: 0.6357\n",
      "Early Stopping!\n",
      "Test the model on fold 4:\n",
      "{'roc_auc': 0.6950757575757576, 'pr_auc': 0.7313979394722625, 'fmax': 0.6896504637657992}\n",
      "Evaluate pretrained model on disease class Dermatological (7/12)\n",
      "Starting Fold: 0\n",
      "Epoch 0, train_loss: 0.6924, val_loss: 0.6915\n",
      "Epoch 1, train_loss: 0.6738, val_loss: 0.6846\n",
      "Epoch 2, train_loss: 0.6696, val_loss: 0.6780\n",
      "Epoch 3, train_loss: 0.6757, val_loss: 0.6728\n",
      "Epoch 4, train_loss: 0.6545, val_loss: 0.6668\n",
      "Epoch 5, train_loss: 0.6497, val_loss: 0.6614\n",
      "Epoch 6, train_loss: 0.6392, val_loss: 0.6543\n",
      "Epoch 7, train_loss: 0.6341, val_loss: 0.6473\n",
      "Epoch 8, train_loss: 0.6267, val_loss: 0.6412\n",
      "Epoch 9, train_loss: 0.6171, val_loss: 0.6366\n",
      "Epoch 10, train_loss: 0.6010, val_loss: 0.6304\n",
      "Epoch 11, train_loss: 0.5921, val_loss: 0.6237\n",
      "Epoch 12, train_loss: 0.5889, val_loss: 0.6171\n",
      "Epoch 13, train_loss: 0.5711, val_loss: 0.6108\n",
      "Epoch 14, train_loss: 0.5734, val_loss: 0.6045\n",
      "Epoch 15, train_loss: 0.5661, val_loss: 0.5980\n",
      "Epoch 16, train_loss: 0.5639, val_loss: 0.5915\n",
      "Epoch 17, train_loss: 0.5548, val_loss: 0.5855\n",
      "Epoch 18, train_loss: 0.5390, val_loss: 0.5794\n",
      "Epoch 19, train_loss: 0.5335, val_loss: 0.5739\n",
      "Epoch 20, train_loss: 0.5272, val_loss: 0.5681\n",
      "Epoch 21, train_loss: 0.5159, val_loss: 0.5625\n",
      "Epoch 22, train_loss: 0.4954, val_loss: 0.5582\n",
      "Epoch 23, train_loss: 0.4985, val_loss: 0.5532\n",
      "Epoch 24, train_loss: 0.5035, val_loss: 0.5483\n",
      "Epoch 25, train_loss: 0.4910, val_loss: 0.5434\n",
      "Epoch 26, train_loss: 0.4669, val_loss: 0.5387\n",
      "Epoch 27, train_loss: 0.4651, val_loss: 0.5336\n",
      "Epoch 28, train_loss: 0.4515, val_loss: 0.5292\n",
      "Epoch 29, train_loss: 0.4602, val_loss: 0.5258\n",
      "Epoch 30, train_loss: 0.4422, val_loss: 0.5215\n",
      "Epoch 31, train_loss: 0.4435, val_loss: 0.5165\n",
      "Epoch 32, train_loss: 0.4233, val_loss: 0.5109\n",
      "Epoch 33, train_loss: 0.4061, val_loss: 0.5056\n",
      "Epoch 34, train_loss: 0.4283, val_loss: 0.5006\n",
      "Epoch 35, train_loss: 0.4076, val_loss: 0.4956\n",
      "Epoch 36, train_loss: 0.4119, val_loss: 0.4923\n",
      "Epoch 37, train_loss: 0.3983, val_loss: 0.4890\n",
      "Epoch 38, train_loss: 0.3904, val_loss: 0.4859\n",
      "Epoch 39, train_loss: 0.3838, val_loss: 0.4825\n",
      "Epoch 40, train_loss: 0.3857, val_loss: 0.4796\n",
      "Epoch 41, train_loss: 0.3315, val_loss: 0.4782\n",
      "Epoch 42, train_loss: 0.3436, val_loss: 0.4745\n",
      "Epoch 43, train_loss: 0.3367, val_loss: 0.4716\n",
      "Epoch 44, train_loss: 0.3281, val_loss: 0.4664\n",
      "Epoch 45, train_loss: 0.3228, val_loss: 0.4631\n",
      "Epoch 46, train_loss: 0.3331, val_loss: 0.4599\n",
      "Epoch 47, train_loss: 0.3174, val_loss: 0.4570\n",
      "Epoch 48, train_loss: 0.2909, val_loss: 0.4544\n",
      "Epoch 49, train_loss: 0.2986, val_loss: 0.4517\n",
      "Epoch 50, train_loss: 0.2988, val_loss: 0.4489\n",
      "Epoch 51, train_loss: 0.2776, val_loss: 0.4464\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 52, train_loss: 0.2962, val_loss: 0.4440\n",
      "Epoch 53, train_loss: 0.2818, val_loss: 0.4409\n",
      "Epoch 54, train_loss: 0.2753, val_loss: 0.4374\n",
      "Epoch 55, train_loss: 0.2692, val_loss: 0.4348\n",
      "Epoch 56, train_loss: 0.2499, val_loss: 0.4334\n",
      "Epoch 57, train_loss: 0.2438, val_loss: 0.4322\n",
      "Epoch 58, train_loss: 0.2516, val_loss: 0.4319\n",
      "Epoch 59, train_loss: 0.2415, val_loss: 0.4301\n",
      "Epoch 60, train_loss: 0.2429, val_loss: 0.4286\n",
      "Epoch 61, train_loss: 0.2240, val_loss: 0.4263\n",
      "Epoch 62, train_loss: 0.2587, val_loss: 0.4253\n",
      "Epoch 63, train_loss: 0.2131, val_loss: 0.4243\n",
      "Epoch 64, train_loss: 0.2193, val_loss: 0.4208\n",
      "Epoch 65, train_loss: 0.2074, val_loss: 0.4199\n",
      "Epoch 66, train_loss: 0.2128, val_loss: 0.4184\n",
      "Epoch 67, train_loss: 0.2117, val_loss: 0.4172\n",
      "Epoch 68, train_loss: 0.2369, val_loss: 0.4172\n",
      "Epoch 69, train_loss: 0.1984, val_loss: 0.4165\n",
      "Epoch 70, train_loss: 0.1939, val_loss: 0.4164\n",
      "Epoch 71, train_loss: 0.1892, val_loss: 0.4151\n",
      "Epoch 72, train_loss: 0.1917, val_loss: 0.4145\n",
      "Epoch 73, train_loss: 0.2010, val_loss: 0.4127\n",
      "Epoch 74, train_loss: 0.2058, val_loss: 0.4123\n",
      "Epoch 75, train_loss: 0.1943, val_loss: 0.4137\n",
      "Epoch 76, train_loss: 0.1950, val_loss: 0.4139\n",
      "Epoch 77, train_loss: 0.1723, val_loss: 0.4125\n",
      "Epoch 78, train_loss: 0.1724, val_loss: 0.4116\n",
      "Epoch 79, train_loss: 0.1779, val_loss: 0.4111\n",
      "Epoch 80, train_loss: 0.2098, val_loss: 0.4110\n",
      "Epoch 81, train_loss: 0.2007, val_loss: 0.4109\n",
      "Epoch 82, train_loss: 0.1636, val_loss: 0.4100\n",
      "Epoch 83, train_loss: 0.1589, val_loss: 0.4078\n",
      "Epoch 84, train_loss: 0.1567, val_loss: 0.4052\n",
      "Epoch 85, train_loss: 0.1670, val_loss: 0.4034\n",
      "Epoch 86, train_loss: 0.1527, val_loss: 0.4032\n",
      "Epoch 87, train_loss: 0.1506, val_loss: 0.4033\n",
      "Epoch 88, train_loss: 0.1573, val_loss: 0.4033\n",
      "Epoch 89, train_loss: 0.1541, val_loss: 0.4038\n",
      "Epoch 90, train_loss: 0.1534, val_loss: 0.4045\n",
      "Epoch 91, train_loss: 0.1532, val_loss: 0.4029\n",
      "Epoch 92, train_loss: 0.1499, val_loss: 0.4028\n",
      "Epoch 93, train_loss: 0.1458, val_loss: 0.4022\n",
      "Epoch 94, train_loss: 0.1751, val_loss: 0.4022\n",
      "Epoch 95, train_loss: 0.1864, val_loss: 0.4064\n",
      "Epoch 96, train_loss: 0.1481, val_loss: 0.4070\n",
      "Epoch 97, train_loss: 0.1699, val_loss: 0.4056\n",
      "Epoch 98, train_loss: 0.1351, val_loss: 0.4056\n",
      "Epoch 99, train_loss: 0.1415, val_loss: 0.4052\n",
      "Test the model on fold 0:\n",
      "{'roc_auc': 0.8325358851674641, 'pr_auc': 0.8696687228522121, 'fmax': 0.8499950500288262}\n",
      "Starting Fold: 1\n",
      "Epoch 0, train_loss: 0.6954, val_loss: 0.6928\n",
      "Epoch 1, train_loss: 0.6766, val_loss: 0.6873\n",
      "Epoch 2, train_loss: 0.6648, val_loss: 0.6845\n",
      "Epoch 3, train_loss: 0.6549, val_loss: 0.6811\n",
      "Epoch 4, train_loss: 0.6331, val_loss: 0.6780\n",
      "Epoch 5, train_loss: 0.6300, val_loss: 0.6729\n",
      "Epoch 6, train_loss: 0.6281, val_loss: 0.6700\n",
      "Epoch 7, train_loss: 0.6226, val_loss: 0.6662\n",
      "Epoch 8, train_loss: 0.6006, val_loss: 0.6631\n",
      "Epoch 9, train_loss: 0.5957, val_loss: 0.6593\n",
      "Epoch 10, train_loss: 0.6045, val_loss: 0.6563\n",
      "Epoch 11, train_loss: 0.5806, val_loss: 0.6505\n",
      "Epoch 12, train_loss: 0.5880, val_loss: 0.6473\n",
      "Epoch 13, train_loss: 0.5815, val_loss: 0.6413\n",
      "Epoch 14, train_loss: 0.5745, val_loss: 0.6357\n",
      "Epoch 15, train_loss: 0.5602, val_loss: 0.6327\n",
      "Epoch 16, train_loss: 0.5720, val_loss: 0.6286\n",
      "Epoch 17, train_loss: 0.5393, val_loss: 0.6244\n",
      "Epoch 18, train_loss: 0.5439, val_loss: 0.6202\n",
      "Epoch 19, train_loss: 0.5198, val_loss: 0.6155\n",
      "Epoch 20, train_loss: 0.5228, val_loss: 0.6105\n",
      "Epoch 21, train_loss: 0.5255, val_loss: 0.6068\n",
      "Epoch 22, train_loss: 0.5202, val_loss: 0.6032\n",
      "Epoch 23, train_loss: 0.5088, val_loss: 0.6002\n",
      "Epoch 24, train_loss: 0.4940, val_loss: 0.5964\n",
      "Epoch 25, train_loss: 0.5010, val_loss: 0.5935\n",
      "Epoch 26, train_loss: 0.5022, val_loss: 0.5888\n",
      "Epoch 27, train_loss: 0.4822, val_loss: 0.5855\n",
      "Epoch 28, train_loss: 0.4765, val_loss: 0.5796\n",
      "Epoch 29, train_loss: 0.4801, val_loss: 0.5744\n",
      "Epoch 30, train_loss: 0.4491, val_loss: 0.5716\n",
      "Epoch 31, train_loss: 0.4590, val_loss: 0.5673\n",
      "Epoch 32, train_loss: 0.4356, val_loss: 0.5629\n",
      "Epoch 33, train_loss: 0.4399, val_loss: 0.5591\n",
      "Epoch 34, train_loss: 0.4451, val_loss: 0.5540\n",
      "Epoch 35, train_loss: 0.4342, val_loss: 0.5506\n",
      "Epoch 36, train_loss: 0.4336, val_loss: 0.5480\n",
      "Epoch 37, train_loss: 0.4095, val_loss: 0.5445\n",
      "Epoch 38, train_loss: 0.4193, val_loss: 0.5411\n",
      "Epoch 39, train_loss: 0.4209, val_loss: 0.5387\n",
      "Epoch 40, train_loss: 0.4063, val_loss: 0.5367\n",
      "Epoch 41, train_loss: 0.3919, val_loss: 0.5340\n",
      "Epoch 42, train_loss: 0.3899, val_loss: 0.5306\n",
      "Epoch 43, train_loss: 0.3898, val_loss: 0.5263\n",
      "Epoch 44, train_loss: 0.3648, val_loss: 0.5233\n",
      "Epoch 45, train_loss: 0.3815, val_loss: 0.5217\n",
      "Epoch 46, train_loss: 0.3607, val_loss: 0.5183\n",
      "Epoch 47, train_loss: 0.3771, val_loss: 0.5153\n",
      "Epoch 48, train_loss: 0.3307, val_loss: 0.5127\n",
      "Epoch 49, train_loss: 0.3422, val_loss: 0.5120\n",
      "Epoch 50, train_loss: 0.3235, val_loss: 0.5104\n",
      "Epoch 51, train_loss: 0.3411, val_loss: 0.5091\n",
      "Epoch 52, train_loss: 0.3375, val_loss: 0.5070\n",
      "Epoch 53, train_loss: 0.3169, val_loss: 0.5039\n",
      "Epoch 54, train_loss: 0.3233, val_loss: 0.4997\n",
      "Epoch 55, train_loss: 0.2989, val_loss: 0.4995\n",
      "Epoch 56, train_loss: 0.3025, val_loss: 0.4981\n",
      "Epoch 57, train_loss: 0.2998, val_loss: 0.4963\n",
      "Epoch 58, train_loss: 0.2967, val_loss: 0.4940\n",
      "Epoch 59, train_loss: 0.3149, val_loss: 0.4969\n",
      "Epoch 60, train_loss: 0.3001, val_loss: 0.4927\n",
      "Epoch 61, train_loss: 0.3066, val_loss: 0.4874\n",
      "Epoch 62, train_loss: 0.2946, val_loss: 0.4842\n",
      "Epoch 63, train_loss: 0.2668, val_loss: 0.4832\n",
      "Epoch 64, train_loss: 0.2723, val_loss: 0.4821\n",
      "Epoch 65, train_loss: 0.2590, val_loss: 0.4784\n",
      "Epoch 66, train_loss: 0.2674, val_loss: 0.4774\n",
      "Epoch 67, train_loss: 0.2645, val_loss: 0.4768\n",
      "Epoch 68, train_loss: 0.2451, val_loss: 0.4772\n",
      "Epoch 69, train_loss: 0.2551, val_loss: 0.4760\n",
      "Epoch 70, train_loss: 0.2681, val_loss: 0.4723\n",
      "Epoch 71, train_loss: 0.2321, val_loss: 0.4711\n",
      "Epoch 72, train_loss: 0.2423, val_loss: 0.4705\n",
      "Epoch 73, train_loss: 0.2229, val_loss: 0.4705\n",
      "Epoch 74, train_loss: 0.2263, val_loss: 0.4689\n",
      "Epoch 75, train_loss: 0.2252, val_loss: 0.4680\n",
      "Epoch 76, train_loss: 0.2157, val_loss: 0.4683\n",
      "Epoch 77, train_loss: 0.2461, val_loss: 0.4692\n",
      "Epoch 78, train_loss: 0.2657, val_loss: 0.4699\n",
      "Epoch 79, train_loss: 0.2205, val_loss: 0.4682\n",
      "Epoch 80, train_loss: 0.2233, val_loss: 0.4672\n",
      "Epoch 81, train_loss: 0.2001, val_loss: 0.4656\n",
      "Epoch 82, train_loss: 0.2060, val_loss: 0.4665\n",
      "Epoch 83, train_loss: 0.2256, val_loss: 0.4645\n",
      "Epoch 84, train_loss: 0.2172, val_loss: 0.4658\n",
      "Epoch 85, train_loss: 0.1992, val_loss: 0.4651\n",
      "Epoch 86, train_loss: 0.1954, val_loss: 0.4651\n",
      "Epoch 87, train_loss: 0.2026, val_loss: 0.4625\n",
      "Epoch 88, train_loss: 0.1900, val_loss: 0.4636\n",
      "Epoch 89, train_loss: 0.2000, val_loss: 0.4625\n",
      "Epoch 90, train_loss: 0.1818, val_loss: 0.4615\n",
      "Epoch 91, train_loss: 0.1824, val_loss: 0.4597\n",
      "Epoch 92, train_loss: 0.1812, val_loss: 0.4587\n",
      "Epoch 93, train_loss: 0.1771, val_loss: 0.4582\n",
      "Epoch 94, train_loss: 0.1867, val_loss: 0.4581\n",
      "Epoch 95, train_loss: 0.1814, val_loss: 0.4588\n",
      "Epoch 96, train_loss: 0.1772, val_loss: 0.4598\n",
      "Epoch 97, train_loss: 0.1891, val_loss: 0.4603\n",
      "Epoch 98, train_loss: 0.1855, val_loss: 0.4596\n",
      "Epoch 99, train_loss: 0.1567, val_loss: 0.4581\n",
      "Test the model on fold 1:\n",
      "{'roc_auc': 0.9000000000000001, 'pr_auc': 0.9403269594815533, 'fmax': 0.9047569047895362}\n",
      "Starting Fold: 2\n",
      "Epoch 0, train_loss: 0.6895, val_loss: 0.6840\n",
      "Epoch 1, train_loss: 0.6788, val_loss: 0.6725\n",
      "Epoch 2, train_loss: 0.6534, val_loss: 0.6609\n",
      "Epoch 3, train_loss: 0.6486, val_loss: 0.6499\n",
      "Epoch 4, train_loss: 0.6371, val_loss: 0.6401\n",
      "Epoch 5, train_loss: 0.6236, val_loss: 0.6326\n",
      "Epoch 6, train_loss: 0.6199, val_loss: 0.6250\n",
      "Epoch 7, train_loss: 0.6114, val_loss: 0.6186\n",
      "Epoch 8, train_loss: 0.6147, val_loss: 0.6134\n",
      "Epoch 9, train_loss: 0.6011, val_loss: 0.6082\n",
      "Epoch 10, train_loss: 0.5890, val_loss: 0.6025\n",
      "Epoch 11, train_loss: 0.5818, val_loss: 0.5974\n",
      "Epoch 12, train_loss: 0.5679, val_loss: 0.5924\n",
      "Epoch 13, train_loss: 0.5758, val_loss: 0.5862\n",
      "Epoch 14, train_loss: 0.5450, val_loss: 0.5809\n",
      "Epoch 15, train_loss: 0.5425, val_loss: 0.5782\n",
      "Epoch 16, train_loss: 0.5458, val_loss: 0.5745\n",
      "Epoch 17, train_loss: 0.5336, val_loss: 0.5694\n",
      "Epoch 18, train_loss: 0.5281, val_loss: 0.5645\n",
      "Epoch 19, train_loss: 0.5215, val_loss: 0.5594\n",
      "Epoch 20, train_loss: 0.5230, val_loss: 0.5557\n",
      "Epoch 21, train_loss: 0.5201, val_loss: 0.5515\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22, train_loss: 0.5001, val_loss: 0.5475\n",
      "Epoch 23, train_loss: 0.4868, val_loss: 0.5441\n",
      "Epoch 24, train_loss: 0.4766, val_loss: 0.5414\n",
      "Epoch 25, train_loss: 0.4864, val_loss: 0.5378\n",
      "Epoch 26, train_loss: 0.4621, val_loss: 0.5338\n",
      "Epoch 27, train_loss: 0.4627, val_loss: 0.5308\n",
      "Epoch 28, train_loss: 0.4647, val_loss: 0.5277\n",
      "Epoch 29, train_loss: 0.4618, val_loss: 0.5252\n",
      "Epoch 30, train_loss: 0.4505, val_loss: 0.5217\n",
      "Epoch 31, train_loss: 0.4336, val_loss: 0.5184\n",
      "Epoch 32, train_loss: 0.4196, val_loss: 0.5153\n",
      "Epoch 33, train_loss: 0.4340, val_loss: 0.5114\n",
      "Epoch 34, train_loss: 0.4061, val_loss: 0.5092\n",
      "Epoch 35, train_loss: 0.3981, val_loss: 0.5059\n",
      "Epoch 36, train_loss: 0.4050, val_loss: 0.5029\n",
      "Epoch 37, train_loss: 0.3818, val_loss: 0.5001\n",
      "Epoch 38, train_loss: 0.3901, val_loss: 0.4982\n",
      "Epoch 39, train_loss: 0.3885, val_loss: 0.4932\n",
      "Epoch 40, train_loss: 0.3739, val_loss: 0.4917\n",
      "Epoch 41, train_loss: 0.3832, val_loss: 0.4893\n",
      "Epoch 42, train_loss: 0.3463, val_loss: 0.4872\n",
      "Epoch 43, train_loss: 0.3437, val_loss: 0.4845\n",
      "Epoch 44, train_loss: 0.3282, val_loss: 0.4806\n",
      "Epoch 45, train_loss: 0.3578, val_loss: 0.4797\n",
      "Epoch 46, train_loss: 0.3317, val_loss: 0.4781\n",
      "Epoch 47, train_loss: 0.3234, val_loss: 0.4768\n",
      "Epoch 48, train_loss: 0.3202, val_loss: 0.4751\n",
      "Epoch 49, train_loss: 0.3338, val_loss: 0.4748\n",
      "Epoch 50, train_loss: 0.2981, val_loss: 0.4733\n",
      "Epoch 51, train_loss: 0.2886, val_loss: 0.4726\n",
      "Epoch 52, train_loss: 0.3214, val_loss: 0.4719\n",
      "Epoch 53, train_loss: 0.3246, val_loss: 0.4691\n",
      "Epoch 54, train_loss: 0.2755, val_loss: 0.4669\n",
      "Epoch 55, train_loss: 0.3022, val_loss: 0.4650\n",
      "Epoch 56, train_loss: 0.2667, val_loss: 0.4644\n",
      "Epoch 57, train_loss: 0.2477, val_loss: 0.4610\n",
      "Epoch 58, train_loss: 0.2540, val_loss: 0.4598\n",
      "Epoch 59, train_loss: 0.2720, val_loss: 0.4616\n",
      "Epoch 60, train_loss: 0.2560, val_loss: 0.4591\n",
      "Epoch 61, train_loss: 0.2628, val_loss: 0.4570\n",
      "Epoch 62, train_loss: 0.2411, val_loss: 0.4560\n",
      "Epoch 63, train_loss: 0.2409, val_loss: 0.4551\n",
      "Epoch 64, train_loss: 0.2298, val_loss: 0.4520\n",
      "Epoch 65, train_loss: 0.2496, val_loss: 0.4505\n",
      "Epoch 66, train_loss: 0.2381, val_loss: 0.4483\n",
      "Epoch 67, train_loss: 0.2262, val_loss: 0.4470\n",
      "Epoch 68, train_loss: 0.2142, val_loss: 0.4445\n",
      "Epoch 69, train_loss: 0.2185, val_loss: 0.4432\n",
      "Epoch 70, train_loss: 0.2214, val_loss: 0.4425\n",
      "Epoch 71, train_loss: 0.2285, val_loss: 0.4418\n",
      "Epoch 72, train_loss: 0.2011, val_loss: 0.4426\n",
      "Epoch 73, train_loss: 0.2169, val_loss: 0.4458\n",
      "Epoch 74, train_loss: 0.2259, val_loss: 0.4451\n",
      "Epoch 75, train_loss: 0.2332, val_loss: 0.4434\n",
      "Epoch 76, train_loss: 0.2115, val_loss: 0.4426\n",
      "Epoch 77, train_loss: 0.2084, val_loss: 0.4416\n",
      "Epoch 78, train_loss: 0.1944, val_loss: 0.4416\n",
      "Epoch 79, train_loss: 0.2108, val_loss: 0.4453\n",
      "Epoch 80, train_loss: 0.1849, val_loss: 0.4452\n",
      "Epoch 81, train_loss: 0.1869, val_loss: 0.4432\n",
      "Epoch 82, train_loss: 0.1768, val_loss: 0.4418\n",
      "Epoch 83, train_loss: 0.1795, val_loss: 0.4358\n",
      "Epoch 84, train_loss: 0.1720, val_loss: 0.4355\n",
      "Epoch 85, train_loss: 0.1718, val_loss: 0.4377\n",
      "Epoch 86, train_loss: 0.1712, val_loss: 0.4376\n",
      "Epoch 87, train_loss: 0.1622, val_loss: 0.4393\n",
      "Epoch 88, train_loss: 0.1637, val_loss: 0.4396\n",
      "Epoch 89, train_loss: 0.1643, val_loss: 0.4387\n",
      "Epoch 90, train_loss: 0.1518, val_loss: 0.4381\n",
      "Epoch 91, train_loss: 0.1607, val_loss: 0.4422\n",
      "Epoch 92, train_loss: 0.1788, val_loss: 0.4406\n",
      "Epoch 93, train_loss: 0.1617, val_loss: 0.4396\n",
      "Epoch 94, train_loss: 0.1500, val_loss: 0.4377\n",
      "Epoch 95, train_loss: 0.1572, val_loss: 0.4364\n",
      "Epoch 96, train_loss: 0.1671, val_loss: 0.4378\n",
      "Epoch 97, train_loss: 0.1567, val_loss: 0.4398\n",
      "Epoch 98, train_loss: 0.1508, val_loss: 0.4385\n",
      "Epoch 99, train_loss: 0.1468, val_loss: 0.4371\n",
      "Test the model on fold 2:\n",
      "{'roc_auc': 0.8833333333333333, 'pr_auc': 0.912422972048302, 'fmax': 0.8571379592116616}\n",
      "Starting Fold: 3\n",
      "Epoch 0, train_loss: 0.6904, val_loss: 0.6948\n",
      "Epoch 1, train_loss: 0.6737, val_loss: 0.6887\n",
      "Epoch 2, train_loss: 0.6594, val_loss: 0.6792\n",
      "Epoch 3, train_loss: 0.6460, val_loss: 0.6731\n",
      "Epoch 4, train_loss: 0.6281, val_loss: 0.6671\n",
      "Epoch 5, train_loss: 0.6273, val_loss: 0.6624\n",
      "Epoch 6, train_loss: 0.6118, val_loss: 0.6541\n",
      "Epoch 7, train_loss: 0.6120, val_loss: 0.6482\n",
      "Epoch 8, train_loss: 0.5991, val_loss: 0.6415\n",
      "Epoch 9, train_loss: 0.5977, val_loss: 0.6360\n",
      "Epoch 10, train_loss: 0.5825, val_loss: 0.6297\n",
      "Epoch 11, train_loss: 0.5780, val_loss: 0.6230\n",
      "Epoch 12, train_loss: 0.5662, val_loss: 0.6182\n",
      "Epoch 13, train_loss: 0.5730, val_loss: 0.6142\n",
      "Epoch 14, train_loss: 0.5497, val_loss: 0.6098\n",
      "Epoch 15, train_loss: 0.5369, val_loss: 0.6056\n",
      "Epoch 16, train_loss: 0.5388, val_loss: 0.6014\n",
      "Epoch 17, train_loss: 0.5208, val_loss: 0.5991\n",
      "Epoch 18, train_loss: 0.5194, val_loss: 0.5967\n",
      "Epoch 19, train_loss: 0.5081, val_loss: 0.5896\n",
      "Epoch 20, train_loss: 0.5111, val_loss: 0.5861\n",
      "Epoch 21, train_loss: 0.5044, val_loss: 0.5815\n",
      "Epoch 22, train_loss: 0.4939, val_loss: 0.5777\n",
      "Epoch 23, train_loss: 0.4658, val_loss: 0.5747\n",
      "Epoch 24, train_loss: 0.4907, val_loss: 0.5717\n",
      "Epoch 25, train_loss: 0.4701, val_loss: 0.5692\n",
      "Epoch 26, train_loss: 0.4785, val_loss: 0.5654\n",
      "Epoch 27, train_loss: 0.4519, val_loss: 0.5617\n",
      "Epoch 28, train_loss: 0.4343, val_loss: 0.5588\n",
      "Epoch 29, train_loss: 0.4574, val_loss: 0.5565\n",
      "Epoch 30, train_loss: 0.4538, val_loss: 0.5548\n",
      "Epoch 31, train_loss: 0.4110, val_loss: 0.5535\n",
      "Epoch 32, train_loss: 0.4214, val_loss: 0.5517\n",
      "Epoch 33, train_loss: 0.4165, val_loss: 0.5478\n",
      "Epoch 34, train_loss: 0.4151, val_loss: 0.5461\n",
      "Epoch 35, train_loss: 0.4133, val_loss: 0.5440\n",
      "Epoch 36, train_loss: 0.3974, val_loss: 0.5409\n",
      "Epoch 37, train_loss: 0.3961, val_loss: 0.5385\n",
      "Epoch 38, train_loss: 0.3842, val_loss: 0.5379\n",
      "Epoch 39, train_loss: 0.3914, val_loss: 0.5325\n",
      "Epoch 40, train_loss: 0.3775, val_loss: 0.5299\n",
      "Epoch 41, train_loss: 0.3746, val_loss: 0.5311\n",
      "Epoch 42, train_loss: 0.3670, val_loss: 0.5301\n",
      "Epoch 43, train_loss: 0.3429, val_loss: 0.5280\n",
      "Epoch 44, train_loss: 0.3426, val_loss: 0.5273\n",
      "Epoch 45, train_loss: 0.3429, val_loss: 0.5261\n",
      "Epoch 46, train_loss: 0.3240, val_loss: 0.5235\n",
      "Epoch 47, train_loss: 0.3327, val_loss: 0.5236\n",
      "Epoch 48, train_loss: 0.3211, val_loss: 0.5221\n",
      "Epoch 49, train_loss: 0.3291, val_loss: 0.5217\n",
      "Epoch 50, train_loss: 0.3073, val_loss: 0.5224\n",
      "Epoch 51, train_loss: 0.3303, val_loss: 0.5225\n",
      "Epoch 52, train_loss: 0.2960, val_loss: 0.5205\n",
      "Epoch 53, train_loss: 0.2987, val_loss: 0.5206\n",
      "Epoch 54, train_loss: 0.2757, val_loss: 0.5204\n",
      "Epoch 55, train_loss: 0.2875, val_loss: 0.5204\n",
      "Epoch 56, train_loss: 0.2892, val_loss: 0.5212\n",
      "Epoch 57, train_loss: 0.2565, val_loss: 0.5199\n",
      "Epoch 58, train_loss: 0.2831, val_loss: 0.5196\n",
      "Epoch 59, train_loss: 0.2529, val_loss: 0.5194\n",
      "Epoch 60, train_loss: 0.2446, val_loss: 0.5214\n",
      "Epoch 61, train_loss: 0.2707, val_loss: 0.5197\n",
      "Epoch 62, train_loss: 0.2625, val_loss: 0.5198\n",
      "Epoch 63, train_loss: 0.2332, val_loss: 0.5205\n",
      "Epoch 64, train_loss: 0.2387, val_loss: 0.5209\n",
      "Epoch 65, train_loss: 0.2327, val_loss: 0.5236\n",
      "Early Stopping!\n",
      "Test the model on fold 3:\n",
      "{'roc_auc': 0.8595238095238096, 'pr_auc': 0.8752688345516966, 'fmax': 0.833328411487403}\n",
      "Starting Fold: 4\n",
      "Epoch 0, train_loss: 0.6873, val_loss: 0.6865\n",
      "Epoch 1, train_loss: 0.6816, val_loss: 0.6777\n",
      "Epoch 2, train_loss: 0.6768, val_loss: 0.6696\n",
      "Epoch 3, train_loss: 0.6524, val_loss: 0.6625\n",
      "Epoch 4, train_loss: 0.6381, val_loss: 0.6561\n",
      "Epoch 5, train_loss: 0.6380, val_loss: 0.6489\n",
      "Epoch 6, train_loss: 0.6367, val_loss: 0.6433\n",
      "Epoch 7, train_loss: 0.6213, val_loss: 0.6388\n",
      "Epoch 8, train_loss: 0.6044, val_loss: 0.6343\n",
      "Epoch 9, train_loss: 0.6132, val_loss: 0.6298\n",
      "Epoch 10, train_loss: 0.6032, val_loss: 0.6244\n",
      "Epoch 11, train_loss: 0.5863, val_loss: 0.6192\n",
      "Epoch 12, train_loss: 0.5839, val_loss: 0.6150\n",
      "Epoch 13, train_loss: 0.5700, val_loss: 0.6103\n",
      "Epoch 14, train_loss: 0.5511, val_loss: 0.6058\n",
      "Epoch 15, train_loss: 0.5716, val_loss: 0.6012\n",
      "Epoch 16, train_loss: 0.5520, val_loss: 0.5974\n",
      "Epoch 17, train_loss: 0.5611, val_loss: 0.5936\n",
      "Epoch 18, train_loss: 0.5445, val_loss: 0.5900\n",
      "Epoch 19, train_loss: 0.5480, val_loss: 0.5867\n",
      "Epoch 20, train_loss: 0.5431, val_loss: 0.5830\n",
      "Epoch 21, train_loss: 0.5489, val_loss: 0.5791\n",
      "Epoch 22, train_loss: 0.5193, val_loss: 0.5765\n",
      "Epoch 23, train_loss: 0.5272, val_loss: 0.5739\n",
      "Epoch 24, train_loss: 0.5168, val_loss: 0.5710\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25, train_loss: 0.5039, val_loss: 0.5675\n",
      "Epoch 26, train_loss: 0.4943, val_loss: 0.5645\n",
      "Epoch 27, train_loss: 0.4825, val_loss: 0.5612\n",
      "Epoch 28, train_loss: 0.4831, val_loss: 0.5576\n",
      "Epoch 29, train_loss: 0.4887, val_loss: 0.5537\n",
      "Epoch 30, train_loss: 0.4819, val_loss: 0.5505\n",
      "Epoch 31, train_loss: 0.4711, val_loss: 0.5475\n",
      "Epoch 32, train_loss: 0.4603, val_loss: 0.5445\n",
      "Epoch 33, train_loss: 0.4442, val_loss: 0.5411\n",
      "Epoch 34, train_loss: 0.4431, val_loss: 0.5378\n",
      "Epoch 35, train_loss: 0.4400, val_loss: 0.5348\n",
      "Epoch 36, train_loss: 0.4333, val_loss: 0.5317\n",
      "Epoch 37, train_loss: 0.4200, val_loss: 0.5286\n",
      "Epoch 38, train_loss: 0.3964, val_loss: 0.5252\n",
      "Epoch 39, train_loss: 0.4075, val_loss: 0.5229\n",
      "Epoch 40, train_loss: 0.4096, val_loss: 0.5203\n",
      "Epoch 41, train_loss: 0.3969, val_loss: 0.5178\n",
      "Epoch 42, train_loss: 0.3869, val_loss: 0.5152\n",
      "Epoch 43, train_loss: 0.3738, val_loss: 0.5124\n",
      "Epoch 44, train_loss: 0.3866, val_loss: 0.5101\n",
      "Epoch 45, train_loss: 0.3734, val_loss: 0.5076\n",
      "Epoch 46, train_loss: 0.3769, val_loss: 0.5048\n",
      "Epoch 47, train_loss: 0.3727, val_loss: 0.5027\n",
      "Epoch 48, train_loss: 0.3456, val_loss: 0.5002\n",
      "Epoch 49, train_loss: 0.3417, val_loss: 0.4974\n",
      "Epoch 50, train_loss: 0.3342, val_loss: 0.4943\n",
      "Epoch 51, train_loss: 0.3366, val_loss: 0.4920\n",
      "Epoch 52, train_loss: 0.3403, val_loss: 0.4899\n",
      "Epoch 53, train_loss: 0.3325, val_loss: 0.4877\n",
      "Epoch 54, train_loss: 0.3282, val_loss: 0.4855\n",
      "Epoch 55, train_loss: 0.3127, val_loss: 0.4842\n",
      "Epoch 56, train_loss: 0.2825, val_loss: 0.4827\n",
      "Epoch 57, train_loss: 0.3055, val_loss: 0.4811\n",
      "Epoch 58, train_loss: 0.3044, val_loss: 0.4806\n",
      "Epoch 59, train_loss: 0.2856, val_loss: 0.4799\n",
      "Epoch 60, train_loss: 0.2707, val_loss: 0.4785\n",
      "Epoch 61, train_loss: 0.2834, val_loss: 0.4762\n",
      "Epoch 62, train_loss: 0.2704, val_loss: 0.4756\n",
      "Epoch 63, train_loss: 0.2736, val_loss: 0.4748\n",
      "Epoch 64, train_loss: 0.2968, val_loss: 0.4733\n",
      "Epoch 65, train_loss: 0.2620, val_loss: 0.4730\n",
      "Epoch 66, train_loss: 0.2594, val_loss: 0.4717\n",
      "Epoch 67, train_loss: 0.2434, val_loss: 0.4706\n",
      "Epoch 68, train_loss: 0.2619, val_loss: 0.4699\n",
      "Epoch 69, train_loss: 0.2328, val_loss: 0.4691\n",
      "Epoch 70, train_loss: 0.2382, val_loss: 0.4681\n",
      "Epoch 71, train_loss: 0.2197, val_loss: 0.4671\n",
      "Epoch 72, train_loss: 0.2250, val_loss: 0.4661\n",
      "Epoch 73, train_loss: 0.2532, val_loss: 0.4649\n",
      "Epoch 74, train_loss: 0.2212, val_loss: 0.4645\n",
      "Epoch 75, train_loss: 0.2139, val_loss: 0.4635\n",
      "Epoch 76, train_loss: 0.2086, val_loss: 0.4631\n",
      "Epoch 77, train_loss: 0.2192, val_loss: 0.4624\n",
      "Epoch 78, train_loss: 0.1976, val_loss: 0.4621\n",
      "Epoch 79, train_loss: 0.2155, val_loss: 0.4613\n",
      "Epoch 80, train_loss: 0.2143, val_loss: 0.4610\n",
      "Epoch 81, train_loss: 0.2013, val_loss: 0.4613\n",
      "Epoch 82, train_loss: 0.1979, val_loss: 0.4613\n",
      "Epoch 83, train_loss: 0.2068, val_loss: 0.4609\n",
      "Epoch 84, train_loss: 0.1983, val_loss: 0.4621\n",
      "Epoch 85, train_loss: 0.1964, val_loss: 0.4620\n",
      "Epoch 86, train_loss: 0.2200, val_loss: 0.4623\n",
      "Epoch 87, train_loss: 0.1748, val_loss: 0.4623\n",
      "Epoch 88, train_loss: 0.1808, val_loss: 0.4619\n",
      "Epoch 89, train_loss: 0.1832, val_loss: 0.4617\n",
      "Epoch 90, train_loss: 0.1786, val_loss: 0.4608\n",
      "Epoch 91, train_loss: 0.1717, val_loss: 0.4609\n",
      "Epoch 92, train_loss: 0.1909, val_loss: 0.4611\n",
      "Epoch 93, train_loss: 0.1789, val_loss: 0.4622\n",
      "Epoch 94, train_loss: 0.1636, val_loss: 0.4622\n",
      "Epoch 95, train_loss: 0.1810, val_loss: 0.4618\n",
      "Epoch 96, train_loss: 0.1576, val_loss: 0.4622\n",
      "Epoch 97, train_loss: 0.1544, val_loss: 0.4631\n",
      "Early Stopping!\n",
      "Test the model on fold 4:\n",
      "{'roc_auc': 0.8939393939393939, 'pr_auc': 0.916182517666436, 'fmax': 0.8749950781526854}\n",
      "Evaluate pretrained model on disease class Renal (8/12)\n",
      "Starting Fold: 0\n",
      "Epoch 0, train_loss: 0.6919, val_loss: 0.6927\n",
      "Epoch 1, train_loss: 0.6957, val_loss: 0.6921\n",
      "Epoch 2, train_loss: 0.6872, val_loss: 0.6906\n",
      "Epoch 3, train_loss: 0.6806, val_loss: 0.6886\n",
      "Epoch 4, train_loss: 0.6673, val_loss: 0.6876\n",
      "Epoch 5, train_loss: 0.6732, val_loss: 0.6867\n",
      "Epoch 6, train_loss: 0.6621, val_loss: 0.6866\n",
      "Epoch 7, train_loss: 0.6549, val_loss: 0.6863\n",
      "Epoch 8, train_loss: 0.6477, val_loss: 0.6856\n",
      "Epoch 9, train_loss: 0.6440, val_loss: 0.6852\n",
      "Epoch 10, train_loss: 0.6442, val_loss: 0.6840\n",
      "Epoch 11, train_loss: 0.6392, val_loss: 0.6828\n",
      "Epoch 12, train_loss: 0.6275, val_loss: 0.6813\n",
      "Epoch 13, train_loss: 0.6296, val_loss: 0.6795\n",
      "Epoch 14, train_loss: 0.6183, val_loss: 0.6781\n",
      "Epoch 15, train_loss: 0.6227, val_loss: 0.6772\n",
      "Epoch 16, train_loss: 0.6148, val_loss: 0.6768\n",
      "Epoch 17, train_loss: 0.6233, val_loss: 0.6756\n",
      "Epoch 18, train_loss: 0.6051, val_loss: 0.6744\n",
      "Epoch 19, train_loss: 0.5975, val_loss: 0.6731\n",
      "Epoch 20, train_loss: 0.5931, val_loss: 0.6717\n",
      "Epoch 21, train_loss: 0.5851, val_loss: 0.6702\n",
      "Epoch 22, train_loss: 0.5891, val_loss: 0.6684\n",
      "Epoch 23, train_loss: 0.5894, val_loss: 0.6662\n",
      "Epoch 24, train_loss: 0.5720, val_loss: 0.6645\n",
      "Epoch 25, train_loss: 0.5710, val_loss: 0.6630\n",
      "Epoch 26, train_loss: 0.5647, val_loss: 0.6619\n",
      "Epoch 27, train_loss: 0.5639, val_loss: 0.6610\n",
      "Epoch 28, train_loss: 0.5617, val_loss: 0.6599\n",
      "Epoch 29, train_loss: 0.5528, val_loss: 0.6591\n",
      "Epoch 30, train_loss: 0.5558, val_loss: 0.6577\n",
      "Epoch 31, train_loss: 0.5417, val_loss: 0.6569\n",
      "Epoch 32, train_loss: 0.5252, val_loss: 0.6564\n",
      "Epoch 33, train_loss: 0.5295, val_loss: 0.6552\n",
      "Epoch 34, train_loss: 0.5237, val_loss: 0.6537\n",
      "Epoch 35, train_loss: 0.5203, val_loss: 0.6523\n",
      "Epoch 36, train_loss: 0.5117, val_loss: 0.6504\n",
      "Epoch 37, train_loss: 0.5082, val_loss: 0.6492\n",
      "Epoch 38, train_loss: 0.4952, val_loss: 0.6482\n",
      "Epoch 39, train_loss: 0.4941, val_loss: 0.6469\n",
      "Epoch 40, train_loss: 0.4962, val_loss: 0.6453\n",
      "Epoch 41, train_loss: 0.4979, val_loss: 0.6439\n",
      "Epoch 42, train_loss: 0.4801, val_loss: 0.6429\n",
      "Epoch 43, train_loss: 0.4717, val_loss: 0.6414\n",
      "Epoch 44, train_loss: 0.4728, val_loss: 0.6405\n",
      "Epoch 45, train_loss: 0.4607, val_loss: 0.6395\n",
      "Epoch 46, train_loss: 0.4684, val_loss: 0.6378\n",
      "Epoch 47, train_loss: 0.4533, val_loss: 0.6368\n",
      "Epoch 48, train_loss: 0.4518, val_loss: 0.6359\n",
      "Epoch 49, train_loss: 0.4426, val_loss: 0.6342\n",
      "Epoch 50, train_loss: 0.4423, val_loss: 0.6323\n",
      "Epoch 51, train_loss: 0.4231, val_loss: 0.6307\n",
      "Epoch 52, train_loss: 0.4123, val_loss: 0.6292\n",
      "Epoch 53, train_loss: 0.4158, val_loss: 0.6276\n",
      "Epoch 54, train_loss: 0.4184, val_loss: 0.6264\n",
      "Epoch 55, train_loss: 0.4098, val_loss: 0.6255\n",
      "Epoch 56, train_loss: 0.3986, val_loss: 0.6243\n",
      "Epoch 57, train_loss: 0.3936, val_loss: 0.6230\n",
      "Epoch 58, train_loss: 0.3951, val_loss: 0.6222\n",
      "Epoch 59, train_loss: 0.3860, val_loss: 0.6217\n",
      "Epoch 60, train_loss: 0.3738, val_loss: 0.6209\n",
      "Epoch 61, train_loss: 0.3793, val_loss: 0.6198\n",
      "Epoch 62, train_loss: 0.3683, val_loss: 0.6187\n",
      "Epoch 63, train_loss: 0.3752, val_loss: 0.6171\n",
      "Epoch 64, train_loss: 0.3579, val_loss: 0.6156\n",
      "Epoch 65, train_loss: 0.3511, val_loss: 0.6143\n",
      "Epoch 66, train_loss: 0.3508, val_loss: 0.6135\n",
      "Epoch 67, train_loss: 0.3441, val_loss: 0.6126\n",
      "Epoch 68, train_loss: 0.3356, val_loss: 0.6117\n",
      "Epoch 69, train_loss: 0.3377, val_loss: 0.6107\n",
      "Epoch 70, train_loss: 0.3289, val_loss: 0.6096\n",
      "Epoch 71, train_loss: 0.3226, val_loss: 0.6097\n",
      "Epoch 72, train_loss: 0.3201, val_loss: 0.6094\n",
      "Epoch 73, train_loss: 0.3051, val_loss: 0.6088\n",
      "Epoch 74, train_loss: 0.3099, val_loss: 0.6080\n",
      "Epoch 75, train_loss: 0.3114, val_loss: 0.6070\n",
      "Epoch 76, train_loss: 0.2993, val_loss: 0.6062\n",
      "Epoch 77, train_loss: 0.2903, val_loss: 0.6061\n",
      "Epoch 78, train_loss: 0.2863, val_loss: 0.6046\n",
      "Epoch 79, train_loss: 0.2891, val_loss: 0.6037\n",
      "Epoch 80, train_loss: 0.2765, val_loss: 0.6031\n",
      "Epoch 81, train_loss: 0.2818, val_loss: 0.6026\n",
      "Epoch 82, train_loss: 0.2850, val_loss: 0.6024\n",
      "Epoch 83, train_loss: 0.2631, val_loss: 0.6012\n",
      "Epoch 84, train_loss: 0.2626, val_loss: 0.6006\n",
      "Epoch 85, train_loss: 0.2587, val_loss: 0.6000\n",
      "Epoch 86, train_loss: 0.2559, val_loss: 0.6003\n",
      "Epoch 87, train_loss: 0.2572, val_loss: 0.6000\n",
      "Epoch 88, train_loss: 0.2591, val_loss: 0.5999\n",
      "Epoch 89, train_loss: 0.2473, val_loss: 0.5992\n",
      "Epoch 90, train_loss: 0.2503, val_loss: 0.5987\n",
      "Epoch 91, train_loss: 0.2473, val_loss: 0.5979\n",
      "Epoch 92, train_loss: 0.2355, val_loss: 0.5971\n",
      "Epoch 93, train_loss: 0.2328, val_loss: 0.5964\n",
      "Epoch 94, train_loss: 0.2332, val_loss: 0.5965\n",
      "Epoch 95, train_loss: 0.2277, val_loss: 0.5970\n",
      "Epoch 96, train_loss: 0.2264, val_loss: 0.5973\n",
      "Epoch 97, train_loss: 0.2249, val_loss: 0.5968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 98, train_loss: 0.2193, val_loss: 0.5964\n",
      "Epoch 99, train_loss: 0.2174, val_loss: 0.5964\n",
      "Test the model on fold 0:\n",
      "{'roc_auc': 0.7592592592592593, 'pr_auc': 0.8657285107401766, 'fmax': 0.8571379592116616}\n",
      "Starting Fold: 1\n",
      "Epoch 0, train_loss: 0.6904, val_loss: 0.6863\n",
      "Epoch 1, train_loss: 0.6863, val_loss: 0.6805\n",
      "Epoch 2, train_loss: 0.6736, val_loss: 0.6726\n",
      "Epoch 3, train_loss: 0.6696, val_loss: 0.6650\n",
      "Epoch 4, train_loss: 0.6681, val_loss: 0.6604\n",
      "Epoch 5, train_loss: 0.6593, val_loss: 0.6563\n",
      "Epoch 6, train_loss: 0.6521, val_loss: 0.6520\n",
      "Epoch 7, train_loss: 0.6493, val_loss: 0.6494\n",
      "Epoch 8, train_loss: 0.6417, val_loss: 0.6477\n",
      "Epoch 9, train_loss: 0.6303, val_loss: 0.6452\n",
      "Epoch 10, train_loss: 0.6380, val_loss: 0.6429\n",
      "Epoch 11, train_loss: 0.6291, val_loss: 0.6408\n",
      "Epoch 12, train_loss: 0.6177, val_loss: 0.6377\n",
      "Epoch 13, train_loss: 0.6190, val_loss: 0.6345\n",
      "Epoch 14, train_loss: 0.6106, val_loss: 0.6320\n",
      "Epoch 15, train_loss: 0.6117, val_loss: 0.6306\n",
      "Epoch 16, train_loss: 0.6084, val_loss: 0.6290\n",
      "Epoch 17, train_loss: 0.5998, val_loss: 0.6279\n",
      "Epoch 18, train_loss: 0.5953, val_loss: 0.6253\n",
      "Epoch 19, train_loss: 0.5956, val_loss: 0.6230\n",
      "Epoch 20, train_loss: 0.5894, val_loss: 0.6208\n",
      "Epoch 21, train_loss: 0.5906, val_loss: 0.6181\n",
      "Epoch 22, train_loss: 0.5847, val_loss: 0.6152\n",
      "Epoch 23, train_loss: 0.5802, val_loss: 0.6131\n",
      "Epoch 24, train_loss: 0.5800, val_loss: 0.6113\n",
      "Epoch 25, train_loss: 0.5638, val_loss: 0.6092\n",
      "Epoch 26, train_loss: 0.5603, val_loss: 0.6070\n",
      "Epoch 27, train_loss: 0.5604, val_loss: 0.6049\n",
      "Epoch 28, train_loss: 0.5622, val_loss: 0.6027\n",
      "Epoch 29, train_loss: 0.5548, val_loss: 0.6007\n",
      "Epoch 30, train_loss: 0.5468, val_loss: 0.5977\n",
      "Epoch 31, train_loss: 0.5501, val_loss: 0.5956\n",
      "Epoch 32, train_loss: 0.5411, val_loss: 0.5937\n",
      "Epoch 33, train_loss: 0.5313, val_loss: 0.5924\n",
      "Epoch 34, train_loss: 0.5322, val_loss: 0.5908\n",
      "Epoch 35, train_loss: 0.5307, val_loss: 0.5889\n",
      "Epoch 36, train_loss: 0.5261, val_loss: 0.5868\n",
      "Epoch 37, train_loss: 0.5231, val_loss: 0.5849\n",
      "Epoch 38, train_loss: 0.5133, val_loss: 0.5832\n",
      "Epoch 39, train_loss: 0.5098, val_loss: 0.5809\n",
      "Epoch 40, train_loss: 0.5141, val_loss: 0.5800\n",
      "Epoch 41, train_loss: 0.5024, val_loss: 0.5782\n",
      "Epoch 42, train_loss: 0.5067, val_loss: 0.5766\n",
      "Epoch 43, train_loss: 0.4978, val_loss: 0.5743\n",
      "Epoch 44, train_loss: 0.4938, val_loss: 0.5726\n",
      "Epoch 45, train_loss: 0.4906, val_loss: 0.5710\n",
      "Epoch 46, train_loss: 0.4825, val_loss: 0.5684\n",
      "Epoch 47, train_loss: 0.4929, val_loss: 0.5673\n",
      "Epoch 48, train_loss: 0.4786, val_loss: 0.5657\n",
      "Epoch 49, train_loss: 0.4865, val_loss: 0.5639\n",
      "Epoch 50, train_loss: 0.4745, val_loss: 0.5620\n",
      "Epoch 51, train_loss: 0.4697, val_loss: 0.5601\n",
      "Epoch 52, train_loss: 0.4632, val_loss: 0.5583\n",
      "Epoch 53, train_loss: 0.4575, val_loss: 0.5562\n",
      "Epoch 54, train_loss: 0.4495, val_loss: 0.5544\n",
      "Epoch 55, train_loss: 0.4396, val_loss: 0.5520\n",
      "Epoch 56, train_loss: 0.4511, val_loss: 0.5509\n",
      "Epoch 57, train_loss: 0.4435, val_loss: 0.5489\n",
      "Epoch 58, train_loss: 0.4313, val_loss: 0.5472\n",
      "Epoch 59, train_loss: 0.4324, val_loss: 0.5454\n",
      "Epoch 60, train_loss: 0.4339, val_loss: 0.5430\n",
      "Epoch 61, train_loss: 0.4181, val_loss: 0.5407\n",
      "Epoch 62, train_loss: 0.4199, val_loss: 0.5386\n",
      "Epoch 63, train_loss: 0.4114, val_loss: 0.5370\n",
      "Epoch 64, train_loss: 0.4027, val_loss: 0.5355\n",
      "Epoch 65, train_loss: 0.3973, val_loss: 0.5339\n",
      "Epoch 66, train_loss: 0.4069, val_loss: 0.5323\n",
      "Epoch 67, train_loss: 0.3943, val_loss: 0.5309\n",
      "Epoch 68, train_loss: 0.3978, val_loss: 0.5296\n",
      "Epoch 69, train_loss: 0.3788, val_loss: 0.5282\n",
      "Epoch 70, train_loss: 0.3791, val_loss: 0.5260\n",
      "Epoch 71, train_loss: 0.3833, val_loss: 0.5241\n",
      "Epoch 72, train_loss: 0.3721, val_loss: 0.5222\n",
      "Epoch 73, train_loss: 0.3749, val_loss: 0.5200\n",
      "Epoch 74, train_loss: 0.3652, val_loss: 0.5185\n",
      "Epoch 75, train_loss: 0.3683, val_loss: 0.5172\n",
      "Epoch 76, train_loss: 0.3637, val_loss: 0.5163\n",
      "Epoch 77, train_loss: 0.3625, val_loss: 0.5147\n",
      "Epoch 78, train_loss: 0.3417, val_loss: 0.5132\n",
      "Epoch 79, train_loss: 0.3401, val_loss: 0.5129\n",
      "Epoch 80, train_loss: 0.3422, val_loss: 0.5122\n",
      "Epoch 81, train_loss: 0.3306, val_loss: 0.5102\n",
      "Epoch 82, train_loss: 0.3387, val_loss: 0.5086\n",
      "Epoch 83, train_loss: 0.3340, val_loss: 0.5071\n",
      "Epoch 84, train_loss: 0.3199, val_loss: 0.5055\n",
      "Epoch 85, train_loss: 0.3246, val_loss: 0.5040\n",
      "Epoch 86, train_loss: 0.3176, val_loss: 0.5025\n",
      "Epoch 87, train_loss: 0.3217, val_loss: 0.5004\n",
      "Epoch 88, train_loss: 0.3089, val_loss: 0.4983\n",
      "Epoch 89, train_loss: 0.3004, val_loss: 0.4973\n",
      "Epoch 90, train_loss: 0.2976, val_loss: 0.4958\n",
      "Epoch 91, train_loss: 0.2954, val_loss: 0.4948\n",
      "Epoch 92, train_loss: 0.2915, val_loss: 0.4932\n",
      "Epoch 93, train_loss: 0.2883, val_loss: 0.4929\n",
      "Epoch 94, train_loss: 0.2879, val_loss: 0.4925\n",
      "Epoch 95, train_loss: 0.2855, val_loss: 0.4919\n",
      "Epoch 96, train_loss: 0.2826, val_loss: 0.4907\n",
      "Epoch 97, train_loss: 0.2788, val_loss: 0.4901\n",
      "Epoch 98, train_loss: 0.2745, val_loss: 0.4885\n",
      "Epoch 99, train_loss: 0.2613, val_loss: 0.4871\n",
      "Test the model on fold 1:\n",
      "{'roc_auc': 0.8500000000000001, 'pr_auc': 0.8438679923877294, 'fmax': 0.7999950080311499}\n",
      "Starting Fold: 2\n",
      "Epoch 0, train_loss: 0.6997, val_loss: 0.6880\n",
      "Epoch 1, train_loss: 0.6817, val_loss: 0.6877\n",
      "Epoch 2, train_loss: 0.6809, val_loss: 0.6879\n",
      "Epoch 3, train_loss: 0.6723, val_loss: 0.6876\n",
      "Epoch 4, train_loss: 0.6657, val_loss: 0.6876\n",
      "Epoch 5, train_loss: 0.6549, val_loss: 0.6871\n",
      "Epoch 6, train_loss: 0.6505, val_loss: 0.6868\n",
      "Epoch 7, train_loss: 0.6409, val_loss: 0.6864\n",
      "Epoch 8, train_loss: 0.6349, val_loss: 0.6861\n",
      "Epoch 9, train_loss: 0.6300, val_loss: 0.6858\n",
      "Epoch 10, train_loss: 0.6240, val_loss: 0.6857\n",
      "Epoch 11, train_loss: 0.6197, val_loss: 0.6851\n",
      "Epoch 12, train_loss: 0.6170, val_loss: 0.6854\n",
      "Epoch 13, train_loss: 0.6134, val_loss: 0.6852\n",
      "Epoch 14, train_loss: 0.6097, val_loss: 0.6850\n",
      "Epoch 15, train_loss: 0.6024, val_loss: 0.6849\n",
      "Epoch 16, train_loss: 0.5998, val_loss: 0.6844\n",
      "Epoch 17, train_loss: 0.5901, val_loss: 0.6850\n",
      "Epoch 18, train_loss: 0.5867, val_loss: 0.6851\n",
      "Epoch 19, train_loss: 0.5825, val_loss: 0.6844\n",
      "Epoch 20, train_loss: 0.5745, val_loss: 0.6837\n",
      "Epoch 21, train_loss: 0.5798, val_loss: 0.6835\n",
      "Epoch 22, train_loss: 0.5643, val_loss: 0.6836\n",
      "Epoch 23, train_loss: 0.5702, val_loss: 0.6833\n",
      "Epoch 24, train_loss: 0.5673, val_loss: 0.6828\n",
      "Epoch 25, train_loss: 0.5621, val_loss: 0.6827\n",
      "Epoch 26, train_loss: 0.5562, val_loss: 0.6824\n",
      "Epoch 27, train_loss: 0.5539, val_loss: 0.6832\n",
      "Epoch 28, train_loss: 0.5429, val_loss: 0.6841\n",
      "Epoch 29, train_loss: 0.5444, val_loss: 0.6846\n",
      "Epoch 30, train_loss: 0.5345, val_loss: 0.6845\n",
      "Epoch 31, train_loss: 0.5385, val_loss: 0.6843\n",
      "Epoch 32, train_loss: 0.5345, val_loss: 0.6838\n",
      "Epoch 33, train_loss: 0.5234, val_loss: 0.6833\n",
      "Epoch 34, train_loss: 0.5303, val_loss: 0.6828\n",
      "Epoch 35, train_loss: 0.5165, val_loss: 0.6825\n",
      "Epoch 36, train_loss: 0.5172, val_loss: 0.6822\n",
      "Epoch 37, train_loss: 0.5077, val_loss: 0.6823\n",
      "Epoch 38, train_loss: 0.4971, val_loss: 0.6825\n",
      "Epoch 39, train_loss: 0.5022, val_loss: 0.6820\n",
      "Epoch 40, train_loss: 0.4962, val_loss: 0.6812\n",
      "Epoch 41, train_loss: 0.4911, val_loss: 0.6799\n",
      "Epoch 42, train_loss: 0.4813, val_loss: 0.6797\n",
      "Epoch 43, train_loss: 0.4833, val_loss: 0.6803\n",
      "Epoch 44, train_loss: 0.4752, val_loss: 0.6805\n",
      "Epoch 45, train_loss: 0.4793, val_loss: 0.6805\n",
      "Epoch 46, train_loss: 0.4623, val_loss: 0.6803\n",
      "Epoch 47, train_loss: 0.4617, val_loss: 0.6798\n",
      "Epoch 48, train_loss: 0.4635, val_loss: 0.6792\n",
      "Epoch 49, train_loss: 0.4509, val_loss: 0.6789\n",
      "Epoch 50, train_loss: 0.4562, val_loss: 0.6786\n",
      "Epoch 51, train_loss: 0.4502, val_loss: 0.6788\n",
      "Epoch 52, train_loss: 0.4387, val_loss: 0.6787\n",
      "Epoch 53, train_loss: 0.4358, val_loss: 0.6783\n",
      "Epoch 54, train_loss: 0.4299, val_loss: 0.6789\n",
      "Epoch 55, train_loss: 0.4245, val_loss: 0.6791\n",
      "Epoch 56, train_loss: 0.4108, val_loss: 0.6790\n",
      "Epoch 57, train_loss: 0.4171, val_loss: 0.6784\n",
      "Epoch 58, train_loss: 0.4057, val_loss: 0.6768\n",
      "Epoch 59, train_loss: 0.4014, val_loss: 0.6756\n",
      "Epoch 60, train_loss: 0.4014, val_loss: 0.6751\n",
      "Epoch 61, train_loss: 0.3878, val_loss: 0.6753\n",
      "Epoch 62, train_loss: 0.3913, val_loss: 0.6759\n",
      "Epoch 63, train_loss: 0.3860, val_loss: 0.6762\n",
      "Epoch 64, train_loss: 0.3719, val_loss: 0.6757\n",
      "Epoch 65, train_loss: 0.3726, val_loss: 0.6760\n",
      "Epoch 66, train_loss: 0.3724, val_loss: 0.6770\n",
      "Epoch 67, train_loss: 0.3725, val_loss: 0.6760\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 68, train_loss: 0.3550, val_loss: 0.6758\n",
      "Epoch 69, train_loss: 0.3553, val_loss: 0.6751\n",
      "Epoch 70, train_loss: 0.3455, val_loss: 0.6754\n",
      "Epoch 71, train_loss: 0.3466, val_loss: 0.6749\n",
      "Epoch 72, train_loss: 0.3379, val_loss: 0.6748\n",
      "Epoch 73, train_loss: 0.3450, val_loss: 0.6741\n",
      "Epoch 74, train_loss: 0.3225, val_loss: 0.6732\n",
      "Epoch 75, train_loss: 0.3252, val_loss: 0.6727\n",
      "Epoch 76, train_loss: 0.3262, val_loss: 0.6723\n",
      "Epoch 77, train_loss: 0.3144, val_loss: 0.6717\n",
      "Epoch 78, train_loss: 0.3107, val_loss: 0.6707\n",
      "Epoch 79, train_loss: 0.3126, val_loss: 0.6708\n",
      "Epoch 80, train_loss: 0.3014, val_loss: 0.6715\n",
      "Epoch 81, train_loss: 0.3040, val_loss: 0.6713\n",
      "Epoch 82, train_loss: 0.2922, val_loss: 0.6713\n",
      "Epoch 83, train_loss: 0.2913, val_loss: 0.6701\n",
      "Epoch 84, train_loss: 0.2892, val_loss: 0.6697\n",
      "Epoch 85, train_loss: 0.2874, val_loss: 0.6693\n",
      "Epoch 86, train_loss: 0.2822, val_loss: 0.6684\n",
      "Epoch 87, train_loss: 0.2704, val_loss: 0.6688\n",
      "Epoch 88, train_loss: 0.2807, val_loss: 0.6681\n",
      "Epoch 89, train_loss: 0.2536, val_loss: 0.6679\n",
      "Epoch 90, train_loss: 0.2703, val_loss: 0.6673\n",
      "Epoch 91, train_loss: 0.2575, val_loss: 0.6662\n",
      "Epoch 92, train_loss: 0.2556, val_loss: 0.6651\n",
      "Epoch 93, train_loss: 0.2541, val_loss: 0.6651\n",
      "Epoch 94, train_loss: 0.2430, val_loss: 0.6642\n",
      "Epoch 95, train_loss: 0.2409, val_loss: 0.6643\n",
      "Epoch 96, train_loss: 0.2438, val_loss: 0.6639\n",
      "Epoch 97, train_loss: 0.2415, val_loss: 0.6629\n",
      "Epoch 98, train_loss: 0.2436, val_loss: 0.6629\n",
      "Epoch 99, train_loss: 0.2421, val_loss: 0.6636\n",
      "Test the model on fold 2:\n",
      "{'roc_auc': 0.7818181818181819, 'pr_auc': 0.8491105528458469, 'fmax': 0.8275812128719913}\n",
      "Starting Fold: 3\n",
      "Epoch 0, train_loss: 0.6935, val_loss: 0.6853\n",
      "Epoch 1, train_loss: 0.6823, val_loss: 0.6858\n",
      "Epoch 2, train_loss: 0.6748, val_loss: 0.6859\n",
      "Epoch 3, train_loss: 0.6672, val_loss: 0.6853\n",
      "Epoch 4, train_loss: 0.6531, val_loss: 0.6852\n",
      "Epoch 5, train_loss: 0.6404, val_loss: 0.6845\n",
      "Epoch 6, train_loss: 0.6334, val_loss: 0.6830\n",
      "Epoch 7, train_loss: 0.6261, val_loss: 0.6828\n",
      "Epoch 8, train_loss: 0.6190, val_loss: 0.6823\n",
      "Epoch 9, train_loss: 0.6286, val_loss: 0.6825\n",
      "Epoch 10, train_loss: 0.6084, val_loss: 0.6831\n",
      "Epoch 11, train_loss: 0.6138, val_loss: 0.6831\n",
      "Epoch 12, train_loss: 0.5968, val_loss: 0.6843\n",
      "Epoch 13, train_loss: 0.5982, val_loss: 0.6848\n",
      "Epoch 14, train_loss: 0.5971, val_loss: 0.6843\n",
      "Epoch 15, train_loss: 0.5972, val_loss: 0.6837\n",
      "Epoch 16, train_loss: 0.5852, val_loss: 0.6831\n",
      "Epoch 17, train_loss: 0.5805, val_loss: 0.6827\n",
      "Epoch 18, train_loss: 0.5788, val_loss: 0.6831\n",
      "Epoch 19, train_loss: 0.5738, val_loss: 0.6827\n",
      "Epoch 20, train_loss: 0.5674, val_loss: 0.6809\n",
      "Epoch 21, train_loss: 0.5485, val_loss: 0.6794\n",
      "Epoch 22, train_loss: 0.5583, val_loss: 0.6789\n",
      "Epoch 23, train_loss: 0.5491, val_loss: 0.6783\n",
      "Epoch 24, train_loss: 0.5474, val_loss: 0.6774\n",
      "Epoch 25, train_loss: 0.5397, val_loss: 0.6780\n",
      "Epoch 26, train_loss: 0.5398, val_loss: 0.6791\n",
      "Epoch 27, train_loss: 0.5443, val_loss: 0.6791\n",
      "Epoch 28, train_loss: 0.5218, val_loss: 0.6795\n",
      "Epoch 29, train_loss: 0.5211, val_loss: 0.6798\n",
      "Epoch 30, train_loss: 0.5311, val_loss: 0.6800\n",
      "Epoch 31, train_loss: 0.5159, val_loss: 0.6799\n",
      "Epoch 32, train_loss: 0.5178, val_loss: 0.6800\n",
      "Epoch 33, train_loss: 0.5088, val_loss: 0.6800\n",
      "Epoch 34, train_loss: 0.5114, val_loss: 0.6798\n",
      "Epoch 35, train_loss: 0.4979, val_loss: 0.6797\n",
      "Epoch 36, train_loss: 0.5023, val_loss: 0.6787\n",
      "Epoch 37, train_loss: 0.4952, val_loss: 0.6785\n",
      "Epoch 38, train_loss: 0.4931, val_loss: 0.6781\n",
      "Epoch 39, train_loss: 0.4819, val_loss: 0.6784\n",
      "Epoch 40, train_loss: 0.4818, val_loss: 0.6771\n",
      "Epoch 41, train_loss: 0.4720, val_loss: 0.6774\n",
      "Epoch 42, train_loss: 0.4623, val_loss: 0.6773\n",
      "Epoch 43, train_loss: 0.4555, val_loss: 0.6766\n",
      "Epoch 44, train_loss: 0.4580, val_loss: 0.6774\n",
      "Epoch 45, train_loss: 0.4652, val_loss: 0.6788\n",
      "Epoch 46, train_loss: 0.4553, val_loss: 0.6802\n",
      "Early Stopping!\n",
      "Test the model on fold 3:\n",
      "{'roc_auc': 0.7818181818181819, 'pr_auc': 0.7390522504827318, 'fmax': 0.7857095153350855}\n",
      "Starting Fold: 4\n",
      "Epoch 0, train_loss: 0.6901, val_loss: 0.6831\n",
      "Epoch 1, train_loss: 0.6863, val_loss: 0.6813\n",
      "Epoch 2, train_loss: 0.6737, val_loss: 0.6791\n",
      "Epoch 3, train_loss: 0.6698, val_loss: 0.6768\n",
      "Epoch 4, train_loss: 0.6626, val_loss: 0.6746\n",
      "Epoch 5, train_loss: 0.6482, val_loss: 0.6723\n",
      "Epoch 6, train_loss: 0.6379, val_loss: 0.6699\n",
      "Epoch 7, train_loss: 0.6425, val_loss: 0.6678\n",
      "Epoch 8, train_loss: 0.6296, val_loss: 0.6652\n",
      "Epoch 9, train_loss: 0.6246, val_loss: 0.6632\n",
      "Epoch 10, train_loss: 0.6087, val_loss: 0.6623\n",
      "Epoch 11, train_loss: 0.6055, val_loss: 0.6612\n",
      "Epoch 12, train_loss: 0.5964, val_loss: 0.6596\n",
      "Epoch 13, train_loss: 0.6092, val_loss: 0.6580\n",
      "Epoch 14, train_loss: 0.5943, val_loss: 0.6567\n",
      "Epoch 15, train_loss: 0.5913, val_loss: 0.6559\n",
      "Epoch 16, train_loss: 0.5873, val_loss: 0.6553\n",
      "Epoch 17, train_loss: 0.5819, val_loss: 0.6545\n",
      "Epoch 18, train_loss: 0.5815, val_loss: 0.6531\n",
      "Epoch 19, train_loss: 0.5684, val_loss: 0.6512\n",
      "Epoch 20, train_loss: 0.5726, val_loss: 0.6497\n",
      "Epoch 21, train_loss: 0.5625, val_loss: 0.6487\n",
      "Epoch 22, train_loss: 0.5542, val_loss: 0.6478\n",
      "Epoch 23, train_loss: 0.5543, val_loss: 0.6467\n",
      "Epoch 24, train_loss: 0.5497, val_loss: 0.6460\n",
      "Epoch 25, train_loss: 0.5399, val_loss: 0.6446\n",
      "Epoch 26, train_loss: 0.5386, val_loss: 0.6435\n",
      "Epoch 27, train_loss: 0.5239, val_loss: 0.6421\n",
      "Epoch 28, train_loss: 0.5323, val_loss: 0.6406\n",
      "Epoch 29, train_loss: 0.5236, val_loss: 0.6392\n",
      "Epoch 30, train_loss: 0.5225, val_loss: 0.6383\n",
      "Epoch 31, train_loss: 0.5061, val_loss: 0.6375\n",
      "Epoch 32, train_loss: 0.5119, val_loss: 0.6365\n",
      "Epoch 33, train_loss: 0.5155, val_loss: 0.6357\n",
      "Epoch 34, train_loss: 0.5106, val_loss: 0.6345\n",
      "Epoch 35, train_loss: 0.4969, val_loss: 0.6330\n",
      "Epoch 36, train_loss: 0.4910, val_loss: 0.6314\n",
      "Epoch 37, train_loss: 0.4866, val_loss: 0.6301\n",
      "Epoch 38, train_loss: 0.4870, val_loss: 0.6287\n",
      "Epoch 39, train_loss: 0.4820, val_loss: 0.6274\n",
      "Epoch 40, train_loss: 0.4798, val_loss: 0.6267\n",
      "Epoch 41, train_loss: 0.4714, val_loss: 0.6255\n",
      "Epoch 42, train_loss: 0.4643, val_loss: 0.6246\n",
      "Epoch 43, train_loss: 0.4606, val_loss: 0.6236\n",
      "Epoch 44, train_loss: 0.4640, val_loss: 0.6228\n",
      "Epoch 45, train_loss: 0.4431, val_loss: 0.6217\n",
      "Epoch 46, train_loss: 0.4439, val_loss: 0.6202\n",
      "Epoch 47, train_loss: 0.4455, val_loss: 0.6187\n",
      "Epoch 48, train_loss: 0.4436, val_loss: 0.6171\n",
      "Epoch 49, train_loss: 0.4365, val_loss: 0.6158\n",
      "Epoch 50, train_loss: 0.4234, val_loss: 0.6144\n",
      "Epoch 51, train_loss: 0.4260, val_loss: 0.6129\n",
      "Epoch 52, train_loss: 0.4140, val_loss: 0.6118\n",
      "Epoch 53, train_loss: 0.4089, val_loss: 0.6107\n",
      "Epoch 54, train_loss: 0.4057, val_loss: 0.6099\n",
      "Epoch 55, train_loss: 0.3968, val_loss: 0.6091\n",
      "Epoch 56, train_loss: 0.4012, val_loss: 0.6084\n",
      "Epoch 57, train_loss: 0.3897, val_loss: 0.6071\n",
      "Epoch 58, train_loss: 0.3927, val_loss: 0.6061\n",
      "Epoch 59, train_loss: 0.3876, val_loss: 0.6047\n",
      "Epoch 60, train_loss: 0.3783, val_loss: 0.6030\n",
      "Epoch 61, train_loss: 0.3835, val_loss: 0.6015\n",
      "Epoch 62, train_loss: 0.3695, val_loss: 0.5999\n",
      "Epoch 63, train_loss: 0.3653, val_loss: 0.5986\n",
      "Epoch 64, train_loss: 0.3593, val_loss: 0.5975\n",
      "Epoch 65, train_loss: 0.3528, val_loss: 0.5963\n",
      "Epoch 66, train_loss: 0.3551, val_loss: 0.5956\n",
      "Epoch 67, train_loss: 0.3423, val_loss: 0.5949\n",
      "Epoch 68, train_loss: 0.3384, val_loss: 0.5940\n",
      "Epoch 69, train_loss: 0.3427, val_loss: 0.5928\n",
      "Epoch 70, train_loss: 0.3310, val_loss: 0.5922\n",
      "Epoch 71, train_loss: 0.3253, val_loss: 0.5911\n",
      "Epoch 72, train_loss: 0.3323, val_loss: 0.5903\n",
      "Epoch 73, train_loss: 0.3229, val_loss: 0.5890\n",
      "Epoch 74, train_loss: 0.3146, val_loss: 0.5876\n",
      "Epoch 75, train_loss: 0.3142, val_loss: 0.5869\n",
      "Epoch 76, train_loss: 0.3185, val_loss: 0.5861\n",
      "Epoch 77, train_loss: 0.2984, val_loss: 0.5845\n",
      "Epoch 78, train_loss: 0.2993, val_loss: 0.5830\n",
      "Epoch 79, train_loss: 0.2984, val_loss: 0.5819\n",
      "Epoch 80, train_loss: 0.2931, val_loss: 0.5811\n",
      "Epoch 81, train_loss: 0.2878, val_loss: 0.5803\n",
      "Epoch 82, train_loss: 0.2822, val_loss: 0.5801\n",
      "Epoch 83, train_loss: 0.2859, val_loss: 0.5785\n",
      "Epoch 84, train_loss: 0.2744, val_loss: 0.5772\n",
      "Epoch 85, train_loss: 0.2747, val_loss: 0.5760\n",
      "Epoch 86, train_loss: 0.2589, val_loss: 0.5750\n",
      "Epoch 87, train_loss: 0.2630, val_loss: 0.5747\n",
      "Epoch 88, train_loss: 0.2700, val_loss: 0.5743\n",
      "Epoch 89, train_loss: 0.2606, val_loss: 0.5736\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 90, train_loss: 0.2608, val_loss: 0.5728\n",
      "Epoch 91, train_loss: 0.2564, val_loss: 0.5719\n",
      "Epoch 92, train_loss: 0.2456, val_loss: 0.5709\n",
      "Epoch 93, train_loss: 0.2459, val_loss: 0.5691\n",
      "Epoch 94, train_loss: 0.2415, val_loss: 0.5683\n",
      "Epoch 95, train_loss: 0.2344, val_loss: 0.5684\n",
      "Epoch 96, train_loss: 0.2422, val_loss: 0.5675\n",
      "Epoch 97, train_loss: 0.2347, val_loss: 0.5662\n",
      "Epoch 98, train_loss: 0.2331, val_loss: 0.5652\n",
      "Epoch 99, train_loss: 0.2281, val_loss: 0.5644\n",
      "Test the model on fold 4:\n",
      "{'roc_auc': 0.675, 'pr_auc': 0.7033508991008991, 'fmax': 0.6363586777245892}\n",
      "Evaluate pretrained model on disease class Hematological (9/12)\n",
      "Starting Fold: 0\n",
      "Epoch 0, train_loss: 0.6899, val_loss: 0.6797\n",
      "Epoch 1, train_loss: 0.6791, val_loss: 0.6699\n",
      "Epoch 2, train_loss: 0.6697, val_loss: 0.6598\n",
      "Epoch 3, train_loss: 0.6543, val_loss: 0.6500\n",
      "Epoch 4, train_loss: 0.6489, val_loss: 0.6406\n",
      "Epoch 5, train_loss: 0.6350, val_loss: 0.6307\n",
      "Epoch 6, train_loss: 0.6290, val_loss: 0.6216\n",
      "Epoch 7, train_loss: 0.6178, val_loss: 0.6124\n",
      "Epoch 8, train_loss: 0.6098, val_loss: 0.6038\n",
      "Epoch 9, train_loss: 0.6002, val_loss: 0.5964\n",
      "Epoch 10, train_loss: 0.5922, val_loss: 0.5904\n",
      "Epoch 11, train_loss: 0.5756, val_loss: 0.5839\n",
      "Epoch 12, train_loss: 0.5736, val_loss: 0.5759\n",
      "Epoch 13, train_loss: 0.5572, val_loss: 0.5678\n",
      "Epoch 14, train_loss: 0.5502, val_loss: 0.5603\n",
      "Epoch 15, train_loss: 0.5402, val_loss: 0.5537\n",
      "Epoch 16, train_loss: 0.5287, val_loss: 0.5474\n",
      "Epoch 17, train_loss: 0.5188, val_loss: 0.5409\n",
      "Epoch 18, train_loss: 0.5111, val_loss: 0.5339\n",
      "Epoch 19, train_loss: 0.5018, val_loss: 0.5297\n",
      "Epoch 20, train_loss: 0.4886, val_loss: 0.5232\n",
      "Epoch 21, train_loss: 0.4812, val_loss: 0.5185\n",
      "Epoch 22, train_loss: 0.4732, val_loss: 0.5125\n",
      "Epoch 23, train_loss: 0.4594, val_loss: 0.5095\n",
      "Epoch 24, train_loss: 0.4470, val_loss: 0.5055\n",
      "Epoch 25, train_loss: 0.4404, val_loss: 0.5012\n",
      "Epoch 26, train_loss: 0.4345, val_loss: 0.4963\n",
      "Epoch 27, train_loss: 0.4211, val_loss: 0.4934\n",
      "Epoch 28, train_loss: 0.4155, val_loss: 0.4904\n",
      "Epoch 29, train_loss: 0.4042, val_loss: 0.4862\n",
      "Epoch 30, train_loss: 0.3964, val_loss: 0.4854\n",
      "Epoch 31, train_loss: 0.3836, val_loss: 0.4821\n",
      "Epoch 32, train_loss: 0.3765, val_loss: 0.4795\n",
      "Epoch 33, train_loss: 0.3675, val_loss: 0.4759\n",
      "Epoch 34, train_loss: 0.3610, val_loss: 0.4750\n",
      "Epoch 35, train_loss: 0.3512, val_loss: 0.4729\n",
      "Epoch 36, train_loss: 0.3440, val_loss: 0.4709\n",
      "Epoch 37, train_loss: 0.3421, val_loss: 0.4699\n",
      "Epoch 38, train_loss: 0.3239, val_loss: 0.4669\n",
      "Epoch 39, train_loss: 0.3142, val_loss: 0.4665\n",
      "Epoch 40, train_loss: 0.3111, val_loss: 0.4666\n",
      "Epoch 41, train_loss: 0.3066, val_loss: 0.4642\n",
      "Epoch 42, train_loss: 0.2935, val_loss: 0.4624\n",
      "Epoch 43, train_loss: 0.2876, val_loss: 0.4625\n",
      "Epoch 44, train_loss: 0.2795, val_loss: 0.4619\n",
      "Epoch 45, train_loss: 0.2738, val_loss: 0.4615\n",
      "Epoch 46, train_loss: 0.2738, val_loss: 0.4600\n",
      "Epoch 47, train_loss: 0.2639, val_loss: 0.4600\n",
      "Epoch 48, train_loss: 0.2614, val_loss: 0.4594\n",
      "Epoch 49, train_loss: 0.2547, val_loss: 0.4589\n",
      "Epoch 50, train_loss: 0.2481, val_loss: 0.4604\n",
      "Epoch 51, train_loss: 0.2461, val_loss: 0.4581\n",
      "Epoch 52, train_loss: 0.2332, val_loss: 0.4575\n",
      "Epoch 53, train_loss: 0.2323, val_loss: 0.4589\n",
      "Epoch 54, train_loss: 0.2263, val_loss: 0.4586\n",
      "Epoch 55, train_loss: 0.2219, val_loss: 0.4586\n",
      "Epoch 56, train_loss: 0.2138, val_loss: 0.4588\n",
      "Epoch 57, train_loss: 0.2146, val_loss: 0.4587\n",
      "Epoch 58, train_loss: 0.2109, val_loss: 0.4603\n",
      "Epoch 59, train_loss: 0.2028, val_loss: 0.4584\n",
      "Epoch 60, train_loss: 0.2040, val_loss: 0.4593\n",
      "Epoch 61, train_loss: 0.2031, val_loss: 0.4608\n",
      "Early Stopping!\n",
      "Test the model on fold 0:\n",
      "{'roc_auc': 0.8323863636363636, 'pr_auc': 0.8882238890671154, 'fmax': 0.8275812075873412}\n",
      "Starting Fold: 1\n",
      "Epoch 0, train_loss: 0.6912, val_loss: 0.6846\n",
      "Epoch 1, train_loss: 0.6697, val_loss: 0.6746\n",
      "Epoch 2, train_loss: 0.6579, val_loss: 0.6672\n",
      "Epoch 3, train_loss: 0.6421, val_loss: 0.6592\n",
      "Epoch 4, train_loss: 0.6314, val_loss: 0.6524\n",
      "Epoch 5, train_loss: 0.6248, val_loss: 0.6464\n",
      "Epoch 6, train_loss: 0.6184, val_loss: 0.6413\n",
      "Epoch 7, train_loss: 0.6032, val_loss: 0.6372\n",
      "Epoch 8, train_loss: 0.5973, val_loss: 0.6329\n",
      "Epoch 9, train_loss: 0.5886, val_loss: 0.6276\n",
      "Epoch 10, train_loss: 0.5838, val_loss: 0.6229\n",
      "Epoch 11, train_loss: 0.5717, val_loss: 0.6167\n",
      "Epoch 12, train_loss: 0.5663, val_loss: 0.6143\n",
      "Epoch 13, train_loss: 0.5546, val_loss: 0.6117\n",
      "Epoch 14, train_loss: 0.5510, val_loss: 0.6073\n",
      "Epoch 15, train_loss: 0.5448, val_loss: 0.6026\n",
      "Epoch 16, train_loss: 0.5410, val_loss: 0.5984\n",
      "Epoch 17, train_loss: 0.5299, val_loss: 0.5951\n",
      "Epoch 18, train_loss: 0.5180, val_loss: 0.5904\n",
      "Epoch 19, train_loss: 0.5207, val_loss: 0.5872\n",
      "Epoch 20, train_loss: 0.5072, val_loss: 0.5824\n",
      "Epoch 21, train_loss: 0.5005, val_loss: 0.5783\n",
      "Epoch 22, train_loss: 0.4942, val_loss: 0.5757\n",
      "Epoch 23, train_loss: 0.4918, val_loss: 0.5737\n",
      "Epoch 24, train_loss: 0.4831, val_loss: 0.5685\n",
      "Epoch 25, train_loss: 0.4704, val_loss: 0.5647\n",
      "Epoch 26, train_loss: 0.4708, val_loss: 0.5611\n",
      "Epoch 27, train_loss: 0.4613, val_loss: 0.5587\n",
      "Epoch 28, train_loss: 0.4556, val_loss: 0.5547\n",
      "Epoch 29, train_loss: 0.4451, val_loss: 0.5520\n",
      "Epoch 30, train_loss: 0.4374, val_loss: 0.5480\n",
      "Epoch 31, train_loss: 0.4342, val_loss: 0.5458\n",
      "Epoch 32, train_loss: 0.4258, val_loss: 0.5421\n",
      "Epoch 33, train_loss: 0.4183, val_loss: 0.5389\n",
      "Epoch 34, train_loss: 0.4075, val_loss: 0.5370\n",
      "Epoch 35, train_loss: 0.4025, val_loss: 0.5335\n",
      "Epoch 36, train_loss: 0.3959, val_loss: 0.5301\n",
      "Epoch 37, train_loss: 0.3916, val_loss: 0.5273\n",
      "Epoch 38, train_loss: 0.3808, val_loss: 0.5262\n",
      "Epoch 39, train_loss: 0.3732, val_loss: 0.5246\n",
      "Epoch 40, train_loss: 0.3744, val_loss: 0.5197\n",
      "Epoch 41, train_loss: 0.3617, val_loss: 0.5174\n",
      "Epoch 42, train_loss: 0.3565, val_loss: 0.5142\n",
      "Epoch 43, train_loss: 0.3464, val_loss: 0.5142\n",
      "Epoch 44, train_loss: 0.3416, val_loss: 0.5122\n",
      "Epoch 45, train_loss: 0.3325, val_loss: 0.5091\n",
      "Epoch 46, train_loss: 0.3291, val_loss: 0.5082\n",
      "Epoch 47, train_loss: 0.3244, val_loss: 0.5051\n",
      "Epoch 48, train_loss: 0.3162, val_loss: 0.5069\n",
      "Epoch 49, train_loss: 0.3068, val_loss: 0.5053\n",
      "Epoch 50, train_loss: 0.3002, val_loss: 0.5057\n",
      "Epoch 51, train_loss: 0.2926, val_loss: 0.5030\n",
      "Epoch 52, train_loss: 0.2883, val_loss: 0.5012\n",
      "Epoch 53, train_loss: 0.2858, val_loss: 0.5015\n",
      "Epoch 54, train_loss: 0.2819, val_loss: 0.5003\n",
      "Epoch 55, train_loss: 0.2755, val_loss: 0.5012\n",
      "Epoch 56, train_loss: 0.2650, val_loss: 0.5020\n",
      "Epoch 57, train_loss: 0.2601, val_loss: 0.4986\n",
      "Epoch 58, train_loss: 0.2624, val_loss: 0.4990\n",
      "Epoch 59, train_loss: 0.2518, val_loss: 0.5009\n",
      "Epoch 60, train_loss: 0.2474, val_loss: 0.5009\n",
      "Epoch 61, train_loss: 0.2425, val_loss: 0.4973\n",
      "Epoch 62, train_loss: 0.2382, val_loss: 0.4995\n",
      "Epoch 63, train_loss: 0.2316, val_loss: 0.5027\n",
      "Epoch 64, train_loss: 0.2300, val_loss: 0.5004\n",
      "Epoch 65, train_loss: 0.2302, val_loss: 0.4988\n",
      "Epoch 66, train_loss: 0.2202, val_loss: 0.5002\n",
      "Epoch 67, train_loss: 0.2145, val_loss: 0.5017\n",
      "Epoch 68, train_loss: 0.2128, val_loss: 0.5023\n",
      "Epoch 69, train_loss: 0.2104, val_loss: 0.5024\n",
      "Epoch 70, train_loss: 0.2092, val_loss: 0.5061\n",
      "Early Stopping!\n",
      "Test the model on fold 1:\n",
      "{'roc_auc': 0.8870408870408871, 'pr_auc': 0.9014023001298657, 'fmax': 0.8249950281549628}\n",
      "Starting Fold: 2\n",
      "Epoch 0, train_loss: 0.6934, val_loss: 0.6854\n",
      "Epoch 1, train_loss: 0.6670, val_loss: 0.6732\n",
      "Epoch 2, train_loss: 0.6441, val_loss: 0.6670\n",
      "Epoch 3, train_loss: 0.6301, val_loss: 0.6632\n",
      "Epoch 4, train_loss: 0.6163, val_loss: 0.6587\n",
      "Epoch 5, train_loss: 0.6129, val_loss: 0.6535\n",
      "Epoch 6, train_loss: 0.5953, val_loss: 0.6489\n",
      "Epoch 7, train_loss: 0.5882, val_loss: 0.6447\n",
      "Epoch 8, train_loss: 0.5762, val_loss: 0.6411\n",
      "Epoch 9, train_loss: 0.5755, val_loss: 0.6376\n",
      "Epoch 10, train_loss: 0.5642, val_loss: 0.6348\n",
      "Epoch 11, train_loss: 0.5537, val_loss: 0.6317\n",
      "Epoch 12, train_loss: 0.5459, val_loss: 0.6290\n",
      "Epoch 13, train_loss: 0.5383, val_loss: 0.6262\n",
      "Epoch 14, train_loss: 0.5325, val_loss: 0.6241\n",
      "Epoch 15, train_loss: 0.5233, val_loss: 0.6214\n",
      "Epoch 16, train_loss: 0.5185, val_loss: 0.6184\n",
      "Epoch 17, train_loss: 0.5090, val_loss: 0.6147\n",
      "Epoch 18, train_loss: 0.4987, val_loss: 0.6129\n",
      "Epoch 19, train_loss: 0.4939, val_loss: 0.6105\n",
      "Epoch 20, train_loss: 0.4873, val_loss: 0.6078\n",
      "Epoch 21, train_loss: 0.4786, val_loss: 0.6053\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22, train_loss: 0.4737, val_loss: 0.6040\n",
      "Epoch 23, train_loss: 0.4652, val_loss: 0.6015\n",
      "Epoch 24, train_loss: 0.4595, val_loss: 0.6004\n",
      "Epoch 25, train_loss: 0.4506, val_loss: 0.5984\n",
      "Epoch 26, train_loss: 0.4390, val_loss: 0.5960\n",
      "Epoch 27, train_loss: 0.4386, val_loss: 0.5924\n",
      "Epoch 28, train_loss: 0.4254, val_loss: 0.5893\n",
      "Epoch 29, train_loss: 0.4200, val_loss: 0.5889\n",
      "Epoch 30, train_loss: 0.4132, val_loss: 0.5865\n",
      "Epoch 31, train_loss: 0.4013, val_loss: 0.5844\n",
      "Epoch 32, train_loss: 0.3978, val_loss: 0.5810\n",
      "Epoch 33, train_loss: 0.3879, val_loss: 0.5803\n",
      "Epoch 34, train_loss: 0.3796, val_loss: 0.5792\n",
      "Epoch 35, train_loss: 0.3739, val_loss: 0.5768\n",
      "Epoch 36, train_loss: 0.3621, val_loss: 0.5741\n",
      "Epoch 37, train_loss: 0.3591, val_loss: 0.5718\n",
      "Epoch 38, train_loss: 0.3506, val_loss: 0.5702\n",
      "Epoch 39, train_loss: 0.3434, val_loss: 0.5671\n",
      "Epoch 40, train_loss: 0.3364, val_loss: 0.5656\n",
      "Epoch 41, train_loss: 0.3283, val_loss: 0.5645\n",
      "Epoch 42, train_loss: 0.3220, val_loss: 0.5606\n",
      "Epoch 43, train_loss: 0.3153, val_loss: 0.5602\n",
      "Epoch 44, train_loss: 0.3122, val_loss: 0.5587\n",
      "Epoch 45, train_loss: 0.3040, val_loss: 0.5579\n",
      "Epoch 46, train_loss: 0.2959, val_loss: 0.5560\n",
      "Epoch 47, train_loss: 0.2869, val_loss: 0.5533\n",
      "Epoch 48, train_loss: 0.2828, val_loss: 0.5526\n",
      "Epoch 49, train_loss: 0.2811, val_loss: 0.5499\n",
      "Epoch 50, train_loss: 0.2683, val_loss: 0.5481\n",
      "Epoch 51, train_loss: 0.2624, val_loss: 0.5475\n",
      "Epoch 52, train_loss: 0.2586, val_loss: 0.5451\n",
      "Epoch 53, train_loss: 0.2489, val_loss: 0.5426\n",
      "Epoch 54, train_loss: 0.2526, val_loss: 0.5443\n",
      "Epoch 55, train_loss: 0.2422, val_loss: 0.5407\n",
      "Epoch 56, train_loss: 0.2352, val_loss: 0.5416\n",
      "Epoch 57, train_loss: 0.2290, val_loss: 0.5390\n",
      "Epoch 58, train_loss: 0.2249, val_loss: 0.5378\n",
      "Epoch 59, train_loss: 0.2231, val_loss: 0.5363\n",
      "Epoch 60, train_loss: 0.2177, val_loss: 0.5350\n",
      "Epoch 61, train_loss: 0.2112, val_loss: 0.5334\n",
      "Epoch 62, train_loss: 0.2106, val_loss: 0.5340\n",
      "Epoch 63, train_loss: 0.2072, val_loss: 0.5309\n",
      "Epoch 64, train_loss: 0.2034, val_loss: 0.5311\n",
      "Epoch 65, train_loss: 0.1953, val_loss: 0.5303\n",
      "Epoch 66, train_loss: 0.1961, val_loss: 0.5294\n",
      "Epoch 67, train_loss: 0.1914, val_loss: 0.5303\n",
      "Epoch 68, train_loss: 0.1932, val_loss: 0.5270\n",
      "Epoch 69, train_loss: 0.1891, val_loss: 0.5281\n",
      "Epoch 70, train_loss: 0.1832, val_loss: 0.5279\n",
      "Epoch 71, train_loss: 0.1811, val_loss: 0.5269\n",
      "Epoch 72, train_loss: 0.1776, val_loss: 0.5269\n",
      "Epoch 73, train_loss: 0.1733, val_loss: 0.5241\n",
      "Epoch 74, train_loss: 0.1752, val_loss: 0.5237\n",
      "Epoch 75, train_loss: 0.1702, val_loss: 0.5222\n",
      "Epoch 76, train_loss: 0.1713, val_loss: 0.5233\n",
      "Epoch 77, train_loss: 0.1700, val_loss: 0.5203\n",
      "Epoch 78, train_loss: 0.1633, val_loss: 0.5192\n",
      "Epoch 79, train_loss: 0.1657, val_loss: 0.5205\n",
      "Epoch 80, train_loss: 0.1619, val_loss: 0.5186\n",
      "Epoch 81, train_loss: 0.1579, val_loss: 0.5181\n",
      "Epoch 82, train_loss: 0.1591, val_loss: 0.5178\n",
      "Epoch 83, train_loss: 0.1560, val_loss: 0.5187\n",
      "Epoch 84, train_loss: 0.1584, val_loss: 0.5174\n",
      "Epoch 85, train_loss: 0.1510, val_loss: 0.5189\n",
      "Epoch 86, train_loss: 0.1516, val_loss: 0.5199\n",
      "Epoch 87, train_loss: 0.1509, val_loss: 0.5179\n",
      "Epoch 88, train_loss: 0.1493, val_loss: 0.5145\n",
      "Epoch 89, train_loss: 0.1504, val_loss: 0.5152\n",
      "Epoch 90, train_loss: 0.1495, val_loss: 0.5148\n",
      "Epoch 91, train_loss: 0.1483, val_loss: 0.5152\n",
      "Epoch 92, train_loss: 0.1482, val_loss: 0.5170\n",
      "Epoch 93, train_loss: 0.1413, val_loss: 0.5152\n",
      "Epoch 94, train_loss: 0.1454, val_loss: 0.5143\n",
      "Epoch 95, train_loss: 0.1441, val_loss: 0.5136\n",
      "Epoch 96, train_loss: 0.1394, val_loss: 0.5126\n",
      "Epoch 97, train_loss: 0.1380, val_loss: 0.5144\n",
      "Epoch 98, train_loss: 0.1379, val_loss: 0.5146\n",
      "Epoch 99, train_loss: 0.1356, val_loss: 0.5122\n",
      "Test the model on fold 2:\n",
      "{'roc_auc': 0.8364518364518365, 'pr_auc': 0.8577115283935915, 'fmax': 0.7816043070724541}\n",
      "Starting Fold: 3\n",
      "Epoch 0, train_loss: 0.6854, val_loss: 0.6841\n",
      "Epoch 1, train_loss: 0.6625, val_loss: 0.6715\n",
      "Epoch 2, train_loss: 0.6381, val_loss: 0.6636\n",
      "Epoch 3, train_loss: 0.6256, val_loss: 0.6569\n",
      "Epoch 4, train_loss: 0.6155, val_loss: 0.6509\n",
      "Epoch 5, train_loss: 0.5982, val_loss: 0.6460\n",
      "Epoch 6, train_loss: 0.5905, val_loss: 0.6416\n",
      "Epoch 7, train_loss: 0.5781, val_loss: 0.6372\n",
      "Epoch 8, train_loss: 0.5651, val_loss: 0.6335\n",
      "Epoch 9, train_loss: 0.5613, val_loss: 0.6279\n",
      "Epoch 10, train_loss: 0.5461, val_loss: 0.6222\n",
      "Epoch 11, train_loss: 0.5415, val_loss: 0.6182\n",
      "Epoch 12, train_loss: 0.5323, val_loss: 0.6149\n",
      "Epoch 13, train_loss: 0.5231, val_loss: 0.6096\n",
      "Epoch 14, train_loss: 0.5127, val_loss: 0.6057\n",
      "Epoch 15, train_loss: 0.5031, val_loss: 0.6020\n",
      "Epoch 16, train_loss: 0.4955, val_loss: 0.5983\n",
      "Epoch 17, train_loss: 0.4856, val_loss: 0.5949\n",
      "Epoch 18, train_loss: 0.4787, val_loss: 0.5898\n",
      "Epoch 19, train_loss: 0.4629, val_loss: 0.5864\n",
      "Epoch 20, train_loss: 0.4616, val_loss: 0.5831\n",
      "Epoch 21, train_loss: 0.4512, val_loss: 0.5798\n",
      "Epoch 22, train_loss: 0.4461, val_loss: 0.5765\n",
      "Epoch 23, train_loss: 0.4323, val_loss: 0.5723\n",
      "Epoch 24, train_loss: 0.4263, val_loss: 0.5694\n",
      "Epoch 25, train_loss: 0.4168, val_loss: 0.5677\n",
      "Epoch 26, train_loss: 0.4110, val_loss: 0.5645\n",
      "Epoch 27, train_loss: 0.4054, val_loss: 0.5633\n",
      "Epoch 28, train_loss: 0.3949, val_loss: 0.5607\n",
      "Epoch 29, train_loss: 0.3939, val_loss: 0.5595\n",
      "Epoch 30, train_loss: 0.3758, val_loss: 0.5578\n",
      "Epoch 31, train_loss: 0.3705, val_loss: 0.5537\n",
      "Epoch 32, train_loss: 0.3650, val_loss: 0.5504\n",
      "Epoch 33, train_loss: 0.3534, val_loss: 0.5494\n",
      "Epoch 34, train_loss: 0.3543, val_loss: 0.5468\n",
      "Epoch 35, train_loss: 0.3450, val_loss: 0.5450\n",
      "Epoch 36, train_loss: 0.3322, val_loss: 0.5446\n",
      "Epoch 37, train_loss: 0.3271, val_loss: 0.5416\n",
      "Epoch 38, train_loss: 0.3249, val_loss: 0.5401\n",
      "Epoch 39, train_loss: 0.3143, val_loss: 0.5384\n",
      "Epoch 40, train_loss: 0.3061, val_loss: 0.5368\n",
      "Epoch 41, train_loss: 0.3037, val_loss: 0.5371\n",
      "Epoch 42, train_loss: 0.2948, val_loss: 0.5372\n",
      "Epoch 43, train_loss: 0.2888, val_loss: 0.5357\n",
      "Epoch 44, train_loss: 0.2856, val_loss: 0.5362\n",
      "Epoch 45, train_loss: 0.2784, val_loss: 0.5351\n",
      "Epoch 46, train_loss: 0.2693, val_loss: 0.5345\n",
      "Epoch 47, train_loss: 0.2672, val_loss: 0.5343\n",
      "Epoch 48, train_loss: 0.2584, val_loss: 0.5345\n",
      "Epoch 49, train_loss: 0.2565, val_loss: 0.5333\n",
      "Epoch 50, train_loss: 0.2483, val_loss: 0.5330\n",
      "Epoch 51, train_loss: 0.2467, val_loss: 0.5357\n",
      "Epoch 52, train_loss: 0.2399, val_loss: 0.5352\n",
      "Epoch 53, train_loss: 0.2389, val_loss: 0.5338\n",
      "Epoch 54, train_loss: 0.2341, val_loss: 0.5352\n",
      "Epoch 55, train_loss: 0.2280, val_loss: 0.5341\n",
      "Epoch 56, train_loss: 0.2244, val_loss: 0.5359\n",
      "Epoch 57, train_loss: 0.2173, val_loss: 0.5373\n",
      "Early Stopping!\n",
      "Test the model on fold 3:\n",
      "{'roc_auc': 0.8954480796586061, 'pr_auc': 0.8939771654294966, 'fmax': 0.8395012102107108}\n",
      "Starting Fold: 4\n",
      "Epoch 0, train_loss: 0.6858, val_loss: 0.6789\n",
      "Epoch 1, train_loss: 0.6603, val_loss: 0.6502\n",
      "Epoch 2, train_loss: 0.6388, val_loss: 0.6358\n",
      "Epoch 3, train_loss: 0.6224, val_loss: 0.6267\n",
      "Epoch 4, train_loss: 0.6130, val_loss: 0.6199\n",
      "Epoch 5, train_loss: 0.5972, val_loss: 0.6119\n",
      "Epoch 6, train_loss: 0.5911, val_loss: 0.6031\n",
      "Epoch 7, train_loss: 0.5835, val_loss: 0.5974\n",
      "Epoch 8, train_loss: 0.5706, val_loss: 0.5908\n",
      "Epoch 9, train_loss: 0.5579, val_loss: 0.5872\n",
      "Epoch 10, train_loss: 0.5527, val_loss: 0.5822\n",
      "Epoch 11, train_loss: 0.5473, val_loss: 0.5785\n",
      "Epoch 12, train_loss: 0.5355, val_loss: 0.5739\n",
      "Epoch 13, train_loss: 0.5239, val_loss: 0.5703\n",
      "Epoch 14, train_loss: 0.5152, val_loss: 0.5653\n",
      "Epoch 15, train_loss: 0.5041, val_loss: 0.5614\n",
      "Epoch 16, train_loss: 0.5055, val_loss: 0.5574\n",
      "Epoch 17, train_loss: 0.4944, val_loss: 0.5540\n",
      "Epoch 18, train_loss: 0.4846, val_loss: 0.5518\n",
      "Epoch 19, train_loss: 0.4755, val_loss: 0.5481\n",
      "Epoch 20, train_loss: 0.4650, val_loss: 0.5449\n",
      "Epoch 21, train_loss: 0.4608, val_loss: 0.5413\n",
      "Epoch 22, train_loss: 0.4471, val_loss: 0.5393\n",
      "Epoch 23, train_loss: 0.4461, val_loss: 0.5364\n",
      "Epoch 24, train_loss: 0.4310, val_loss: 0.5351\n",
      "Epoch 25, train_loss: 0.4309, val_loss: 0.5337\n",
      "Epoch 26, train_loss: 0.4193, val_loss: 0.5295\n",
      "Epoch 27, train_loss: 0.4072, val_loss: 0.5273\n",
      "Epoch 28, train_loss: 0.3997, val_loss: 0.5252\n",
      "Epoch 29, train_loss: 0.3939, val_loss: 0.5243\n",
      "Epoch 30, train_loss: 0.3812, val_loss: 0.5212\n",
      "Epoch 31, train_loss: 0.3741, val_loss: 0.5184\n",
      "Epoch 32, train_loss: 0.3678, val_loss: 0.5173\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33, train_loss: 0.3594, val_loss: 0.5158\n",
      "Epoch 34, train_loss: 0.3503, val_loss: 0.5137\n",
      "Epoch 35, train_loss: 0.3426, val_loss: 0.5125\n",
      "Epoch 36, train_loss: 0.3322, val_loss: 0.5103\n",
      "Epoch 37, train_loss: 0.3275, val_loss: 0.5091\n",
      "Epoch 38, train_loss: 0.3134, val_loss: 0.5066\n",
      "Epoch 39, train_loss: 0.3133, val_loss: 0.5051\n",
      "Epoch 40, train_loss: 0.3062, val_loss: 0.5040\n",
      "Epoch 41, train_loss: 0.2977, val_loss: 0.5043\n",
      "Epoch 42, train_loss: 0.2958, val_loss: 0.5017\n",
      "Epoch 43, train_loss: 0.2835, val_loss: 0.5022\n",
      "Epoch 44, train_loss: 0.2812, val_loss: 0.5018\n",
      "Epoch 45, train_loss: 0.2760, val_loss: 0.5005\n",
      "Epoch 46, train_loss: 0.2661, val_loss: 0.4990\n",
      "Epoch 47, train_loss: 0.2638, val_loss: 0.4990\n",
      "Epoch 48, train_loss: 0.2555, val_loss: 0.4978\n",
      "Epoch 49, train_loss: 0.2465, val_loss: 0.4980\n",
      "Epoch 50, train_loss: 0.2466, val_loss: 0.4989\n",
      "Epoch 51, train_loss: 0.2386, val_loss: 0.4995\n",
      "Epoch 52, train_loss: 0.2307, val_loss: 0.4973\n",
      "Epoch 53, train_loss: 0.2335, val_loss: 0.4992\n",
      "Epoch 54, train_loss: 0.2238, val_loss: 0.5006\n",
      "Epoch 55, train_loss: 0.2228, val_loss: 0.5001\n",
      "Epoch 56, train_loss: 0.2147, val_loss: 0.5005\n",
      "Epoch 57, train_loss: 0.2115, val_loss: 0.5006\n",
      "Epoch 58, train_loss: 0.2104, val_loss: 0.5011\n",
      "Epoch 59, train_loss: 0.2055, val_loss: 0.5004\n",
      "Epoch 60, train_loss: 0.2039, val_loss: 0.5007\n",
      "Epoch 61, train_loss: 0.2006, val_loss: 0.5004\n",
      "Epoch 62, train_loss: 0.1943, val_loss: 0.5026\n",
      "Early Stopping!\n",
      "Test the model on fold 4:\n",
      "{'roc_auc': 0.8228120516499282, 'pr_auc': 0.8403616258480816, 'fmax': 0.7499950195643234}\n",
      "Evaluate pretrained model on disease class Immunological (10/12)\n",
      "Starting Fold: 0\n",
      "Epoch 0, train_loss: 0.6844, val_loss: 0.6845\n",
      "Epoch 1, train_loss: 0.6802, val_loss: 0.6762\n",
      "Epoch 2, train_loss: 0.6629, val_loss: 0.6695\n",
      "Epoch 3, train_loss: 0.6517, val_loss: 0.6602\n",
      "Epoch 4, train_loss: 0.6401, val_loss: 0.6524\n",
      "Epoch 5, train_loss: 0.6332, val_loss: 0.6441\n",
      "Epoch 6, train_loss: 0.6217, val_loss: 0.6341\n",
      "Epoch 7, train_loss: 0.6211, val_loss: 0.6287\n",
      "Epoch 8, train_loss: 0.6102, val_loss: 0.6231\n",
      "Epoch 9, train_loss: 0.5949, val_loss: 0.6163\n",
      "Epoch 10, train_loss: 0.5930, val_loss: 0.6069\n",
      "Epoch 11, train_loss: 0.5803, val_loss: 0.5983\n",
      "Epoch 12, train_loss: 0.5622, val_loss: 0.5909\n",
      "Epoch 13, train_loss: 0.5661, val_loss: 0.5837\n",
      "Epoch 14, train_loss: 0.5486, val_loss: 0.5761\n",
      "Epoch 15, train_loss: 0.5415, val_loss: 0.5662\n",
      "Epoch 16, train_loss: 0.5291, val_loss: 0.5580\n",
      "Epoch 17, train_loss: 0.5230, val_loss: 0.5495\n",
      "Epoch 18, train_loss: 0.5009, val_loss: 0.5450\n",
      "Epoch 19, train_loss: 0.5051, val_loss: 0.5364\n",
      "Epoch 20, train_loss: 0.5033, val_loss: 0.5264\n",
      "Epoch 21, train_loss: 0.4816, val_loss: 0.5206\n",
      "Epoch 22, train_loss: 0.4680, val_loss: 0.5140\n",
      "Epoch 23, train_loss: 0.4675, val_loss: 0.5058\n",
      "Epoch 24, train_loss: 0.4631, val_loss: 0.4996\n",
      "Epoch 25, train_loss: 0.4473, val_loss: 0.4939\n",
      "Epoch 26, train_loss: 0.4371, val_loss: 0.4882\n",
      "Epoch 27, train_loss: 0.4452, val_loss: 0.4841\n",
      "Epoch 28, train_loss: 0.4273, val_loss: 0.4787\n",
      "Epoch 29, train_loss: 0.4185, val_loss: 0.4729\n",
      "Epoch 30, train_loss: 0.4155, val_loss: 0.4670\n",
      "Epoch 31, train_loss: 0.4020, val_loss: 0.4625\n",
      "Epoch 32, train_loss: 0.3970, val_loss: 0.4579\n",
      "Epoch 33, train_loss: 0.3837, val_loss: 0.4534\n",
      "Epoch 34, train_loss: 0.3740, val_loss: 0.4485\n",
      "Epoch 35, train_loss: 0.3787, val_loss: 0.4449\n",
      "Epoch 36, train_loss: 0.3694, val_loss: 0.4417\n",
      "Epoch 37, train_loss: 0.3620, val_loss: 0.4378\n",
      "Epoch 38, train_loss: 0.3548, val_loss: 0.4334\n",
      "Epoch 39, train_loss: 0.3480, val_loss: 0.4286\n",
      "Epoch 40, train_loss: 0.3384, val_loss: 0.4245\n",
      "Epoch 41, train_loss: 0.3288, val_loss: 0.4201\n",
      "Epoch 42, train_loss: 0.3361, val_loss: 0.4174\n",
      "Epoch 43, train_loss: 0.3298, val_loss: 0.4149\n",
      "Epoch 44, train_loss: 0.3091, val_loss: 0.4108\n",
      "Epoch 45, train_loss: 0.3026, val_loss: 0.4070\n",
      "Epoch 46, train_loss: 0.3101, val_loss: 0.4032\n",
      "Epoch 47, train_loss: 0.3042, val_loss: 0.4014\n",
      "Epoch 48, train_loss: 0.2995, val_loss: 0.3987\n",
      "Epoch 49, train_loss: 0.2904, val_loss: 0.3967\n",
      "Epoch 50, train_loss: 0.2813, val_loss: 0.3961\n",
      "Epoch 51, train_loss: 0.2784, val_loss: 0.3921\n",
      "Epoch 52, train_loss: 0.2813, val_loss: 0.3904\n",
      "Epoch 53, train_loss: 0.2668, val_loss: 0.3870\n",
      "Epoch 54, train_loss: 0.2578, val_loss: 0.3858\n",
      "Epoch 55, train_loss: 0.2723, val_loss: 0.3852\n",
      "Epoch 56, train_loss: 0.2569, val_loss: 0.3836\n",
      "Epoch 57, train_loss: 0.2448, val_loss: 0.3813\n",
      "Epoch 58, train_loss: 0.2393, val_loss: 0.3786\n",
      "Epoch 59, train_loss: 0.2391, val_loss: 0.3760\n",
      "Epoch 60, train_loss: 0.2483, val_loss: 0.3756\n",
      "Epoch 61, train_loss: 0.2327, val_loss: 0.3720\n",
      "Epoch 62, train_loss: 0.2300, val_loss: 0.3737\n",
      "Epoch 63, train_loss: 0.2300, val_loss: 0.3720\n",
      "Epoch 64, train_loss: 0.2288, val_loss: 0.3691\n",
      "Epoch 65, train_loss: 0.2221, val_loss: 0.3689\n",
      "Epoch 66, train_loss: 0.2155, val_loss: 0.3686\n",
      "Epoch 67, train_loss: 0.2129, val_loss: 0.3686\n",
      "Epoch 68, train_loss: 0.2097, val_loss: 0.3668\n",
      "Epoch 69, train_loss: 0.2012, val_loss: 0.3677\n",
      "Epoch 70, train_loss: 0.1984, val_loss: 0.3665\n",
      "Epoch 71, train_loss: 0.2005, val_loss: 0.3642\n",
      "Epoch 72, train_loss: 0.2022, val_loss: 0.3647\n",
      "Epoch 73, train_loss: 0.2015, val_loss: 0.3657\n",
      "Epoch 74, train_loss: 0.2004, val_loss: 0.3642\n",
      "Epoch 75, train_loss: 0.1927, val_loss: 0.3615\n",
      "Epoch 76, train_loss: 0.1904, val_loss: 0.3595\n",
      "Epoch 77, train_loss: 0.1884, val_loss: 0.3609\n",
      "Epoch 78, train_loss: 0.1808, val_loss: 0.3604\n",
      "Epoch 79, train_loss: 0.1796, val_loss: 0.3594\n",
      "Epoch 80, train_loss: 0.1809, val_loss: 0.3595\n",
      "Epoch 81, train_loss: 0.1801, val_loss: 0.3586\n",
      "Epoch 82, train_loss: 0.1811, val_loss: 0.3580\n",
      "Epoch 83, train_loss: 0.1801, val_loss: 0.3594\n",
      "Epoch 84, train_loss: 0.1779, val_loss: 0.3600\n",
      "Epoch 85, train_loss: 0.1726, val_loss: 0.3584\n",
      "Epoch 86, train_loss: 0.1701, val_loss: 0.3594\n",
      "Epoch 87, train_loss: 0.1625, val_loss: 0.3557\n",
      "Epoch 88, train_loss: 0.1684, val_loss: 0.3582\n",
      "Epoch 89, train_loss: 0.1653, val_loss: 0.3582\n",
      "Epoch 90, train_loss: 0.1662, val_loss: 0.3580\n",
      "Epoch 91, train_loss: 0.1626, val_loss: 0.3585\n",
      "Epoch 92, train_loss: 0.1563, val_loss: 0.3570\n",
      "Epoch 93, train_loss: 0.1587, val_loss: 0.3541\n",
      "Epoch 94, train_loss: 0.1565, val_loss: 0.3554\n",
      "Epoch 95, train_loss: 0.1572, val_loss: 0.3553\n",
      "Epoch 96, train_loss: 0.1564, val_loss: 0.3552\n",
      "Epoch 97, train_loss: 0.1568, val_loss: 0.3555\n",
      "Epoch 98, train_loss: 0.1485, val_loss: 0.3553\n",
      "Epoch 99, train_loss: 0.1451, val_loss: 0.3568\n",
      "Test the model on fold 0:\n",
      "{'roc_auc': 0.9055059523809523, 'pr_auc': 0.9160056249458423, 'fmax': 0.8461488757688417}\n",
      "Starting Fold: 1\n",
      "Epoch 0, train_loss: 0.6891, val_loss: 0.6924\n",
      "Epoch 1, train_loss: 0.6679, val_loss: 0.6827\n",
      "Epoch 2, train_loss: 0.6536, val_loss: 0.6720\n",
      "Epoch 3, train_loss: 0.6388, val_loss: 0.6651\n",
      "Epoch 4, train_loss: 0.6251, val_loss: 0.6647\n",
      "Epoch 5, train_loss: 0.6205, val_loss: 0.6587\n",
      "Epoch 6, train_loss: 0.6096, val_loss: 0.6530\n",
      "Epoch 7, train_loss: 0.5937, val_loss: 0.6511\n",
      "Epoch 8, train_loss: 0.5866, val_loss: 0.6477\n",
      "Epoch 9, train_loss: 0.5769, val_loss: 0.6423\n",
      "Epoch 10, train_loss: 0.5674, val_loss: 0.6383\n",
      "Epoch 11, train_loss: 0.5568, val_loss: 0.6333\n",
      "Epoch 12, train_loss: 0.5531, val_loss: 0.6329\n",
      "Epoch 13, train_loss: 0.5455, val_loss: 0.6320\n",
      "Epoch 14, train_loss: 0.5389, val_loss: 0.6280\n",
      "Epoch 15, train_loss: 0.5284, val_loss: 0.6285\n",
      "Epoch 16, train_loss: 0.5225, val_loss: 0.6231\n",
      "Epoch 17, train_loss: 0.5067, val_loss: 0.6198\n",
      "Epoch 18, train_loss: 0.5042, val_loss: 0.6201\n",
      "Epoch 19, train_loss: 0.4971, val_loss: 0.6176\n",
      "Epoch 20, train_loss: 0.4886, val_loss: 0.6214\n",
      "Epoch 21, train_loss: 0.4754, val_loss: 0.6185\n",
      "Epoch 22, train_loss: 0.4644, val_loss: 0.6164\n",
      "Epoch 23, train_loss: 0.4630, val_loss: 0.6141\n",
      "Epoch 24, train_loss: 0.4518, val_loss: 0.6121\n",
      "Epoch 25, train_loss: 0.4452, val_loss: 0.6116\n",
      "Epoch 26, train_loss: 0.4292, val_loss: 0.6121\n",
      "Epoch 27, train_loss: 0.4244, val_loss: 0.6072\n",
      "Epoch 28, train_loss: 0.4212, val_loss: 0.6110\n",
      "Epoch 29, train_loss: 0.4076, val_loss: 0.6098\n",
      "Epoch 30, train_loss: 0.4033, val_loss: 0.6037\n",
      "Epoch 31, train_loss: 0.3970, val_loss: 0.6030\n",
      "Epoch 32, train_loss: 0.3880, val_loss: 0.5994\n",
      "Epoch 33, train_loss: 0.3778, val_loss: 0.5992\n",
      "Epoch 34, train_loss: 0.3733, val_loss: 0.5996\n",
      "Epoch 35, train_loss: 0.3675, val_loss: 0.6001\n",
      "Epoch 36, train_loss: 0.3636, val_loss: 0.6027\n",
      "Epoch 37, train_loss: 0.3424, val_loss: 0.6012\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38, train_loss: 0.3457, val_loss: 0.5936\n",
      "Epoch 39, train_loss: 0.3347, val_loss: 0.5882\n",
      "Epoch 40, train_loss: 0.3319, val_loss: 0.5921\n",
      "Epoch 41, train_loss: 0.3302, val_loss: 0.5910\n",
      "Epoch 42, train_loss: 0.3286, val_loss: 0.5890\n",
      "Epoch 43, train_loss: 0.3188, val_loss: 0.5913\n",
      "Epoch 44, train_loss: 0.3077, val_loss: 0.5932\n",
      "Epoch 45, train_loss: 0.3072, val_loss: 0.5938\n",
      "Epoch 46, train_loss: 0.2950, val_loss: 0.5893\n",
      "Epoch 47, train_loss: 0.2865, val_loss: 0.5929\n",
      "Epoch 48, train_loss: 0.2894, val_loss: 0.5923\n",
      "Epoch 49, train_loss: 0.2811, val_loss: 0.5941\n",
      "Epoch 50, train_loss: 0.2744, val_loss: 0.5966\n",
      "Epoch 51, train_loss: 0.2681, val_loss: 0.5877\n",
      "Epoch 52, train_loss: 0.2787, val_loss: 0.5936\n",
      "Epoch 53, train_loss: 0.2570, val_loss: 0.5973\n",
      "Early Stopping!\n",
      "Test the model on fold 1:\n",
      "{'roc_auc': 0.8967741935483872, 'pr_auc': 0.9480164841936759, 'fmax': 0.8852409030112945}\n",
      "Starting Fold: 2\n",
      "Epoch 0, train_loss: 0.6929, val_loss: 0.6843\n",
      "Epoch 1, train_loss: 0.6714, val_loss: 0.6780\n",
      "Epoch 2, train_loss: 0.6636, val_loss: 0.6661\n",
      "Epoch 3, train_loss: 0.6446, val_loss: 0.6618\n",
      "Epoch 4, train_loss: 0.6344, val_loss: 0.6601\n",
      "Epoch 5, train_loss: 0.6239, val_loss: 0.6545\n",
      "Epoch 6, train_loss: 0.6170, val_loss: 0.6483\n",
      "Epoch 7, train_loss: 0.5994, val_loss: 0.6404\n",
      "Epoch 8, train_loss: 0.5951, val_loss: 0.6331\n",
      "Epoch 9, train_loss: 0.5898, val_loss: 0.6289\n",
      "Epoch 10, train_loss: 0.5849, val_loss: 0.6230\n",
      "Epoch 11, train_loss: 0.5653, val_loss: 0.6174\n",
      "Epoch 12, train_loss: 0.5569, val_loss: 0.6104\n",
      "Epoch 13, train_loss: 0.5528, val_loss: 0.6057\n",
      "Epoch 14, train_loss: 0.5571, val_loss: 0.6012\n",
      "Epoch 15, train_loss: 0.5324, val_loss: 0.5979\n",
      "Epoch 16, train_loss: 0.5355, val_loss: 0.5923\n",
      "Epoch 17, train_loss: 0.5286, val_loss: 0.5867\n",
      "Epoch 18, train_loss: 0.5121, val_loss: 0.5819\n",
      "Epoch 19, train_loss: 0.5094, val_loss: 0.5767\n",
      "Epoch 20, train_loss: 0.5018, val_loss: 0.5720\n",
      "Epoch 21, train_loss: 0.4936, val_loss: 0.5674\n",
      "Epoch 22, train_loss: 0.4856, val_loss: 0.5634\n",
      "Epoch 23, train_loss: 0.4839, val_loss: 0.5586\n",
      "Epoch 24, train_loss: 0.4694, val_loss: 0.5545\n",
      "Epoch 25, train_loss: 0.4663, val_loss: 0.5515\n",
      "Epoch 26, train_loss: 0.4618, val_loss: 0.5470\n",
      "Epoch 27, train_loss: 0.4587, val_loss: 0.5458\n",
      "Epoch 28, train_loss: 0.4453, val_loss: 0.5429\n",
      "Epoch 29, train_loss: 0.4447, val_loss: 0.5397\n",
      "Epoch 30, train_loss: 0.4298, val_loss: 0.5340\n",
      "Epoch 31, train_loss: 0.4254, val_loss: 0.5311\n",
      "Epoch 32, train_loss: 0.4137, val_loss: 0.5301\n",
      "Epoch 33, train_loss: 0.4095, val_loss: 0.5284\n",
      "Epoch 34, train_loss: 0.4078, val_loss: 0.5267\n",
      "Epoch 35, train_loss: 0.3915, val_loss: 0.5257\n",
      "Epoch 36, train_loss: 0.3967, val_loss: 0.5236\n",
      "Epoch 37, train_loss: 0.3860, val_loss: 0.5220\n",
      "Epoch 38, train_loss: 0.3830, val_loss: 0.5194\n",
      "Epoch 39, train_loss: 0.3782, val_loss: 0.5179\n",
      "Epoch 40, train_loss: 0.3677, val_loss: 0.5160\n",
      "Epoch 41, train_loss: 0.3675, val_loss: 0.5148\n",
      "Epoch 42, train_loss: 0.3535, val_loss: 0.5117\n",
      "Epoch 43, train_loss: 0.3522, val_loss: 0.5109\n",
      "Epoch 44, train_loss: 0.3462, val_loss: 0.5090\n",
      "Epoch 45, train_loss: 0.3368, val_loss: 0.5095\n",
      "Epoch 46, train_loss: 0.3390, val_loss: 0.5066\n",
      "Epoch 47, train_loss: 0.3372, val_loss: 0.5029\n",
      "Epoch 48, train_loss: 0.3311, val_loss: 0.5015\n",
      "Epoch 49, train_loss: 0.3180, val_loss: 0.5012\n",
      "Epoch 50, train_loss: 0.3062, val_loss: 0.5007\n",
      "Epoch 51, train_loss: 0.2964, val_loss: 0.5003\n",
      "Epoch 52, train_loss: 0.3086, val_loss: 0.4990\n",
      "Epoch 53, train_loss: 0.2902, val_loss: 0.5000\n",
      "Epoch 54, train_loss: 0.2914, val_loss: 0.4955\n",
      "Epoch 55, train_loss: 0.2837, val_loss: 0.4950\n",
      "Epoch 56, train_loss: 0.2882, val_loss: 0.4961\n",
      "Epoch 57, train_loss: 0.2787, val_loss: 0.4955\n",
      "Epoch 58, train_loss: 0.2717, val_loss: 0.4970\n",
      "Epoch 59, train_loss: 0.2662, val_loss: 0.4987\n",
      "Epoch 60, train_loss: 0.2672, val_loss: 0.4958\n",
      "Epoch 61, train_loss: 0.2634, val_loss: 0.4952\n",
      "Epoch 62, train_loss: 0.2644, val_loss: 0.4951\n",
      "Epoch 63, train_loss: 0.2560, val_loss: 0.4955\n",
      "Epoch 64, train_loss: 0.2496, val_loss: 0.4926\n",
      "Epoch 65, train_loss: 0.2469, val_loss: 0.4899\n",
      "Epoch 66, train_loss: 0.2419, val_loss: 0.4917\n",
      "Epoch 67, train_loss: 0.2383, val_loss: 0.4917\n",
      "Epoch 68, train_loss: 0.2389, val_loss: 0.4947\n",
      "Epoch 69, train_loss: 0.2340, val_loss: 0.4941\n",
      "Epoch 70, train_loss: 0.2357, val_loss: 0.4925\n",
      "Epoch 71, train_loss: 0.2248, val_loss: 0.4909\n",
      "Epoch 72, train_loss: 0.2258, val_loss: 0.4899\n",
      "Epoch 73, train_loss: 0.2214, val_loss: 0.4871\n",
      "Epoch 74, train_loss: 0.2159, val_loss: 0.4868\n",
      "Epoch 75, train_loss: 0.2206, val_loss: 0.4913\n",
      "Epoch 76, train_loss: 0.2155, val_loss: 0.4895\n",
      "Epoch 77, train_loss: 0.2117, val_loss: 0.4909\n",
      "Epoch 78, train_loss: 0.2140, val_loss: 0.4920\n",
      "Epoch 79, train_loss: 0.2142, val_loss: 0.4915\n",
      "Epoch 80, train_loss: 0.2046, val_loss: 0.4926\n",
      "Epoch 81, train_loss: 0.2175, val_loss: 0.4937\n",
      "Epoch 82, train_loss: 0.1987, val_loss: 0.4955\n",
      "Early Stopping!\n",
      "Test the model on fold 2:\n",
      "{'roc_auc': 0.9583333333333334, 'pr_auc': 0.9692676969573253, 'fmax': 0.9565167486082682}\n",
      "Starting Fold: 3\n",
      "Epoch 0, train_loss: 0.6932, val_loss: 0.6902\n",
      "Epoch 1, train_loss: 0.6719, val_loss: 0.6762\n",
      "Epoch 2, train_loss: 0.6572, val_loss: 0.6647\n",
      "Epoch 3, train_loss: 0.6415, val_loss: 0.6573\n",
      "Epoch 4, train_loss: 0.6299, val_loss: 0.6507\n",
      "Epoch 5, train_loss: 0.6168, val_loss: 0.6425\n",
      "Epoch 6, train_loss: 0.6116, val_loss: 0.6360\n",
      "Epoch 7, train_loss: 0.6031, val_loss: 0.6303\n",
      "Epoch 8, train_loss: 0.5932, val_loss: 0.6281\n",
      "Epoch 9, train_loss: 0.5842, val_loss: 0.6203\n",
      "Epoch 10, train_loss: 0.5755, val_loss: 0.6133\n",
      "Epoch 11, train_loss: 0.5649, val_loss: 0.6050\n",
      "Epoch 12, train_loss: 0.5545, val_loss: 0.6006\n",
      "Epoch 13, train_loss: 0.5431, val_loss: 0.5964\n",
      "Epoch 14, train_loss: 0.5322, val_loss: 0.5907\n",
      "Epoch 15, train_loss: 0.5274, val_loss: 0.5879\n",
      "Epoch 16, train_loss: 0.5218, val_loss: 0.5846\n",
      "Epoch 17, train_loss: 0.5059, val_loss: 0.5803\n",
      "Epoch 18, train_loss: 0.5022, val_loss: 0.5742\n",
      "Epoch 19, train_loss: 0.4956, val_loss: 0.5683\n",
      "Epoch 20, train_loss: 0.4846, val_loss: 0.5607\n",
      "Epoch 21, train_loss: 0.4765, val_loss: 0.5591\n",
      "Epoch 22, train_loss: 0.4678, val_loss: 0.5571\n",
      "Epoch 23, train_loss: 0.4639, val_loss: 0.5509\n",
      "Epoch 24, train_loss: 0.4591, val_loss: 0.5488\n",
      "Epoch 25, train_loss: 0.4432, val_loss: 0.5428\n",
      "Epoch 26, train_loss: 0.4408, val_loss: 0.5394\n",
      "Epoch 27, train_loss: 0.4262, val_loss: 0.5362\n",
      "Epoch 28, train_loss: 0.4283, val_loss: 0.5334\n",
      "Epoch 29, train_loss: 0.4232, val_loss: 0.5295\n",
      "Epoch 30, train_loss: 0.4103, val_loss: 0.5241\n",
      "Epoch 31, train_loss: 0.4005, val_loss: 0.5250\n",
      "Epoch 32, train_loss: 0.3981, val_loss: 0.5234\n",
      "Epoch 33, train_loss: 0.3916, val_loss: 0.5193\n",
      "Epoch 34, train_loss: 0.3785, val_loss: 0.5146\n",
      "Epoch 35, train_loss: 0.3717, val_loss: 0.5152\n",
      "Epoch 36, train_loss: 0.3708, val_loss: 0.5127\n",
      "Epoch 37, train_loss: 0.3599, val_loss: 0.5106\n",
      "Epoch 38, train_loss: 0.3572, val_loss: 0.5063\n",
      "Epoch 39, train_loss: 0.3462, val_loss: 0.5041\n",
      "Epoch 40, train_loss: 0.3424, val_loss: 0.5005\n",
      "Epoch 41, train_loss: 0.3300, val_loss: 0.5013\n",
      "Epoch 42, train_loss: 0.3341, val_loss: 0.5024\n",
      "Epoch 43, train_loss: 0.3212, val_loss: 0.4967\n",
      "Epoch 44, train_loss: 0.3211, val_loss: 0.4932\n",
      "Epoch 45, train_loss: 0.3129, val_loss: 0.4930\n",
      "Epoch 46, train_loss: 0.3135, val_loss: 0.4879\n",
      "Epoch 47, train_loss: 0.3015, val_loss: 0.4870\n",
      "Epoch 48, train_loss: 0.2990, val_loss: 0.4856\n",
      "Epoch 49, train_loss: 0.2940, val_loss: 0.4810\n",
      "Epoch 50, train_loss: 0.2852, val_loss: 0.4783\n",
      "Epoch 51, train_loss: 0.2895, val_loss: 0.4767\n",
      "Epoch 52, train_loss: 0.2767, val_loss: 0.4780\n",
      "Epoch 53, train_loss: 0.2681, val_loss: 0.4744\n",
      "Epoch 54, train_loss: 0.2661, val_loss: 0.4698\n",
      "Epoch 55, train_loss: 0.2602, val_loss: 0.4682\n",
      "Epoch 56, train_loss: 0.2581, val_loss: 0.4697\n",
      "Epoch 57, train_loss: 0.2455, val_loss: 0.4691\n",
      "Epoch 58, train_loss: 0.2522, val_loss: 0.4675\n",
      "Epoch 59, train_loss: 0.2470, val_loss: 0.4638\n",
      "Epoch 60, train_loss: 0.2385, val_loss: 0.4630\n",
      "Epoch 61, train_loss: 0.2357, val_loss: 0.4626\n",
      "Epoch 62, train_loss: 0.2365, val_loss: 0.4603\n",
      "Epoch 63, train_loss: 0.2314, val_loss: 0.4595\n",
      "Epoch 64, train_loss: 0.2329, val_loss: 0.4590\n",
      "Epoch 65, train_loss: 0.2268, val_loss: 0.4601\n",
      "Epoch 66, train_loss: 0.2213, val_loss: 0.4586\n",
      "Epoch 67, train_loss: 0.2166, val_loss: 0.4593\n",
      "Epoch 68, train_loss: 0.2165, val_loss: 0.4582\n",
      "Epoch 69, train_loss: 0.2126, val_loss: 0.4564\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 70, train_loss: 0.2113, val_loss: 0.4536\n",
      "Epoch 71, train_loss: 0.2013, val_loss: 0.4537\n",
      "Epoch 72, train_loss: 0.1981, val_loss: 0.4512\n",
      "Epoch 73, train_loss: 0.2129, val_loss: 0.4520\n",
      "Epoch 74, train_loss: 0.1979, val_loss: 0.4506\n",
      "Epoch 75, train_loss: 0.1965, val_loss: 0.4478\n",
      "Epoch 76, train_loss: 0.1977, val_loss: 0.4487\n",
      "Epoch 77, train_loss: 0.1880, val_loss: 0.4442\n",
      "Epoch 78, train_loss: 0.1935, val_loss: 0.4465\n",
      "Epoch 79, train_loss: 0.1908, val_loss: 0.4437\n",
      "Epoch 80, train_loss: 0.1836, val_loss: 0.4457\n",
      "Epoch 81, train_loss: 0.1878, val_loss: 0.4499\n",
      "Epoch 82, train_loss: 0.1801, val_loss: 0.4432\n",
      "Epoch 83, train_loss: 0.1760, val_loss: 0.4409\n",
      "Epoch 84, train_loss: 0.1830, val_loss: 0.4415\n",
      "Epoch 85, train_loss: 0.1758, val_loss: 0.4446\n",
      "Epoch 86, train_loss: 0.1733, val_loss: 0.4446\n",
      "Epoch 87, train_loss: 0.1706, val_loss: 0.4428\n",
      "Epoch 88, train_loss: 0.1736, val_loss: 0.4411\n",
      "Epoch 89, train_loss: 0.1692, val_loss: 0.4414\n",
      "Epoch 90, train_loss: 0.1704, val_loss: 0.4384\n",
      "Epoch 91, train_loss: 0.1724, val_loss: 0.4386\n",
      "Epoch 92, train_loss: 0.1671, val_loss: 0.4429\n",
      "Epoch 93, train_loss: 0.1640, val_loss: 0.4409\n",
      "Epoch 94, train_loss: 0.1584, val_loss: 0.4387\n",
      "Epoch 95, train_loss: 0.1642, val_loss: 0.4389\n",
      "Epoch 96, train_loss: 0.1612, val_loss: 0.4353\n",
      "Epoch 97, train_loss: 0.1543, val_loss: 0.4401\n",
      "Epoch 98, train_loss: 0.1584, val_loss: 0.4408\n",
      "Epoch 99, train_loss: 0.1603, val_loss: 0.4380\n",
      "Test the model on fold 3:\n",
      "{'roc_auc': 0.8753846153846154, 'pr_auc': 0.9078303459455087, 'fmax': 0.8301836952951397}\n",
      "Starting Fold: 4\n",
      "Epoch 0, train_loss: 0.6923, val_loss: 0.6775\n",
      "Epoch 1, train_loss: 0.6686, val_loss: 0.6630\n",
      "Epoch 2, train_loss: 0.6541, val_loss: 0.6544\n",
      "Epoch 3, train_loss: 0.6428, val_loss: 0.6384\n",
      "Epoch 4, train_loss: 0.6296, val_loss: 0.6295\n",
      "Epoch 5, train_loss: 0.6133, val_loss: 0.6180\n",
      "Epoch 6, train_loss: 0.6015, val_loss: 0.6115\n",
      "Epoch 7, train_loss: 0.5913, val_loss: 0.6002\n",
      "Epoch 8, train_loss: 0.5783, val_loss: 0.5937\n",
      "Epoch 9, train_loss: 0.5793, val_loss: 0.5845\n",
      "Epoch 10, train_loss: 0.5591, val_loss: 0.5759\n",
      "Epoch 11, train_loss: 0.5575, val_loss: 0.5675\n",
      "Epoch 12, train_loss: 0.5485, val_loss: 0.5587\n",
      "Epoch 13, train_loss: 0.5343, val_loss: 0.5509\n",
      "Epoch 14, train_loss: 0.5314, val_loss: 0.5452\n",
      "Epoch 15, train_loss: 0.5194, val_loss: 0.5410\n",
      "Epoch 16, train_loss: 0.5092, val_loss: 0.5343\n",
      "Epoch 17, train_loss: 0.5044, val_loss: 0.5270\n",
      "Epoch 18, train_loss: 0.5018, val_loss: 0.5226\n",
      "Epoch 19, train_loss: 0.4795, val_loss: 0.5194\n",
      "Epoch 20, train_loss: 0.4817, val_loss: 0.5139\n",
      "Epoch 21, train_loss: 0.4646, val_loss: 0.5102\n",
      "Epoch 22, train_loss: 0.4655, val_loss: 0.5028\n",
      "Epoch 23, train_loss: 0.4551, val_loss: 0.5004\n",
      "Epoch 24, train_loss: 0.4465, val_loss: 0.4958\n",
      "Epoch 25, train_loss: 0.4361, val_loss: 0.4952\n",
      "Epoch 26, train_loss: 0.4328, val_loss: 0.4899\n",
      "Epoch 27, train_loss: 0.4277, val_loss: 0.4894\n",
      "Epoch 28, train_loss: 0.4218, val_loss: 0.4881\n",
      "Epoch 29, train_loss: 0.4050, val_loss: 0.4847\n",
      "Epoch 30, train_loss: 0.4091, val_loss: 0.4821\n",
      "Epoch 31, train_loss: 0.4032, val_loss: 0.4774\n",
      "Epoch 32, train_loss: 0.3953, val_loss: 0.4765\n",
      "Epoch 33, train_loss: 0.3825, val_loss: 0.4761\n",
      "Epoch 34, train_loss: 0.3719, val_loss: 0.4725\n",
      "Epoch 35, train_loss: 0.3705, val_loss: 0.4727\n",
      "Epoch 36, train_loss: 0.3661, val_loss: 0.4719\n",
      "Epoch 37, train_loss: 0.3470, val_loss: 0.4682\n",
      "Epoch 38, train_loss: 0.3440, val_loss: 0.4641\n",
      "Epoch 39, train_loss: 0.3379, val_loss: 0.4627\n",
      "Epoch 40, train_loss: 0.3353, val_loss: 0.4614\n",
      "Epoch 41, train_loss: 0.3263, val_loss: 0.4597\n",
      "Epoch 42, train_loss: 0.3259, val_loss: 0.4578\n",
      "Epoch 43, train_loss: 0.3139, val_loss: 0.4589\n",
      "Epoch 44, train_loss: 0.3168, val_loss: 0.4603\n",
      "Epoch 45, train_loss: 0.3039, val_loss: 0.4561\n",
      "Epoch 46, train_loss: 0.3023, val_loss: 0.4544\n",
      "Epoch 47, train_loss: 0.2975, val_loss: 0.4525\n",
      "Epoch 48, train_loss: 0.2926, val_loss: 0.4531\n",
      "Epoch 49, train_loss: 0.2811, val_loss: 0.4529\n",
      "Epoch 50, train_loss: 0.2752, val_loss: 0.4571\n",
      "Epoch 51, train_loss: 0.2740, val_loss: 0.4557\n",
      "Epoch 52, train_loss: 0.2659, val_loss: 0.4550\n",
      "Epoch 53, train_loss: 0.2662, val_loss: 0.4559\n",
      "Epoch 54, train_loss: 0.2555, val_loss: 0.4537\n",
      "Epoch 55, train_loss: 0.2534, val_loss: 0.4541\n",
      "Epoch 56, train_loss: 0.2597, val_loss: 0.4534\n",
      "Epoch 57, train_loss: 0.2469, val_loss: 0.4555\n",
      "Epoch 58, train_loss: 0.2429, val_loss: 0.4556\n",
      "Epoch 59, train_loss: 0.2335, val_loss: 0.4569\n",
      "Epoch 60, train_loss: 0.2300, val_loss: 0.4580\n",
      "Early Stopping!\n",
      "Test the model on fold 4:\n",
      "{'roc_auc': 0.8101851851851852, 'pr_auc': 0.8199141421548164, 'fmax': 0.7999950080311499}\n",
      "Evaluate pretrained model on disease class Muscular (11/12)\n",
      "Starting Fold: 0\n",
      "Epoch 0, train_loss: 0.6917, val_loss: 0.6757\n",
      "Epoch 1, train_loss: 0.6895, val_loss: 0.6711\n",
      "Epoch 2, train_loss: 0.6792, val_loss: 0.6657\n",
      "Epoch 3, train_loss: 0.6740, val_loss: 0.6611\n",
      "Epoch 4, train_loss: 0.6579, val_loss: 0.6565\n",
      "Epoch 5, train_loss: 0.6534, val_loss: 0.6526\n",
      "Epoch 6, train_loss: 0.6419, val_loss: 0.6476\n",
      "Epoch 7, train_loss: 0.6413, val_loss: 0.6442\n",
      "Epoch 8, train_loss: 0.6358, val_loss: 0.6403\n",
      "Epoch 9, train_loss: 0.6235, val_loss: 0.6363\n",
      "Epoch 10, train_loss: 0.6067, val_loss: 0.6324\n",
      "Epoch 11, train_loss: 0.6014, val_loss: 0.6283\n",
      "Epoch 12, train_loss: 0.6021, val_loss: 0.6237\n",
      "Epoch 13, train_loss: 0.5773, val_loss: 0.6204\n",
      "Epoch 14, train_loss: 0.5805, val_loss: 0.6156\n",
      "Epoch 15, train_loss: 0.5774, val_loss: 0.6117\n",
      "Epoch 16, train_loss: 0.5682, val_loss: 0.6081\n",
      "Epoch 17, train_loss: 0.5674, val_loss: 0.6040\n",
      "Epoch 18, train_loss: 0.5578, val_loss: 0.6001\n",
      "Epoch 19, train_loss: 0.5557, val_loss: 0.5970\n",
      "Epoch 20, train_loss: 0.5428, val_loss: 0.5941\n",
      "Epoch 21, train_loss: 0.5368, val_loss: 0.5903\n",
      "Epoch 22, train_loss: 0.5272, val_loss: 0.5875\n",
      "Epoch 23, train_loss: 0.5281, val_loss: 0.5843\n",
      "Epoch 24, train_loss: 0.5199, val_loss: 0.5802\n",
      "Epoch 25, train_loss: 0.5107, val_loss: 0.5762\n",
      "Epoch 26, train_loss: 0.5127, val_loss: 0.5723\n",
      "Epoch 27, train_loss: 0.4924, val_loss: 0.5694\n",
      "Epoch 28, train_loss: 0.4906, val_loss: 0.5670\n",
      "Epoch 29, train_loss: 0.4852, val_loss: 0.5634\n",
      "Epoch 30, train_loss: 0.4739, val_loss: 0.5609\n",
      "Epoch 31, train_loss: 0.4697, val_loss: 0.5581\n",
      "Epoch 32, train_loss: 0.4697, val_loss: 0.5552\n",
      "Epoch 33, train_loss: 0.4651, val_loss: 0.5516\n",
      "Epoch 34, train_loss: 0.4503, val_loss: 0.5486\n",
      "Epoch 35, train_loss: 0.4561, val_loss: 0.5456\n",
      "Epoch 36, train_loss: 0.4421, val_loss: 0.5432\n",
      "Epoch 37, train_loss: 0.4350, val_loss: 0.5404\n",
      "Epoch 38, train_loss: 0.4197, val_loss: 0.5379\n",
      "Epoch 39, train_loss: 0.4212, val_loss: 0.5349\n",
      "Epoch 40, train_loss: 0.4201, val_loss: 0.5325\n",
      "Epoch 41, train_loss: 0.4086, val_loss: 0.5302\n",
      "Epoch 42, train_loss: 0.4032, val_loss: 0.5270\n",
      "Epoch 43, train_loss: 0.4000, val_loss: 0.5246\n",
      "Epoch 44, train_loss: 0.3895, val_loss: 0.5220\n",
      "Epoch 45, train_loss: 0.3802, val_loss: 0.5180\n",
      "Epoch 46, train_loss: 0.3771, val_loss: 0.5157\n",
      "Epoch 47, train_loss: 0.3654, val_loss: 0.5115\n",
      "Epoch 48, train_loss: 0.3662, val_loss: 0.5069\n",
      "Epoch 49, train_loss: 0.3637, val_loss: 0.5036\n",
      "Epoch 50, train_loss: 0.3522, val_loss: 0.5009\n",
      "Epoch 51, train_loss: 0.3468, val_loss: 0.4982\n",
      "Epoch 52, train_loss: 0.3359, val_loss: 0.4963\n",
      "Epoch 53, train_loss: 0.3284, val_loss: 0.4938\n",
      "Epoch 54, train_loss: 0.3319, val_loss: 0.4910\n",
      "Epoch 55, train_loss: 0.3253, val_loss: 0.4877\n",
      "Epoch 56, train_loss: 0.3139, val_loss: 0.4844\n",
      "Epoch 57, train_loss: 0.3255, val_loss: 0.4816\n",
      "Epoch 58, train_loss: 0.3163, val_loss: 0.4811\n",
      "Epoch 59, train_loss: 0.2966, val_loss: 0.4783\n",
      "Epoch 60, train_loss: 0.2977, val_loss: 0.4763\n",
      "Epoch 61, train_loss: 0.3012, val_loss: 0.4746\n",
      "Epoch 62, train_loss: 0.2805, val_loss: 0.4720\n",
      "Epoch 63, train_loss: 0.2931, val_loss: 0.4698\n",
      "Epoch 64, train_loss: 0.2829, val_loss: 0.4682\n",
      "Epoch 65, train_loss: 0.2733, val_loss: 0.4662\n",
      "Epoch 66, train_loss: 0.2605, val_loss: 0.4651\n",
      "Epoch 67, train_loss: 0.2696, val_loss: 0.4631\n",
      "Epoch 68, train_loss: 0.2602, val_loss: 0.4619\n",
      "Epoch 69, train_loss: 0.2680, val_loss: 0.4601\n",
      "Epoch 70, train_loss: 0.2532, val_loss: 0.4571\n",
      "Epoch 71, train_loss: 0.2360, val_loss: 0.4548\n",
      "Epoch 72, train_loss: 0.2495, val_loss: 0.4532\n",
      "Epoch 73, train_loss: 0.2464, val_loss: 0.4521\n",
      "Epoch 74, train_loss: 0.2420, val_loss: 0.4502\n",
      "Epoch 75, train_loss: 0.2344, val_loss: 0.4484\n",
      "Epoch 76, train_loss: 0.2377, val_loss: 0.4478\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 77, train_loss: 0.2345, val_loss: 0.4463\n",
      "Epoch 78, train_loss: 0.2317, val_loss: 0.4447\n",
      "Epoch 79, train_loss: 0.2193, val_loss: 0.4438\n",
      "Epoch 80, train_loss: 0.2173, val_loss: 0.4417\n",
      "Epoch 81, train_loss: 0.2147, val_loss: 0.4405\n",
      "Epoch 82, train_loss: 0.2120, val_loss: 0.4395\n",
      "Epoch 83, train_loss: 0.2131, val_loss: 0.4373\n",
      "Epoch 84, train_loss: 0.2131, val_loss: 0.4353\n",
      "Epoch 85, train_loss: 0.2017, val_loss: 0.4350\n",
      "Epoch 86, train_loss: 0.1980, val_loss: 0.4343\n",
      "Epoch 87, train_loss: 0.1959, val_loss: 0.4335\n",
      "Epoch 88, train_loss: 0.1992, val_loss: 0.4314\n",
      "Epoch 89, train_loss: 0.1963, val_loss: 0.4301\n",
      "Epoch 90, train_loss: 0.1961, val_loss: 0.4295\n",
      "Epoch 91, train_loss: 0.1933, val_loss: 0.4288\n",
      "Epoch 92, train_loss: 0.1885, val_loss: 0.4281\n",
      "Epoch 93, train_loss: 0.1947, val_loss: 0.4261\n",
      "Epoch 94, train_loss: 0.1924, val_loss: 0.4246\n",
      "Epoch 95, train_loss: 0.1885, val_loss: 0.4244\n",
      "Epoch 96, train_loss: 0.1947, val_loss: 0.4238\n",
      "Epoch 97, train_loss: 0.1764, val_loss: 0.4228\n",
      "Epoch 98, train_loss: 0.1773, val_loss: 0.4224\n",
      "Epoch 99, train_loss: 0.1809, val_loss: 0.4210\n",
      "Test the model on fold 0:\n",
      "{'roc_auc': 0.7821428571428571, 'pr_auc': 0.8863475993250093, 'fmax': 0.7999951020708036}\n",
      "Starting Fold: 1\n",
      "Epoch 0, train_loss: 0.6968, val_loss: 0.6667\n",
      "Epoch 1, train_loss: 0.6751, val_loss: 0.6528\n",
      "Epoch 2, train_loss: 0.6730, val_loss: 0.6357\n",
      "Epoch 3, train_loss: 0.6520, val_loss: 0.6217\n",
      "Epoch 4, train_loss: 0.6386, val_loss: 0.6081\n",
      "Epoch 5, train_loss: 0.6305, val_loss: 0.5949\n",
      "Epoch 6, train_loss: 0.6267, val_loss: 0.5841\n",
      "Epoch 7, train_loss: 0.6094, val_loss: 0.5743\n",
      "Epoch 8, train_loss: 0.6022, val_loss: 0.5665\n",
      "Epoch 9, train_loss: 0.6055, val_loss: 0.5541\n",
      "Epoch 10, train_loss: 0.5962, val_loss: 0.5463\n",
      "Epoch 11, train_loss: 0.5910, val_loss: 0.5404\n",
      "Epoch 12, train_loss: 0.5856, val_loss: 0.5326\n",
      "Epoch 13, train_loss: 0.5826, val_loss: 0.5265\n",
      "Epoch 14, train_loss: 0.5630, val_loss: 0.5190\n",
      "Epoch 15, train_loss: 0.5659, val_loss: 0.5121\n",
      "Epoch 16, train_loss: 0.5598, val_loss: 0.5065\n",
      "Epoch 17, train_loss: 0.5489, val_loss: 0.5026\n",
      "Epoch 18, train_loss: 0.5494, val_loss: 0.4965\n",
      "Epoch 19, train_loss: 0.5444, val_loss: 0.4907\n",
      "Epoch 20, train_loss: 0.5353, val_loss: 0.4873\n",
      "Epoch 21, train_loss: 0.5276, val_loss: 0.4813\n",
      "Epoch 22, train_loss: 0.5246, val_loss: 0.4785\n",
      "Epoch 23, train_loss: 0.5306, val_loss: 0.4761\n",
      "Epoch 24, train_loss: 0.5160, val_loss: 0.4741\n",
      "Epoch 25, train_loss: 0.4947, val_loss: 0.4706\n",
      "Epoch 26, train_loss: 0.5204, val_loss: 0.4658\n",
      "Epoch 27, train_loss: 0.5063, val_loss: 0.4625\n",
      "Epoch 28, train_loss: 0.5001, val_loss: 0.4565\n",
      "Epoch 29, train_loss: 0.4805, val_loss: 0.4533\n",
      "Epoch 30, train_loss: 0.4796, val_loss: 0.4508\n",
      "Epoch 31, train_loss: 0.4820, val_loss: 0.4481\n",
      "Epoch 32, train_loss: 0.4686, val_loss: 0.4449\n",
      "Epoch 33, train_loss: 0.4680, val_loss: 0.4410\n",
      "Epoch 34, train_loss: 0.4706, val_loss: 0.4372\n",
      "Epoch 35, train_loss: 0.4573, val_loss: 0.4335\n",
      "Epoch 36, train_loss: 0.4551, val_loss: 0.4293\n",
      "Epoch 37, train_loss: 0.4597, val_loss: 0.4276\n",
      "Epoch 38, train_loss: 0.4528, val_loss: 0.4245\n",
      "Epoch 39, train_loss: 0.4478, val_loss: 0.4244\n",
      "Epoch 40, train_loss: 0.4429, val_loss: 0.4230\n",
      "Epoch 41, train_loss: 0.4239, val_loss: 0.4206\n",
      "Epoch 42, train_loss: 0.4301, val_loss: 0.4161\n",
      "Epoch 43, train_loss: 0.4190, val_loss: 0.4116\n",
      "Epoch 44, train_loss: 0.4096, val_loss: 0.4089\n",
      "Epoch 45, train_loss: 0.4081, val_loss: 0.4046\n",
      "Epoch 46, train_loss: 0.3991, val_loss: 0.4020\n",
      "Epoch 47, train_loss: 0.3915, val_loss: 0.4004\n",
      "Epoch 48, train_loss: 0.3891, val_loss: 0.3974\n",
      "Epoch 49, train_loss: 0.3943, val_loss: 0.3952\n",
      "Epoch 50, train_loss: 0.3765, val_loss: 0.3924\n",
      "Epoch 51, train_loss: 0.3822, val_loss: 0.3910\n",
      "Epoch 52, train_loss: 0.3679, val_loss: 0.3884\n",
      "Epoch 53, train_loss: 0.3709, val_loss: 0.3845\n",
      "Epoch 54, train_loss: 0.3647, val_loss: 0.3817\n",
      "Epoch 55, train_loss: 0.3531, val_loss: 0.3790\n",
      "Epoch 56, train_loss: 0.3433, val_loss: 0.3781\n",
      "Epoch 57, train_loss: 0.3494, val_loss: 0.3777\n",
      "Epoch 58, train_loss: 0.3345, val_loss: 0.3755\n",
      "Epoch 59, train_loss: 0.3285, val_loss: 0.3724\n",
      "Epoch 60, train_loss: 0.3310, val_loss: 0.3712\n",
      "Epoch 61, train_loss: 0.3225, val_loss: 0.3693\n",
      "Epoch 62, train_loss: 0.3213, val_loss: 0.3683\n",
      "Epoch 63, train_loss: 0.3157, val_loss: 0.3660\n",
      "Epoch 64, train_loss: 0.3034, val_loss: 0.3632\n",
      "Epoch 65, train_loss: 0.3171, val_loss: 0.3580\n",
      "Epoch 66, train_loss: 0.3009, val_loss: 0.3576\n",
      "Epoch 67, train_loss: 0.2993, val_loss: 0.3575\n",
      "Epoch 68, train_loss: 0.2888, val_loss: 0.3586\n",
      "Epoch 69, train_loss: 0.2828, val_loss: 0.3552\n",
      "Epoch 70, train_loss: 0.2826, val_loss: 0.3539\n",
      "Epoch 71, train_loss: 0.2794, val_loss: 0.3521\n",
      "Epoch 72, train_loss: 0.2734, val_loss: 0.3519\n",
      "Epoch 73, train_loss: 0.2648, val_loss: 0.3495\n",
      "Epoch 74, train_loss: 0.2633, val_loss: 0.3483\n",
      "Epoch 75, train_loss: 0.2653, val_loss: 0.3478\n",
      "Epoch 76, train_loss: 0.2567, val_loss: 0.3469\n",
      "Epoch 77, train_loss: 0.2521, val_loss: 0.3436\n",
      "Epoch 78, train_loss: 0.2524, val_loss: 0.3420\n",
      "Epoch 79, train_loss: 0.2513, val_loss: 0.3428\n",
      "Epoch 80, train_loss: 0.2456, val_loss: 0.3415\n",
      "Epoch 81, train_loss: 0.2379, val_loss: 0.3396\n",
      "Epoch 82, train_loss: 0.2382, val_loss: 0.3390\n",
      "Epoch 83, train_loss: 0.2310, val_loss: 0.3391\n",
      "Epoch 84, train_loss: 0.2236, val_loss: 0.3384\n",
      "Epoch 85, train_loss: 0.2243, val_loss: 0.3367\n",
      "Epoch 86, train_loss: 0.2227, val_loss: 0.3345\n",
      "Epoch 87, train_loss: 0.2163, val_loss: 0.3320\n",
      "Epoch 88, train_loss: 0.2143, val_loss: 0.3321\n",
      "Epoch 89, train_loss: 0.2090, val_loss: 0.3309\n",
      "Epoch 90, train_loss: 0.2038, val_loss: 0.3287\n",
      "Epoch 91, train_loss: 0.2124, val_loss: 0.3312\n",
      "Epoch 92, train_loss: 0.2133, val_loss: 0.3296\n",
      "Epoch 93, train_loss: 0.2014, val_loss: 0.3278\n",
      "Epoch 94, train_loss: 0.2036, val_loss: 0.3279\n",
      "Epoch 95, train_loss: 0.2003, val_loss: 0.3271\n",
      "Epoch 96, train_loss: 0.1981, val_loss: 0.3262\n",
      "Epoch 97, train_loss: 0.1912, val_loss: 0.3253\n",
      "Epoch 98, train_loss: 0.1945, val_loss: 0.3249\n",
      "Epoch 99, train_loss: 0.1803, val_loss: 0.3242\n",
      "Test the model on fold 1:\n",
      "{'roc_auc': 0.8107638888888888, 'pr_auc': 0.7682533768587202, 'fmax': 0.7777728395375266}\n",
      "Starting Fold: 2\n",
      "Epoch 0, train_loss: 0.6962, val_loss: 0.6600\n",
      "Epoch 1, train_loss: 0.6836, val_loss: 0.6396\n",
      "Epoch 2, train_loss: 0.6670, val_loss: 0.6207\n",
      "Epoch 3, train_loss: 0.6601, val_loss: 0.6055\n",
      "Epoch 4, train_loss: 0.6440, val_loss: 0.5925\n",
      "Epoch 5, train_loss: 0.6354, val_loss: 0.5809\n",
      "Epoch 6, train_loss: 0.6325, val_loss: 0.5704\n",
      "Epoch 7, train_loss: 0.6185, val_loss: 0.5638\n",
      "Epoch 8, train_loss: 0.6242, val_loss: 0.5567\n",
      "Epoch 9, train_loss: 0.6108, val_loss: 0.5493\n",
      "Epoch 10, train_loss: 0.5965, val_loss: 0.5443\n",
      "Epoch 11, train_loss: 0.5813, val_loss: 0.5380\n",
      "Epoch 12, train_loss: 0.5958, val_loss: 0.5285\n",
      "Epoch 13, train_loss: 0.5830, val_loss: 0.5209\n",
      "Epoch 14, train_loss: 0.5706, val_loss: 0.5144\n",
      "Epoch 15, train_loss: 0.5650, val_loss: 0.5104\n",
      "Epoch 16, train_loss: 0.5541, val_loss: 0.5049\n",
      "Epoch 17, train_loss: 0.5529, val_loss: 0.5007\n",
      "Epoch 18, train_loss: 0.5433, val_loss: 0.4973\n",
      "Epoch 19, train_loss: 0.5430, val_loss: 0.4930\n",
      "Epoch 20, train_loss: 0.5346, val_loss: 0.4892\n",
      "Epoch 21, train_loss: 0.5362, val_loss: 0.4848\n",
      "Epoch 22, train_loss: 0.5248, val_loss: 0.4821\n",
      "Epoch 23, train_loss: 0.5173, val_loss: 0.4789\n",
      "Epoch 24, train_loss: 0.5175, val_loss: 0.4757\n",
      "Epoch 25, train_loss: 0.5106, val_loss: 0.4717\n",
      "Epoch 26, train_loss: 0.5023, val_loss: 0.4698\n",
      "Epoch 27, train_loss: 0.5005, val_loss: 0.4661\n",
      "Epoch 28, train_loss: 0.4866, val_loss: 0.4640\n",
      "Epoch 29, train_loss: 0.4781, val_loss: 0.4625\n",
      "Epoch 30, train_loss: 0.4854, val_loss: 0.4597\n",
      "Epoch 31, train_loss: 0.4712, val_loss: 0.4560\n",
      "Epoch 32, train_loss: 0.4664, val_loss: 0.4534\n",
      "Epoch 33, train_loss: 0.4594, val_loss: 0.4508\n",
      "Epoch 34, train_loss: 0.4564, val_loss: 0.4480\n",
      "Epoch 35, train_loss: 0.4543, val_loss: 0.4468\n",
      "Epoch 36, train_loss: 0.4454, val_loss: 0.4447\n",
      "Epoch 37, train_loss: 0.4413, val_loss: 0.4424\n",
      "Epoch 38, train_loss: 0.4356, val_loss: 0.4411\n",
      "Epoch 39, train_loss: 0.4292, val_loss: 0.4397\n",
      "Epoch 40, train_loss: 0.4204, val_loss: 0.4370\n",
      "Epoch 41, train_loss: 0.4145, val_loss: 0.4345\n",
      "Epoch 42, train_loss: 0.4109, val_loss: 0.4339\n",
      "Epoch 43, train_loss: 0.4038, val_loss: 0.4330\n",
      "Epoch 44, train_loss: 0.3990, val_loss: 0.4311\n",
      "Epoch 45, train_loss: 0.3962, val_loss: 0.4302\n",
      "Epoch 46, train_loss: 0.3858, val_loss: 0.4292\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47, train_loss: 0.3735, val_loss: 0.4266\n",
      "Epoch 48, train_loss: 0.3780, val_loss: 0.4251\n",
      "Epoch 49, train_loss: 0.3635, val_loss: 0.4237\n",
      "Epoch 50, train_loss: 0.3558, val_loss: 0.4229\n",
      "Epoch 51, train_loss: 0.3565, val_loss: 0.4208\n",
      "Epoch 52, train_loss: 0.3530, val_loss: 0.4191\n",
      "Epoch 53, train_loss: 0.3494, val_loss: 0.4185\n",
      "Epoch 54, train_loss: 0.3388, val_loss: 0.4168\n",
      "Epoch 55, train_loss: 0.3375, val_loss: 0.4146\n",
      "Epoch 56, train_loss: 0.3257, val_loss: 0.4119\n",
      "Epoch 57, train_loss: 0.3325, val_loss: 0.4100\n",
      "Epoch 58, train_loss: 0.3143, val_loss: 0.4091\n",
      "Epoch 59, train_loss: 0.3061, val_loss: 0.4075\n",
      "Epoch 60, train_loss: 0.3016, val_loss: 0.4057\n",
      "Epoch 61, train_loss: 0.3023, val_loss: 0.4035\n",
      "Epoch 62, train_loss: 0.2919, val_loss: 0.4015\n",
      "Epoch 63, train_loss: 0.2859, val_loss: 0.4014\n",
      "Epoch 64, train_loss: 0.2806, val_loss: 0.4001\n",
      "Epoch 65, train_loss: 0.2731, val_loss: 0.3983\n",
      "Epoch 66, train_loss: 0.2716, val_loss: 0.3982\n",
      "Epoch 67, train_loss: 0.2651, val_loss: 0.3968\n",
      "Epoch 68, train_loss: 0.2652, val_loss: 0.3953\n",
      "Epoch 69, train_loss: 0.2632, val_loss: 0.3960\n",
      "Epoch 70, train_loss: 0.2572, val_loss: 0.3961\n",
      "Epoch 71, train_loss: 0.2419, val_loss: 0.3954\n",
      "Epoch 72, train_loss: 0.2504, val_loss: 0.3936\n",
      "Epoch 73, train_loss: 0.2386, val_loss: 0.3929\n",
      "Epoch 74, train_loss: 0.2407, val_loss: 0.3911\n",
      "Epoch 75, train_loss: 0.2311, val_loss: 0.3907\n",
      "Epoch 76, train_loss: 0.2270, val_loss: 0.3901\n",
      "Epoch 77, train_loss: 0.2284, val_loss: 0.3895\n",
      "Epoch 78, train_loss: 0.2250, val_loss: 0.3906\n",
      "Epoch 79, train_loss: 0.2136, val_loss: 0.3900\n",
      "Epoch 80, train_loss: 0.2186, val_loss: 0.3894\n",
      "Epoch 81, train_loss: 0.2188, val_loss: 0.3872\n",
      "Epoch 82, train_loss: 0.2092, val_loss: 0.3868\n",
      "Epoch 83, train_loss: 0.2113, val_loss: 0.3861\n",
      "Epoch 84, train_loss: 0.2084, val_loss: 0.3870\n",
      "Epoch 85, train_loss: 0.1981, val_loss: 0.3858\n",
      "Epoch 86, train_loss: 0.2003, val_loss: 0.3849\n",
      "Epoch 87, train_loss: 0.1942, val_loss: 0.3859\n",
      "Epoch 88, train_loss: 0.1974, val_loss: 0.3865\n",
      "Epoch 89, train_loss: 0.1893, val_loss: 0.3845\n",
      "Epoch 90, train_loss: 0.1886, val_loss: 0.3841\n",
      "Epoch 91, train_loss: 0.1876, val_loss: 0.3834\n",
      "Epoch 92, train_loss: 0.1821, val_loss: 0.3820\n",
      "Epoch 93, train_loss: 0.1872, val_loss: 0.3817\n",
      "Epoch 94, train_loss: 0.1778, val_loss: 0.3806\n",
      "Epoch 95, train_loss: 0.1837, val_loss: 0.3820\n",
      "Epoch 96, train_loss: 0.1749, val_loss: 0.3811\n",
      "Epoch 97, train_loss: 0.1730, val_loss: 0.3811\n",
      "Epoch 98, train_loss: 0.1780, val_loss: 0.3805\n",
      "Epoch 99, train_loss: 0.1763, val_loss: 0.3822\n",
      "Test the model on fold 2:\n",
      "{'roc_auc': 0.8650519031141869, 'pr_auc': 0.8558056015495462, 'fmax': 0.8205079027250701}\n",
      "Starting Fold: 3\n",
      "Epoch 0, train_loss: 0.7032, val_loss: 0.6728\n",
      "Epoch 1, train_loss: 0.6813, val_loss: 0.6518\n",
      "Epoch 2, train_loss: 0.6663, val_loss: 0.6315\n",
      "Epoch 3, train_loss: 0.6515, val_loss: 0.6125\n",
      "Epoch 4, train_loss: 0.6508, val_loss: 0.6018\n",
      "Epoch 5, train_loss: 0.6432, val_loss: 0.5872\n",
      "Epoch 6, train_loss: 0.6282, val_loss: 0.5757\n",
      "Epoch 7, train_loss: 0.6244, val_loss: 0.5651\n",
      "Epoch 8, train_loss: 0.6068, val_loss: 0.5587\n",
      "Epoch 9, train_loss: 0.5986, val_loss: 0.5509\n",
      "Epoch 10, train_loss: 0.5982, val_loss: 0.5439\n",
      "Epoch 11, train_loss: 0.5903, val_loss: 0.5389\n",
      "Epoch 12, train_loss: 0.5920, val_loss: 0.5311\n",
      "Epoch 13, train_loss: 0.5763, val_loss: 0.5274\n",
      "Epoch 14, train_loss: 0.5722, val_loss: 0.5221\n",
      "Epoch 15, train_loss: 0.5691, val_loss: 0.5178\n",
      "Epoch 16, train_loss: 0.5666, val_loss: 0.5150\n",
      "Epoch 17, train_loss: 0.5646, val_loss: 0.5120\n",
      "Epoch 18, train_loss: 0.5609, val_loss: 0.5079\n",
      "Epoch 19, train_loss: 0.5587, val_loss: 0.5036\n",
      "Epoch 20, train_loss: 0.5473, val_loss: 0.5005\n",
      "Epoch 21, train_loss: 0.5396, val_loss: 0.4973\n",
      "Epoch 22, train_loss: 0.5320, val_loss: 0.4941\n",
      "Epoch 23, train_loss: 0.5339, val_loss: 0.4920\n",
      "Epoch 24, train_loss: 0.5248, val_loss: 0.4890\n",
      "Epoch 25, train_loss: 0.5188, val_loss: 0.4848\n",
      "Epoch 26, train_loss: 0.5136, val_loss: 0.4833\n",
      "Epoch 27, train_loss: 0.5222, val_loss: 0.4806\n",
      "Epoch 28, train_loss: 0.5064, val_loss: 0.4787\n",
      "Epoch 29, train_loss: 0.5039, val_loss: 0.4747\n",
      "Epoch 30, train_loss: 0.4994, val_loss: 0.4723\n",
      "Epoch 31, train_loss: 0.4955, val_loss: 0.4691\n",
      "Epoch 32, train_loss: 0.4873, val_loss: 0.4658\n",
      "Epoch 33, train_loss: 0.4742, val_loss: 0.4648\n",
      "Epoch 34, train_loss: 0.4712, val_loss: 0.4624\n",
      "Epoch 35, train_loss: 0.4767, val_loss: 0.4595\n",
      "Epoch 36, train_loss: 0.4621, val_loss: 0.4575\n",
      "Epoch 37, train_loss: 0.4611, val_loss: 0.4539\n",
      "Epoch 38, train_loss: 0.4593, val_loss: 0.4495\n",
      "Epoch 39, train_loss: 0.4475, val_loss: 0.4474\n",
      "Epoch 40, train_loss: 0.4468, val_loss: 0.4453\n",
      "Epoch 41, train_loss: 0.4336, val_loss: 0.4420\n",
      "Epoch 42, train_loss: 0.4367, val_loss: 0.4407\n",
      "Epoch 43, train_loss: 0.4279, val_loss: 0.4391\n",
      "Epoch 44, train_loss: 0.4257, val_loss: 0.4372\n",
      "Epoch 45, train_loss: 0.4203, val_loss: 0.4346\n",
      "Epoch 46, train_loss: 0.4047, val_loss: 0.4325\n",
      "Epoch 47, train_loss: 0.4254, val_loss: 0.4300\n",
      "Epoch 48, train_loss: 0.3931, val_loss: 0.4282\n",
      "Epoch 49, train_loss: 0.3978, val_loss: 0.4261\n",
      "Epoch 50, train_loss: 0.3773, val_loss: 0.4229\n",
      "Epoch 51, train_loss: 0.3862, val_loss: 0.4212\n",
      "Epoch 52, train_loss: 0.3725, val_loss: 0.4180\n",
      "Epoch 53, train_loss: 0.3674, val_loss: 0.4162\n",
      "Epoch 54, train_loss: 0.3723, val_loss: 0.4134\n",
      "Epoch 55, train_loss: 0.3527, val_loss: 0.4108\n",
      "Epoch 56, train_loss: 0.3552, val_loss: 0.4086\n",
      "Epoch 57, train_loss: 0.3484, val_loss: 0.4064\n",
      "Epoch 58, train_loss: 0.3455, val_loss: 0.4052\n",
      "Epoch 59, train_loss: 0.3336, val_loss: 0.4034\n",
      "Epoch 60, train_loss: 0.3339, val_loss: 0.4025\n",
      "Epoch 61, train_loss: 0.3229, val_loss: 0.4017\n",
      "Epoch 62, train_loss: 0.3229, val_loss: 0.4011\n",
      "Epoch 63, train_loss: 0.3173, val_loss: 0.3982\n",
      "Epoch 64, train_loss: 0.3158, val_loss: 0.3953\n",
      "Epoch 65, train_loss: 0.3074, val_loss: 0.3932\n",
      "Epoch 66, train_loss: 0.3004, val_loss: 0.3915\n",
      "Epoch 67, train_loss: 0.3009, val_loss: 0.3909\n",
      "Epoch 68, train_loss: 0.2918, val_loss: 0.3903\n",
      "Epoch 69, train_loss: 0.3051, val_loss: 0.3891\n",
      "Epoch 70, train_loss: 0.2819, val_loss: 0.3879\n",
      "Epoch 71, train_loss: 0.2860, val_loss: 0.3853\n",
      "Epoch 72, train_loss: 0.2855, val_loss: 0.3837\n",
      "Epoch 73, train_loss: 0.2669, val_loss: 0.3827\n",
      "Epoch 74, train_loss: 0.2693, val_loss: 0.3811\n",
      "Epoch 75, train_loss: 0.2634, val_loss: 0.3788\n",
      "Epoch 76, train_loss: 0.2595, val_loss: 0.3789\n",
      "Epoch 77, train_loss: 0.2559, val_loss: 0.3778\n",
      "Epoch 78, train_loss: 0.2487, val_loss: 0.3740\n",
      "Epoch 79, train_loss: 0.2467, val_loss: 0.3727\n",
      "Epoch 80, train_loss: 0.2417, val_loss: 0.3723\n",
      "Epoch 81, train_loss: 0.2390, val_loss: 0.3719\n",
      "Epoch 82, train_loss: 0.2288, val_loss: 0.3710\n",
      "Epoch 83, train_loss: 0.2395, val_loss: 0.3698\n",
      "Epoch 84, train_loss: 0.2343, val_loss: 0.3673\n",
      "Epoch 85, train_loss: 0.2212, val_loss: 0.3681\n",
      "Epoch 86, train_loss: 0.2224, val_loss: 0.3673\n",
      "Epoch 87, train_loss: 0.2359, val_loss: 0.3646\n",
      "Epoch 88, train_loss: 0.2322, val_loss: 0.3635\n",
      "Epoch 89, train_loss: 0.2201, val_loss: 0.3630\n",
      "Epoch 90, train_loss: 0.2147, val_loss: 0.3616\n",
      "Epoch 91, train_loss: 0.2118, val_loss: 0.3593\n",
      "Epoch 92, train_loss: 0.2124, val_loss: 0.3575\n",
      "Epoch 93, train_loss: 0.2120, val_loss: 0.3570\n",
      "Epoch 94, train_loss: 0.2100, val_loss: 0.3585\n",
      "Epoch 95, train_loss: 0.2031, val_loss: 0.3589\n",
      "Epoch 96, train_loss: 0.1922, val_loss: 0.3596\n",
      "Epoch 97, train_loss: 0.2006, val_loss: 0.3583\n",
      "Epoch 98, train_loss: 0.1962, val_loss: 0.3552\n",
      "Epoch 99, train_loss: 0.1987, val_loss: 0.3539\n",
      "Test the model on fold 3:\n",
      "{'roc_auc': 0.9166666666666666, 'pr_auc': 0.9466197408076091, 'fmax': 0.8749950781526854}\n",
      "Starting Fold: 4\n",
      "Epoch 0, train_loss: 0.6908, val_loss: 0.6648\n",
      "Epoch 1, train_loss: 0.6760, val_loss: 0.6481\n",
      "Epoch 2, train_loss: 0.6693, val_loss: 0.6263\n",
      "Epoch 3, train_loss: 0.6501, val_loss: 0.6107\n",
      "Epoch 4, train_loss: 0.6364, val_loss: 0.5973\n",
      "Epoch 5, train_loss: 0.6266, val_loss: 0.5861\n",
      "Epoch 6, train_loss: 0.6145, val_loss: 0.5757\n",
      "Epoch 7, train_loss: 0.6033, val_loss: 0.5678\n",
      "Epoch 8, train_loss: 0.5972, val_loss: 0.5596\n",
      "Epoch 9, train_loss: 0.5998, val_loss: 0.5523\n",
      "Epoch 10, train_loss: 0.5869, val_loss: 0.5445\n",
      "Epoch 11, train_loss: 0.5824, val_loss: 0.5386\n",
      "Epoch 12, train_loss: 0.5757, val_loss: 0.5319\n",
      "Epoch 13, train_loss: 0.5625, val_loss: 0.5261\n",
      "Epoch 14, train_loss: 0.5645, val_loss: 0.5210\n",
      "Epoch 15, train_loss: 0.5582, val_loss: 0.5163\n",
      "Epoch 16, train_loss: 0.5380, val_loss: 0.5144\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17, train_loss: 0.5459, val_loss: 0.5114\n",
      "Epoch 18, train_loss: 0.5336, val_loss: 0.5081\n",
      "Epoch 19, train_loss: 0.5332, val_loss: 0.5027\n",
      "Epoch 20, train_loss: 0.5323, val_loss: 0.4980\n",
      "Epoch 21, train_loss: 0.5321, val_loss: 0.4931\n",
      "Epoch 22, train_loss: 0.5148, val_loss: 0.4912\n",
      "Epoch 23, train_loss: 0.5027, val_loss: 0.4901\n",
      "Epoch 24, train_loss: 0.4932, val_loss: 0.4890\n",
      "Epoch 25, train_loss: 0.5074, val_loss: 0.4853\n",
      "Epoch 26, train_loss: 0.4941, val_loss: 0.4820\n",
      "Epoch 27, train_loss: 0.4842, val_loss: 0.4776\n",
      "Epoch 28, train_loss: 0.4766, val_loss: 0.4736\n",
      "Epoch 29, train_loss: 0.4749, val_loss: 0.4708\n",
      "Epoch 30, train_loss: 0.4708, val_loss: 0.4706\n",
      "Epoch 31, train_loss: 0.4721, val_loss: 0.4686\n",
      "Epoch 32, train_loss: 0.4532, val_loss: 0.4669\n",
      "Epoch 33, train_loss: 0.4447, val_loss: 0.4640\n",
      "Epoch 34, train_loss: 0.4462, val_loss: 0.4605\n",
      "Epoch 35, train_loss: 0.4443, val_loss: 0.4576\n",
      "Epoch 36, train_loss: 0.4370, val_loss: 0.4555\n",
      "Epoch 37, train_loss: 0.4228, val_loss: 0.4542\n",
      "Epoch 38, train_loss: 0.4227, val_loss: 0.4517\n",
      "Epoch 39, train_loss: 0.4105, val_loss: 0.4504\n",
      "Epoch 40, train_loss: 0.4128, val_loss: 0.4477\n",
      "Epoch 41, train_loss: 0.4101, val_loss: 0.4470\n",
      "Epoch 42, train_loss: 0.3905, val_loss: 0.4465\n",
      "Epoch 43, train_loss: 0.3974, val_loss: 0.4441\n",
      "Epoch 44, train_loss: 0.3780, val_loss: 0.4438\n",
      "Epoch 45, train_loss: 0.3733, val_loss: 0.4422\n",
      "Epoch 46, train_loss: 0.3798, val_loss: 0.4406\n",
      "Epoch 47, train_loss: 0.3619, val_loss: 0.4378\n",
      "Epoch 48, train_loss: 0.3569, val_loss: 0.4355\n",
      "Epoch 49, train_loss: 0.3621, val_loss: 0.4342\n",
      "Epoch 50, train_loss: 0.3499, val_loss: 0.4327\n",
      "Epoch 51, train_loss: 0.3422, val_loss: 0.4318\n",
      "Epoch 52, train_loss: 0.3372, val_loss: 0.4309\n",
      "Epoch 53, train_loss: 0.3384, val_loss: 0.4297\n",
      "Epoch 54, train_loss: 0.3286, val_loss: 0.4280\n",
      "Epoch 55, train_loss: 0.3261, val_loss: 0.4272\n",
      "Epoch 56, train_loss: 0.3208, val_loss: 0.4265\n",
      "Epoch 57, train_loss: 0.3108, val_loss: 0.4256\n",
      "Epoch 58, train_loss: 0.3147, val_loss: 0.4227\n",
      "Epoch 59, train_loss: 0.2952, val_loss: 0.4227\n",
      "Epoch 60, train_loss: 0.3075, val_loss: 0.4209\n",
      "Epoch 61, train_loss: 0.2989, val_loss: 0.4185\n",
      "Epoch 62, train_loss: 0.2900, val_loss: 0.4176\n",
      "Epoch 63, train_loss: 0.2820, val_loss: 0.4165\n",
      "Epoch 64, train_loss: 0.2767, val_loss: 0.4155\n",
      "Epoch 65, train_loss: 0.2785, val_loss: 0.4153\n",
      "Epoch 66, train_loss: 0.2729, val_loss: 0.4108\n",
      "Epoch 67, train_loss: 0.2653, val_loss: 0.4106\n",
      "Epoch 68, train_loss: 0.2613, val_loss: 0.4128\n",
      "Epoch 69, train_loss: 0.2554, val_loss: 0.4133\n",
      "Epoch 70, train_loss: 0.2504, val_loss: 0.4105\n",
      "Epoch 71, train_loss: 0.2458, val_loss: 0.4084\n",
      "Epoch 72, train_loss: 0.2442, val_loss: 0.4070\n",
      "Epoch 73, train_loss: 0.2402, val_loss: 0.4057\n",
      "Epoch 74, train_loss: 0.2314, val_loss: 0.4070\n",
      "Epoch 75, train_loss: 0.2340, val_loss: 0.4053\n",
      "Epoch 76, train_loss: 0.2263, val_loss: 0.4023\n",
      "Epoch 77, train_loss: 0.2240, val_loss: 0.4010\n",
      "Epoch 78, train_loss: 0.2162, val_loss: 0.4006\n",
      "Epoch 79, train_loss: 0.2147, val_loss: 0.4003\n",
      "Epoch 80, train_loss: 0.2218, val_loss: 0.4007\n",
      "Epoch 81, train_loss: 0.2123, val_loss: 0.4002\n",
      "Epoch 82, train_loss: 0.2053, val_loss: 0.4001\n",
      "Epoch 83, train_loss: 0.1985, val_loss: 0.3989\n",
      "Epoch 84, train_loss: 0.1980, val_loss: 0.3986\n",
      "Epoch 85, train_loss: 0.1966, val_loss: 0.3986\n",
      "Epoch 86, train_loss: 0.2032, val_loss: 0.3996\n",
      "Epoch 87, train_loss: 0.1952, val_loss: 0.3972\n",
      "Epoch 88, train_loss: 0.1952, val_loss: 0.3979\n",
      "Epoch 89, train_loss: 0.1921, val_loss: 0.3942\n",
      "Epoch 90, train_loss: 0.1839, val_loss: 0.3929\n",
      "Epoch 91, train_loss: 0.1784, val_loss: 0.3939\n",
      "Epoch 92, train_loss: 0.1782, val_loss: 0.3934\n",
      "Epoch 93, train_loss: 0.1885, val_loss: 0.3923\n",
      "Epoch 94, train_loss: 0.1732, val_loss: 0.3921\n",
      "Epoch 95, train_loss: 0.1779, val_loss: 0.3912\n",
      "Epoch 96, train_loss: 0.1708, val_loss: 0.3912\n",
      "Epoch 97, train_loss: 0.1651, val_loss: 0.3912\n",
      "Epoch 98, train_loss: 0.1730, val_loss: 0.3912\n",
      "Epoch 99, train_loss: 0.1720, val_loss: 0.3917\n",
      "Test the model on fold 4:\n",
      "{'roc_auc': 0.8035714285714285, 'pr_auc': 0.846428743784904, 'fmax': 0.7826039319760663}\n",
      "Evaluate pretrained model on disease class Developmental (12/12)\n",
      "Starting Fold: 0\n",
      "Epoch 0, train_loss: 0.7095, val_loss: 0.6851\n",
      "Epoch 1, train_loss: 0.6721, val_loss: 0.6834\n",
      "Epoch 2, train_loss: 0.6698, val_loss: 0.6803\n",
      "Epoch 3, train_loss: 0.6788, val_loss: 0.6775\n",
      "Epoch 4, train_loss: 0.6549, val_loss: 0.6754\n",
      "Epoch 5, train_loss: 0.6688, val_loss: 0.6734\n",
      "Epoch 6, train_loss: 0.6575, val_loss: 0.6712\n",
      "Epoch 7, train_loss: 0.6527, val_loss: 0.6698\n",
      "Epoch 8, train_loss: 0.6475, val_loss: 0.6681\n",
      "Epoch 9, train_loss: 0.6370, val_loss: 0.6660\n",
      "Epoch 10, train_loss: 0.6526, val_loss: 0.6646\n",
      "Epoch 11, train_loss: 0.6446, val_loss: 0.6629\n",
      "Epoch 12, train_loss: 0.6134, val_loss: 0.6620\n",
      "Epoch 13, train_loss: 0.6247, val_loss: 0.6596\n",
      "Epoch 14, train_loss: 0.6134, val_loss: 0.6579\n",
      "Epoch 15, train_loss: 0.6131, val_loss: 0.6562\n",
      "Epoch 16, train_loss: 0.5869, val_loss: 0.6546\n",
      "Epoch 17, train_loss: 0.5935, val_loss: 0.6530\n",
      "Epoch 18, train_loss: 0.5897, val_loss: 0.6516\n",
      "Epoch 19, train_loss: 0.5905, val_loss: 0.6512\n",
      "Epoch 20, train_loss: 0.5969, val_loss: 0.6498\n",
      "Epoch 21, train_loss: 0.5899, val_loss: 0.6483\n",
      "Epoch 22, train_loss: 0.5722, val_loss: 0.6480\n",
      "Epoch 23, train_loss: 0.5304, val_loss: 0.6461\n",
      "Epoch 24, train_loss: 0.5584, val_loss: 0.6439\n",
      "Epoch 25, train_loss: 0.5430, val_loss: 0.6418\n",
      "Epoch 26, train_loss: 0.5185, val_loss: 0.6403\n",
      "Epoch 27, train_loss: 0.5382, val_loss: 0.6386\n",
      "Epoch 28, train_loss: 0.5342, val_loss: 0.6371\n",
      "Epoch 29, train_loss: 0.5361, val_loss: 0.6352\n",
      "Epoch 30, train_loss: 0.5591, val_loss: 0.6333\n",
      "Epoch 31, train_loss: 0.5417, val_loss: 0.6318\n",
      "Epoch 32, train_loss: 0.5302, val_loss: 0.6302\n",
      "Epoch 33, train_loss: 0.5032, val_loss: 0.6283\n",
      "Epoch 34, train_loss: 0.4953, val_loss: 0.6260\n",
      "Epoch 35, train_loss: 0.5369, val_loss: 0.6225\n",
      "Epoch 36, train_loss: 0.4588, val_loss: 0.6197\n",
      "Epoch 37, train_loss: 0.5059, val_loss: 0.6179\n",
      "Epoch 38, train_loss: 0.4666, val_loss: 0.6172\n",
      "Epoch 39, train_loss: 0.4942, val_loss: 0.6170\n",
      "Epoch 40, train_loss: 0.4940, val_loss: 0.6161\n",
      "Epoch 41, train_loss: 0.4406, val_loss: 0.6145\n",
      "Epoch 42, train_loss: 0.4642, val_loss: 0.6141\n",
      "Epoch 43, train_loss: 0.4363, val_loss: 0.6117\n",
      "Epoch 44, train_loss: 0.4558, val_loss: 0.6103\n",
      "Epoch 45, train_loss: 0.4873, val_loss: 0.6096\n",
      "Epoch 46, train_loss: 0.4579, val_loss: 0.6079\n",
      "Epoch 47, train_loss: 0.4533, val_loss: 0.6046\n",
      "Epoch 48, train_loss: 0.4345, val_loss: 0.6030\n",
      "Epoch 49, train_loss: 0.4734, val_loss: 0.6018\n",
      "Epoch 50, train_loss: 0.4327, val_loss: 0.6000\n",
      "Epoch 51, train_loss: 0.4825, val_loss: 0.5989\n",
      "Epoch 52, train_loss: 0.4467, val_loss: 0.5977\n",
      "Epoch 53, train_loss: 0.3855, val_loss: 0.5965\n",
      "Epoch 54, train_loss: 0.4281, val_loss: 0.5963\n",
      "Epoch 55, train_loss: 0.3885, val_loss: 0.5952\n",
      "Epoch 56, train_loss: 0.4020, val_loss: 0.5939\n",
      "Epoch 57, train_loss: 0.3559, val_loss: 0.5923\n",
      "Epoch 58, train_loss: 0.3982, val_loss: 0.5904\n",
      "Epoch 59, train_loss: 0.3667, val_loss: 0.5886\n",
      "Epoch 60, train_loss: 0.4124, val_loss: 0.5884\n",
      "Epoch 61, train_loss: 0.3626, val_loss: 0.5870\n",
      "Epoch 62, train_loss: 0.4060, val_loss: 0.5851\n",
      "Epoch 63, train_loss: 0.3656, val_loss: 0.5839\n",
      "Epoch 64, train_loss: 0.3322, val_loss: 0.5821\n",
      "Epoch 65, train_loss: 0.3496, val_loss: 0.5790\n",
      "Epoch 66, train_loss: 0.3891, val_loss: 0.5767\n",
      "Epoch 67, train_loss: 0.3750, val_loss: 0.5751\n",
      "Epoch 68, train_loss: 0.3858, val_loss: 0.5738\n",
      "Epoch 69, train_loss: 0.3359, val_loss: 0.5724\n",
      "Epoch 70, train_loss: 0.3625, val_loss: 0.5715\n",
      "Epoch 71, train_loss: 0.3280, val_loss: 0.5697\n",
      "Epoch 72, train_loss: 0.3278, val_loss: 0.5675\n",
      "Epoch 73, train_loss: 0.3213, val_loss: 0.5652\n",
      "Epoch 74, train_loss: 0.3483, val_loss: 0.5630\n",
      "Epoch 75, train_loss: 0.3235, val_loss: 0.5617\n",
      "Epoch 76, train_loss: 0.3429, val_loss: 0.5605\n",
      "Epoch 77, train_loss: 0.3492, val_loss: 0.5592\n",
      "Epoch 78, train_loss: 0.2864, val_loss: 0.5584\n",
      "Epoch 79, train_loss: 0.2681, val_loss: 0.5570\n",
      "Epoch 80, train_loss: 0.2780, val_loss: 0.5562\n",
      "Epoch 81, train_loss: 0.3037, val_loss: 0.5548\n",
      "Epoch 82, train_loss: 0.2748, val_loss: 0.5533\n",
      "Epoch 83, train_loss: 0.3458, val_loss: 0.5522\n",
      "Epoch 84, train_loss: 0.2533, val_loss: 0.5514\n",
      "Epoch 85, train_loss: 0.2506, val_loss: 0.5492\n",
      "Epoch 86, train_loss: 0.3307, val_loss: 0.5464\n",
      "Epoch 87, train_loss: 0.2528, val_loss: 0.5450\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 88, train_loss: 0.2815, val_loss: 0.5436\n",
      "Epoch 89, train_loss: 0.2752, val_loss: 0.5418\n",
      "Epoch 90, train_loss: 0.2394, val_loss: 0.5407\n",
      "Epoch 91, train_loss: 0.2673, val_loss: 0.5399\n",
      "Epoch 92, train_loss: 0.2672, val_loss: 0.5393\n",
      "Epoch 93, train_loss: 0.2556, val_loss: 0.5389\n",
      "Epoch 94, train_loss: 0.2344, val_loss: 0.5379\n",
      "Epoch 95, train_loss: 0.2591, val_loss: 0.5375\n",
      "Epoch 96, train_loss: 0.2354, val_loss: 0.5372\n",
      "Epoch 97, train_loss: 0.2148, val_loss: 0.5359\n",
      "Epoch 98, train_loss: 0.2206, val_loss: 0.5345\n",
      "Epoch 99, train_loss: 0.2355, val_loss: 0.5331\n",
      "Test the model on fold 0:\n",
      "{'roc_auc': 0.7651515151515151, 'pr_auc': 0.7988982529886423, 'fmax': 0.8333283333633332}\n",
      "Starting Fold: 1\n",
      "Epoch 0, train_loss: 0.7008, val_loss: 0.6969\n",
      "Epoch 1, train_loss: 0.6712, val_loss: 0.6957\n",
      "Epoch 2, train_loss: 0.6863, val_loss: 0.6936\n",
      "Epoch 3, train_loss: 0.6851, val_loss: 0.6926\n",
      "Epoch 4, train_loss: 0.6500, val_loss: 0.6926\n",
      "Epoch 5, train_loss: 0.6713, val_loss: 0.6929\n",
      "Epoch 6, train_loss: 0.6481, val_loss: 0.6937\n",
      "Epoch 7, train_loss: 0.6423, val_loss: 0.6959\n",
      "Epoch 8, train_loss: 0.6437, val_loss: 0.6962\n",
      "Epoch 9, train_loss: 0.6315, val_loss: 0.6960\n",
      "Epoch 10, train_loss: 0.6303, val_loss: 0.6971\n",
      "Epoch 11, train_loss: 0.6297, val_loss: 0.6999\n",
      "Epoch 12, train_loss: 0.6188, val_loss: 0.7017\n",
      "Epoch 13, train_loss: 0.6209, val_loss: 0.7025\n",
      "Epoch 14, train_loss: 0.6313, val_loss: 0.7030\n",
      "Epoch 15, train_loss: 0.5990, val_loss: 0.7028\n",
      "Epoch 16, train_loss: 0.6057, val_loss: 0.7026\n",
      "Epoch 17, train_loss: 0.6041, val_loss: 0.7037\n",
      "Early Stopping!\n",
      "Test the model on fold 1:\n",
      "{'roc_auc': 0.7380952380952381, 'pr_auc': 0.8291980787530322, 'fmax': 0.8484799632971809}\n",
      "Starting Fold: 2\n",
      "Epoch 0, train_loss: 0.6995, val_loss: 0.6908\n",
      "Epoch 1, train_loss: 0.7050, val_loss: 0.6892\n",
      "Epoch 2, train_loss: 0.6931, val_loss: 0.6895\n",
      "Epoch 3, train_loss: 0.6722, val_loss: 0.6891\n",
      "Epoch 4, train_loss: 0.6705, val_loss: 0.6877\n",
      "Epoch 5, train_loss: 0.6672, val_loss: 0.6866\n",
      "Epoch 6, train_loss: 0.6440, val_loss: 0.6853\n",
      "Epoch 7, train_loss: 0.6577, val_loss: 0.6839\n",
      "Epoch 8, train_loss: 0.6523, val_loss: 0.6830\n",
      "Epoch 9, train_loss: 0.6477, val_loss: 0.6829\n",
      "Epoch 10, train_loss: 0.6193, val_loss: 0.6824\n",
      "Epoch 11, train_loss: 0.6479, val_loss: 0.6812\n",
      "Epoch 12, train_loss: 0.6438, val_loss: 0.6806\n",
      "Epoch 13, train_loss: 0.6231, val_loss: 0.6803\n",
      "Epoch 14, train_loss: 0.6142, val_loss: 0.6793\n",
      "Epoch 15, train_loss: 0.6073, val_loss: 0.6786\n",
      "Epoch 16, train_loss: 0.6102, val_loss: 0.6767\n",
      "Epoch 17, train_loss: 0.6096, val_loss: 0.6751\n",
      "Epoch 18, train_loss: 0.6176, val_loss: 0.6741\n",
      "Epoch 19, train_loss: 0.5976, val_loss: 0.6732\n",
      "Epoch 20, train_loss: 0.6114, val_loss: 0.6725\n",
      "Epoch 21, train_loss: 0.6053, val_loss: 0.6715\n",
      "Epoch 22, train_loss: 0.6028, val_loss: 0.6705\n",
      "Epoch 23, train_loss: 0.5870, val_loss: 0.6696\n",
      "Epoch 24, train_loss: 0.5886, val_loss: 0.6682\n",
      "Epoch 25, train_loss: 0.6056, val_loss: 0.6673\n",
      "Epoch 26, train_loss: 0.5765, val_loss: 0.6665\n",
      "Epoch 27, train_loss: 0.5568, val_loss: 0.6660\n",
      "Epoch 28, train_loss: 0.5565, val_loss: 0.6661\n",
      "Epoch 29, train_loss: 0.5602, val_loss: 0.6664\n",
      "Epoch 30, train_loss: 0.5758, val_loss: 0.6660\n",
      "Epoch 31, train_loss: 0.5343, val_loss: 0.6652\n",
      "Epoch 32, train_loss: 0.5775, val_loss: 0.6643\n",
      "Epoch 33, train_loss: 0.5912, val_loss: 0.6633\n",
      "Epoch 34, train_loss: 0.5408, val_loss: 0.6624\n",
      "Epoch 35, train_loss: 0.5700, val_loss: 0.6608\n",
      "Epoch 36, train_loss: 0.5673, val_loss: 0.6593\n",
      "Epoch 37, train_loss: 0.5617, val_loss: 0.6577\n",
      "Epoch 38, train_loss: 0.5476, val_loss: 0.6560\n",
      "Epoch 39, train_loss: 0.5218, val_loss: 0.6551\n",
      "Epoch 40, train_loss: 0.5284, val_loss: 0.6545\n",
      "Epoch 41, train_loss: 0.5396, val_loss: 0.6537\n",
      "Epoch 42, train_loss: 0.5350, val_loss: 0.6530\n",
      "Epoch 43, train_loss: 0.5196, val_loss: 0.6520\n",
      "Epoch 44, train_loss: 0.4874, val_loss: 0.6507\n",
      "Epoch 45, train_loss: 0.4998, val_loss: 0.6492\n",
      "Epoch 46, train_loss: 0.5349, val_loss: 0.6475\n",
      "Epoch 47, train_loss: 0.5041, val_loss: 0.6461\n",
      "Epoch 48, train_loss: 0.5364, val_loss: 0.6448\n",
      "Epoch 49, train_loss: 0.5062, val_loss: 0.6441\n",
      "Epoch 50, train_loss: 0.4868, val_loss: 0.6437\n",
      "Epoch 51, train_loss: 0.5041, val_loss: 0.6431\n",
      "Epoch 52, train_loss: 0.5080, val_loss: 0.6426\n",
      "Epoch 53, train_loss: 0.5009, val_loss: 0.6419\n",
      "Epoch 54, train_loss: 0.5105, val_loss: 0.6408\n",
      "Epoch 55, train_loss: 0.4859, val_loss: 0.6397\n",
      "Epoch 56, train_loss: 0.4897, val_loss: 0.6385\n",
      "Epoch 57, train_loss: 0.4810, val_loss: 0.6375\n",
      "Epoch 58, train_loss: 0.4639, val_loss: 0.6359\n",
      "Epoch 59, train_loss: 0.4835, val_loss: 0.6347\n",
      "Epoch 60, train_loss: 0.4330, val_loss: 0.6338\n",
      "Epoch 61, train_loss: 0.4537, val_loss: 0.6328\n",
      "Epoch 62, train_loss: 0.4193, val_loss: 0.6319\n",
      "Epoch 63, train_loss: 0.4228, val_loss: 0.6305\n",
      "Epoch 64, train_loss: 0.4582, val_loss: 0.6295\n",
      "Epoch 65, train_loss: 0.4284, val_loss: 0.6272\n",
      "Epoch 66, train_loss: 0.4214, val_loss: 0.6249\n",
      "Epoch 67, train_loss: 0.4653, val_loss: 0.6225\n",
      "Epoch 68, train_loss: 0.4390, val_loss: 0.6205\n",
      "Epoch 69, train_loss: 0.4261, val_loss: 0.6182\n",
      "Epoch 70, train_loss: 0.4050, val_loss: 0.6171\n",
      "Epoch 71, train_loss: 0.3999, val_loss: 0.6166\n",
      "Epoch 72, train_loss: 0.4190, val_loss: 0.6166\n",
      "Epoch 73, train_loss: 0.3856, val_loss: 0.6159\n",
      "Epoch 74, train_loss: 0.3871, val_loss: 0.6148\n",
      "Epoch 75, train_loss: 0.3666, val_loss: 0.6140\n",
      "Epoch 76, train_loss: 0.3786, val_loss: 0.6119\n",
      "Epoch 77, train_loss: 0.3895, val_loss: 0.6114\n",
      "Epoch 78, train_loss: 0.4171, val_loss: 0.6111\n",
      "Epoch 79, train_loss: 0.3875, val_loss: 0.6098\n",
      "Epoch 80, train_loss: 0.3529, val_loss: 0.6079\n",
      "Epoch 81, train_loss: 0.3471, val_loss: 0.6068\n",
      "Epoch 82, train_loss: 0.3729, val_loss: 0.6059\n",
      "Epoch 83, train_loss: 0.3476, val_loss: 0.6049\n",
      "Epoch 84, train_loss: 0.3389, val_loss: 0.6042\n",
      "Epoch 85, train_loss: 0.3717, val_loss: 0.6037\n",
      "Epoch 86, train_loss: 0.4022, val_loss: 0.6036\n",
      "Epoch 87, train_loss: 0.3395, val_loss: 0.6036\n",
      "Epoch 88, train_loss: 0.3120, val_loss: 0.6033\n",
      "Epoch 89, train_loss: 0.3081, val_loss: 0.6032\n",
      "Epoch 90, train_loss: 0.3013, val_loss: 0.6029\n",
      "Epoch 91, train_loss: 0.3277, val_loss: 0.6027\n",
      "Epoch 92, train_loss: 0.2994, val_loss: 0.6015\n",
      "Epoch 93, train_loss: 0.3277, val_loss: 0.5999\n",
      "Epoch 94, train_loss: 0.3028, val_loss: 0.5996\n",
      "Epoch 95, train_loss: 0.3746, val_loss: 0.5990\n",
      "Epoch 96, train_loss: 0.2901, val_loss: 0.5993\n",
      "Epoch 97, train_loss: 0.2928, val_loss: 0.6001\n",
      "Epoch 98, train_loss: 0.2969, val_loss: 0.6006\n",
      "Epoch 99, train_loss: 0.2727, val_loss: 0.5999\n",
      "Test the model on fold 2:\n",
      "{'roc_auc': 0.65, 'pr_auc': 0.7349026390705958, 'fmax': 0.8333284722505785}\n",
      "Starting Fold: 3\n",
      "Epoch 0, train_loss: 0.7149, val_loss: 0.6720\n",
      "Epoch 1, train_loss: 0.6863, val_loss: 0.6695\n",
      "Epoch 2, train_loss: 0.6768, val_loss: 0.6654\n",
      "Epoch 3, train_loss: 0.6640, val_loss: 0.6614\n",
      "Epoch 4, train_loss: 0.6224, val_loss: 0.6568\n",
      "Epoch 5, train_loss: 0.6402, val_loss: 0.6527\n",
      "Epoch 6, train_loss: 0.6447, val_loss: 0.6497\n",
      "Epoch 7, train_loss: 0.6092, val_loss: 0.6459\n",
      "Epoch 8, train_loss: 0.6438, val_loss: 0.6426\n",
      "Epoch 9, train_loss: 0.6539, val_loss: 0.6408\n",
      "Epoch 10, train_loss: 0.6516, val_loss: 0.6399\n",
      "Epoch 11, train_loss: 0.6156, val_loss: 0.6395\n",
      "Epoch 12, train_loss: 0.6289, val_loss: 0.6384\n",
      "Epoch 13, train_loss: 0.6317, val_loss: 0.6372\n",
      "Epoch 14, train_loss: 0.6080, val_loss: 0.6352\n",
      "Epoch 15, train_loss: 0.6131, val_loss: 0.6342\n",
      "Epoch 16, train_loss: 0.5939, val_loss: 0.6329\n",
      "Epoch 17, train_loss: 0.6270, val_loss: 0.6313\n",
      "Epoch 18, train_loss: 0.5907, val_loss: 0.6310\n",
      "Epoch 19, train_loss: 0.5741, val_loss: 0.6287\n",
      "Epoch 20, train_loss: 0.5866, val_loss: 0.6263\n",
      "Epoch 21, train_loss: 0.5825, val_loss: 0.6233\n",
      "Epoch 22, train_loss: 0.5979, val_loss: 0.6202\n",
      "Epoch 23, train_loss: 0.5787, val_loss: 0.6171\n",
      "Epoch 24, train_loss: 0.5674, val_loss: 0.6152\n",
      "Epoch 25, train_loss: 0.5790, val_loss: 0.6140\n",
      "Epoch 26, train_loss: 0.5872, val_loss: 0.6126\n",
      "Epoch 27, train_loss: 0.5250, val_loss: 0.6116\n",
      "Epoch 28, train_loss: 0.5640, val_loss: 0.6097\n",
      "Epoch 29, train_loss: 0.5939, val_loss: 0.6086\n",
      "Epoch 30, train_loss: 0.5614, val_loss: 0.6083\n",
      "Epoch 31, train_loss: 0.5492, val_loss: 0.6069\n",
      "Epoch 32, train_loss: 0.5006, val_loss: 0.6047\n",
      "Epoch 33, train_loss: 0.5540, val_loss: 0.6016\n",
      "Epoch 34, train_loss: 0.4831, val_loss: 0.5997\n",
      "Epoch 35, train_loss: 0.5562, val_loss: 0.5983\n",
      "Epoch 36, train_loss: 0.5504, val_loss: 0.5977\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37, train_loss: 0.5043, val_loss: 0.5971\n",
      "Epoch 38, train_loss: 0.5108, val_loss: 0.5954\n",
      "Epoch 39, train_loss: 0.5284, val_loss: 0.5911\n",
      "Epoch 40, train_loss: 0.4666, val_loss: 0.5884\n",
      "Epoch 41, train_loss: 0.4672, val_loss: 0.5862\n",
      "Epoch 42, train_loss: 0.5269, val_loss: 0.5841\n",
      "Epoch 43, train_loss: 0.5240, val_loss: 0.5842\n",
      "Epoch 44, train_loss: 0.5117, val_loss: 0.5844\n",
      "Epoch 45, train_loss: 0.4534, val_loss: 0.5838\n",
      "Epoch 46, train_loss: 0.4233, val_loss: 0.5818\n",
      "Epoch 47, train_loss: 0.4908, val_loss: 0.5798\n",
      "Epoch 48, train_loss: 0.4285, val_loss: 0.5775\n",
      "Epoch 49, train_loss: 0.5086, val_loss: 0.5752\n",
      "Epoch 50, train_loss: 0.4814, val_loss: 0.5740\n",
      "Epoch 51, train_loss: 0.4393, val_loss: 0.5727\n",
      "Epoch 52, train_loss: 0.4192, val_loss: 0.5714\n",
      "Epoch 53, train_loss: 0.4974, val_loss: 0.5711\n",
      "Epoch 54, train_loss: 0.4129, val_loss: 0.5708\n",
      "Epoch 55, train_loss: 0.4216, val_loss: 0.5701\n",
      "Epoch 56, train_loss: 0.4471, val_loss: 0.5683\n",
      "Epoch 57, train_loss: 0.5132, val_loss: 0.5662\n",
      "Epoch 58, train_loss: 0.4906, val_loss: 0.5653\n",
      "Epoch 59, train_loss: 0.4228, val_loss: 0.5650\n",
      "Epoch 60, train_loss: 0.4069, val_loss: 0.5637\n",
      "Epoch 61, train_loss: 0.4397, val_loss: 0.5615\n",
      "Epoch 62, train_loss: 0.4622, val_loss: 0.5575\n",
      "Epoch 63, train_loss: 0.3949, val_loss: 0.5547\n",
      "Epoch 64, train_loss: 0.4384, val_loss: 0.5519\n",
      "Epoch 65, train_loss: 0.3773, val_loss: 0.5502\n",
      "Epoch 66, train_loss: 0.4182, val_loss: 0.5484\n",
      "Epoch 67, train_loss: 0.4064, val_loss: 0.5461\n",
      "Epoch 68, train_loss: 0.3741, val_loss: 0.5432\n",
      "Epoch 69, train_loss: 0.3821, val_loss: 0.5412\n",
      "Epoch 70, train_loss: 0.4008, val_loss: 0.5397\n",
      "Epoch 71, train_loss: 0.4208, val_loss: 0.5378\n",
      "Epoch 72, train_loss: 0.4600, val_loss: 0.5369\n",
      "Epoch 73, train_loss: 0.4139, val_loss: 0.5363\n",
      "Epoch 74, train_loss: 0.4014, val_loss: 0.5357\n",
      "Epoch 75, train_loss: 0.4392, val_loss: 0.5340\n",
      "Epoch 76, train_loss: 0.3580, val_loss: 0.5328\n",
      "Epoch 77, train_loss: 0.4472, val_loss: 0.5317\n",
      "Epoch 78, train_loss: 0.3433, val_loss: 0.5314\n",
      "Epoch 79, train_loss: 0.3568, val_loss: 0.5309\n",
      "Epoch 80, train_loss: 0.4269, val_loss: 0.5295\n",
      "Epoch 81, train_loss: 0.3490, val_loss: 0.5294\n",
      "Epoch 82, train_loss: 0.4223, val_loss: 0.5290\n",
      "Epoch 83, train_loss: 0.3681, val_loss: 0.5290\n",
      "Epoch 84, train_loss: 0.4109, val_loss: 0.5281\n",
      "Epoch 85, train_loss: 0.3426, val_loss: 0.5278\n",
      "Epoch 86, train_loss: 0.3615, val_loss: 0.5283\n",
      "Epoch 87, train_loss: 0.3483, val_loss: 0.5272\n",
      "Epoch 88, train_loss: 0.3994, val_loss: 0.5261\n",
      "Epoch 89, train_loss: 0.3897, val_loss: 0.5258\n",
      "Epoch 90, train_loss: 0.3832, val_loss: 0.5257\n",
      "Epoch 91, train_loss: 0.3418, val_loss: 0.5251\n",
      "Epoch 92, train_loss: 0.3436, val_loss: 0.5227\n",
      "Epoch 93, train_loss: 0.4078, val_loss: 0.5221\n",
      "Epoch 94, train_loss: 0.3786, val_loss: 0.5220\n",
      "Epoch 95, train_loss: 0.3878, val_loss: 0.5221\n",
      "Epoch 96, train_loss: 0.3211, val_loss: 0.5220\n",
      "Epoch 97, train_loss: 0.3259, val_loss: 0.5216\n",
      "Epoch 98, train_loss: 0.3240, val_loss: 0.5215\n",
      "Epoch 99, train_loss: 0.3690, val_loss: 0.5208\n",
      "Test the model on fold 3:\n",
      "{'roc_auc': 0.6428571428571428, 'pr_auc': 0.4522137642935962, 'fmax': 0.5833292014181566}\n",
      "Starting Fold: 4\n",
      "Epoch 0, train_loss: 0.6932, val_loss: 0.6907\n",
      "Epoch 1, train_loss: 0.6766, val_loss: 0.6901\n",
      "Epoch 2, train_loss: 0.6698, val_loss: 0.6900\n",
      "Epoch 3, train_loss: 0.6850, val_loss: 0.6908\n",
      "Epoch 4, train_loss: 0.6701, val_loss: 0.6938\n",
      "Epoch 5, train_loss: 0.6547, val_loss: 0.6948\n",
      "Epoch 6, train_loss: 0.6639, val_loss: 0.6949\n",
      "Epoch 7, train_loss: 0.6358, val_loss: 0.6948\n",
      "Epoch 8, train_loss: 0.6455, val_loss: 0.6945\n",
      "Epoch 9, train_loss: 0.6439, val_loss: 0.6944\n",
      "Epoch 10, train_loss: 0.6135, val_loss: 0.6940\n",
      "Epoch 11, train_loss: 0.6206, val_loss: 0.6929\n",
      "Epoch 12, train_loss: 0.6265, val_loss: 0.6924\n",
      "Epoch 13, train_loss: 0.6257, val_loss: 0.6933\n",
      "Epoch 14, train_loss: 0.6105, val_loss: 0.6941\n",
      "Epoch 15, train_loss: 0.6046, val_loss: 0.6940\n",
      "Epoch 16, train_loss: 0.6176, val_loss: 0.6943\n",
      "Epoch 17, train_loss: 0.6012, val_loss: 0.6947\n",
      "Epoch 18, train_loss: 0.5801, val_loss: 0.6953\n",
      "Early Stopping!\n",
      "Test the model on fold 4:\n",
      "{'roc_auc': 0.717948717948718, 'pr_auc': 0.726642678738912, 'fmax': 0.6666618667012265}\n",
      "############################################################\n",
      "# START CLASSIFICATION USING PRETRAINED MODEL FROM FOLD: 2 #\n",
      "############################################################\n",
      "Evaluate pretrained model on disease class Ophthamological (1/12)\n",
      "Starting Fold: 0\n",
      "Epoch 0, train_loss: 0.6876, val_loss: 0.6985\n",
      "Epoch 1, train_loss: 0.6779, val_loss: 0.6974\n",
      "Epoch 2, train_loss: 0.6732, val_loss: 0.6928\n",
      "Epoch 3, train_loss: 0.6711, val_loss: 0.6878\n",
      "Epoch 4, train_loss: 0.6605, val_loss: 0.6846\n",
      "Epoch 5, train_loss: 0.6583, val_loss: 0.6815\n",
      "Epoch 6, train_loss: 0.6527, val_loss: 0.6791\n",
      "Epoch 7, train_loss: 0.6449, val_loss: 0.6766\n",
      "Epoch 8, train_loss: 0.6366, val_loss: 0.6721\n",
      "Epoch 9, train_loss: 0.6342, val_loss: 0.6695\n",
      "Epoch 10, train_loss: 0.6302, val_loss: 0.6677\n",
      "Epoch 11, train_loss: 0.6202, val_loss: 0.6664\n",
      "Epoch 12, train_loss: 0.6174, val_loss: 0.6625\n",
      "Epoch 13, train_loss: 0.6145, val_loss: 0.6596\n",
      "Epoch 14, train_loss: 0.6088, val_loss: 0.6568\n",
      "Epoch 15, train_loss: 0.5945, val_loss: 0.6541\n",
      "Epoch 16, train_loss: 0.5930, val_loss: 0.6498\n",
      "Epoch 17, train_loss: 0.5810, val_loss: 0.6450\n",
      "Epoch 18, train_loss: 0.5825, val_loss: 0.6406\n",
      "Epoch 19, train_loss: 0.5642, val_loss: 0.6357\n",
      "Epoch 20, train_loss: 0.5707, val_loss: 0.6317\n",
      "Epoch 21, train_loss: 0.5603, val_loss: 0.6282\n",
      "Epoch 22, train_loss: 0.5528, val_loss: 0.6221\n",
      "Epoch 23, train_loss: 0.5361, val_loss: 0.6185\n",
      "Epoch 24, train_loss: 0.5328, val_loss: 0.6146\n",
      "Epoch 25, train_loss: 0.5309, val_loss: 0.6090\n",
      "Epoch 26, train_loss: 0.5201, val_loss: 0.6054\n",
      "Epoch 27, train_loss: 0.5114, val_loss: 0.5997\n",
      "Epoch 28, train_loss: 0.5144, val_loss: 0.5928\n",
      "Epoch 29, train_loss: 0.5016, val_loss: 0.5884\n",
      "Epoch 30, train_loss: 0.4971, val_loss: 0.5844\n",
      "Epoch 31, train_loss: 0.4785, val_loss: 0.5778\n",
      "Epoch 32, train_loss: 0.4878, val_loss: 0.5732\n",
      "Epoch 33, train_loss: 0.4767, val_loss: 0.5721\n",
      "Epoch 34, train_loss: 0.4621, val_loss: 0.5677\n",
      "Epoch 35, train_loss: 0.4529, val_loss: 0.5608\n",
      "Epoch 36, train_loss: 0.4483, val_loss: 0.5575\n",
      "Epoch 37, train_loss: 0.4303, val_loss: 0.5517\n",
      "Epoch 38, train_loss: 0.4299, val_loss: 0.5468\n",
      "Epoch 39, train_loss: 0.4111, val_loss: 0.5446\n",
      "Epoch 40, train_loss: 0.4085, val_loss: 0.5388\n",
      "Epoch 41, train_loss: 0.4042, val_loss: 0.5354\n",
      "Epoch 42, train_loss: 0.3962, val_loss: 0.5310\n",
      "Epoch 43, train_loss: 0.3984, val_loss: 0.5260\n",
      "Epoch 44, train_loss: 0.3780, val_loss: 0.5220\n",
      "Epoch 45, train_loss: 0.3746, val_loss: 0.5176\n",
      "Epoch 46, train_loss: 0.3672, val_loss: 0.5136\n",
      "Epoch 47, train_loss: 0.3522, val_loss: 0.5087\n",
      "Epoch 48, train_loss: 0.3571, val_loss: 0.5049\n",
      "Epoch 49, train_loss: 0.3447, val_loss: 0.5029\n",
      "Epoch 50, train_loss: 0.3366, val_loss: 0.5016\n",
      "Epoch 51, train_loss: 0.3281, val_loss: 0.5000\n",
      "Epoch 52, train_loss: 0.3333, val_loss: 0.4968\n",
      "Epoch 53, train_loss: 0.3153, val_loss: 0.4924\n",
      "Epoch 54, train_loss: 0.3043, val_loss: 0.4867\n",
      "Epoch 55, train_loss: 0.2996, val_loss: 0.4834\n",
      "Epoch 56, train_loss: 0.2969, val_loss: 0.4884\n",
      "Epoch 57, train_loss: 0.2921, val_loss: 0.4832\n",
      "Epoch 58, train_loss: 0.2971, val_loss: 0.4820\n",
      "Epoch 59, train_loss: 0.2733, val_loss: 0.4798\n",
      "Epoch 60, train_loss: 0.2771, val_loss: 0.4791\n",
      "Epoch 61, train_loss: 0.2646, val_loss: 0.4750\n",
      "Epoch 62, train_loss: 0.2624, val_loss: 0.4760\n",
      "Epoch 63, train_loss: 0.2559, val_loss: 0.4710\n",
      "Epoch 64, train_loss: 0.2430, val_loss: 0.4683\n",
      "Epoch 65, train_loss: 0.2533, val_loss: 0.4661\n",
      "Epoch 66, train_loss: 0.2413, val_loss: 0.4681\n",
      "Epoch 67, train_loss: 0.2376, val_loss: 0.4653\n",
      "Epoch 68, train_loss: 0.2339, val_loss: 0.4624\n",
      "Epoch 69, train_loss: 0.2299, val_loss: 0.4583\n",
      "Epoch 70, train_loss: 0.2333, val_loss: 0.4619\n",
      "Epoch 71, train_loss: 0.2187, val_loss: 0.4595\n",
      "Epoch 72, train_loss: 0.2300, val_loss: 0.4591\n",
      "Epoch 73, train_loss: 0.2137, val_loss: 0.4574\n",
      "Epoch 74, train_loss: 0.2081, val_loss: 0.4562\n",
      "Epoch 75, train_loss: 0.2022, val_loss: 0.4556\n",
      "Epoch 76, train_loss: 0.2040, val_loss: 0.4515\n",
      "Epoch 77, train_loss: 0.2051, val_loss: 0.4534\n",
      "Epoch 78, train_loss: 0.2003, val_loss: 0.4514\n",
      "Epoch 79, train_loss: 0.1957, val_loss: 0.4518\n",
      "Epoch 80, train_loss: 0.2230, val_loss: 0.4518\n",
      "Epoch 81, train_loss: 0.1991, val_loss: 0.4512\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 82, train_loss: 0.1936, val_loss: 0.4486\n",
      "Epoch 83, train_loss: 0.1936, val_loss: 0.4488\n",
      "Epoch 84, train_loss: 0.1808, val_loss: 0.4471\n",
      "Epoch 85, train_loss: 0.1937, val_loss: 0.4491\n",
      "Epoch 86, train_loss: 0.1913, val_loss: 0.4500\n",
      "Epoch 87, train_loss: 0.1834, val_loss: 0.4482\n",
      "Epoch 88, train_loss: 0.1822, val_loss: 0.4501\n",
      "Epoch 89, train_loss: 0.1744, val_loss: 0.4518\n",
      "Epoch 90, train_loss: 0.1775, val_loss: 0.4472\n",
      "Epoch 91, train_loss: 0.1685, val_loss: 0.4462\n",
      "Epoch 92, train_loss: 0.1726, val_loss: 0.4426\n",
      "Epoch 93, train_loss: 0.1757, val_loss: 0.4478\n",
      "Epoch 94, train_loss: 0.1694, val_loss: 0.4497\n",
      "Epoch 95, train_loss: 0.1734, val_loss: 0.4505\n",
      "Epoch 96, train_loss: 0.1735, val_loss: 0.4489\n",
      "Epoch 97, train_loss: 0.1632, val_loss: 0.4477\n",
      "Epoch 98, train_loss: 0.1671, val_loss: 0.4424\n",
      "Epoch 99, train_loss: 0.1697, val_loss: 0.4449\n",
      "Test the model on fold 0:\n",
      "{'roc_auc': 0.892361111111111, 'pr_auc': 0.931035258752252, 'fmax': 0.8571379592116616}\n",
      "Starting Fold: 1\n",
      "Epoch 0, train_loss: 0.6928, val_loss: 0.6858\n",
      "Epoch 1, train_loss: 0.6779, val_loss: 0.6752\n",
      "Epoch 2, train_loss: 0.6700, val_loss: 0.6676\n",
      "Epoch 3, train_loss: 0.6595, val_loss: 0.6635\n",
      "Epoch 4, train_loss: 0.6462, val_loss: 0.6605\n",
      "Epoch 5, train_loss: 0.6411, val_loss: 0.6566\n",
      "Epoch 6, train_loss: 0.6352, val_loss: 0.6522\n",
      "Epoch 7, train_loss: 0.6249, val_loss: 0.6481\n",
      "Epoch 8, train_loss: 0.6181, val_loss: 0.6452\n",
      "Epoch 9, train_loss: 0.6186, val_loss: 0.6408\n",
      "Epoch 10, train_loss: 0.6102, val_loss: 0.6378\n",
      "Epoch 11, train_loss: 0.6060, val_loss: 0.6341\n",
      "Epoch 12, train_loss: 0.5984, val_loss: 0.6307\n",
      "Epoch 13, train_loss: 0.5880, val_loss: 0.6273\n",
      "Epoch 14, train_loss: 0.5852, val_loss: 0.6251\n",
      "Epoch 15, train_loss: 0.5807, val_loss: 0.6225\n",
      "Epoch 16, train_loss: 0.5724, val_loss: 0.6197\n",
      "Epoch 17, train_loss: 0.5641, val_loss: 0.6152\n",
      "Epoch 18, train_loss: 0.5595, val_loss: 0.6133\n",
      "Epoch 19, train_loss: 0.5581, val_loss: 0.6113\n",
      "Epoch 20, train_loss: 0.5456, val_loss: 0.6057\n",
      "Epoch 21, train_loss: 0.5458, val_loss: 0.6025\n",
      "Epoch 22, train_loss: 0.5303, val_loss: 0.5993\n",
      "Epoch 23, train_loss: 0.5330, val_loss: 0.5945\n",
      "Epoch 24, train_loss: 0.5194, val_loss: 0.5910\n",
      "Epoch 25, train_loss: 0.5068, val_loss: 0.5862\n",
      "Epoch 26, train_loss: 0.5068, val_loss: 0.5809\n",
      "Epoch 27, train_loss: 0.5076, val_loss: 0.5775\n",
      "Epoch 28, train_loss: 0.4960, val_loss: 0.5740\n",
      "Epoch 29, train_loss: 0.4837, val_loss: 0.5703\n",
      "Epoch 30, train_loss: 0.4834, val_loss: 0.5679\n",
      "Epoch 31, train_loss: 0.4803, val_loss: 0.5650\n",
      "Epoch 32, train_loss: 0.4719, val_loss: 0.5615\n",
      "Epoch 33, train_loss: 0.4564, val_loss: 0.5588\n",
      "Epoch 34, train_loss: 0.4498, val_loss: 0.5577\n",
      "Epoch 35, train_loss: 0.4379, val_loss: 0.5544\n",
      "Epoch 36, train_loss: 0.4350, val_loss: 0.5497\n",
      "Epoch 37, train_loss: 0.4312, val_loss: 0.5472\n",
      "Epoch 38, train_loss: 0.4204, val_loss: 0.5443\n",
      "Epoch 39, train_loss: 0.4111, val_loss: 0.5420\n",
      "Epoch 40, train_loss: 0.4185, val_loss: 0.5376\n",
      "Epoch 41, train_loss: 0.3974, val_loss: 0.5357\n",
      "Epoch 42, train_loss: 0.3928, val_loss: 0.5345\n",
      "Epoch 43, train_loss: 0.3849, val_loss: 0.5332\n",
      "Epoch 44, train_loss: 0.3795, val_loss: 0.5317\n",
      "Epoch 45, train_loss: 0.3711, val_loss: 0.5293\n",
      "Epoch 46, train_loss: 0.3650, val_loss: 0.5244\n",
      "Epoch 47, train_loss: 0.3549, val_loss: 0.5252\n",
      "Epoch 48, train_loss: 0.3520, val_loss: 0.5234\n",
      "Epoch 49, train_loss: 0.3421, val_loss: 0.5216\n",
      "Epoch 50, train_loss: 0.3368, val_loss: 0.5201\n",
      "Epoch 51, train_loss: 0.3272, val_loss: 0.5182\n",
      "Epoch 52, train_loss: 0.3190, val_loss: 0.5156\n",
      "Epoch 53, train_loss: 0.3211, val_loss: 0.5144\n",
      "Epoch 54, train_loss: 0.3207, val_loss: 0.5139\n",
      "Epoch 55, train_loss: 0.3057, val_loss: 0.5120\n",
      "Epoch 56, train_loss: 0.2924, val_loss: 0.5111\n",
      "Epoch 57, train_loss: 0.3007, val_loss: 0.5124\n",
      "Epoch 58, train_loss: 0.2980, val_loss: 0.5096\n",
      "Epoch 59, train_loss: 0.2776, val_loss: 0.5072\n",
      "Epoch 60, train_loss: 0.2766, val_loss: 0.5097\n",
      "Epoch 61, train_loss: 0.2815, val_loss: 0.5070\n",
      "Epoch 62, train_loss: 0.2735, val_loss: 0.5070\n",
      "Epoch 63, train_loss: 0.2665, val_loss: 0.5070\n",
      "Epoch 64, train_loss: 0.2569, val_loss: 0.5065\n",
      "Epoch 65, train_loss: 0.2474, val_loss: 0.5070\n",
      "Epoch 66, train_loss: 0.2512, val_loss: 0.5069\n",
      "Epoch 67, train_loss: 0.2424, val_loss: 0.5078\n",
      "Epoch 68, train_loss: 0.2428, val_loss: 0.5044\n",
      "Epoch 69, train_loss: 0.2394, val_loss: 0.5048\n",
      "Epoch 70, train_loss: 0.2328, val_loss: 0.5033\n",
      "Epoch 71, train_loss: 0.2272, val_loss: 0.4998\n",
      "Epoch 72, train_loss: 0.2302, val_loss: 0.5027\n",
      "Epoch 73, train_loss: 0.2280, val_loss: 0.5046\n",
      "Epoch 74, train_loss: 0.2250, val_loss: 0.5065\n",
      "Epoch 75, train_loss: 0.2173, val_loss: 0.5005\n",
      "Epoch 76, train_loss: 0.2075, val_loss: 0.5036\n",
      "Epoch 77, train_loss: 0.2205, val_loss: 0.5055\n",
      "Epoch 78, train_loss: 0.2048, val_loss: 0.5027\n",
      "Epoch 79, train_loss: 0.2031, val_loss: 0.5025\n",
      "Epoch 80, train_loss: 0.1969, val_loss: 0.5025\n",
      "Epoch 81, train_loss: 0.1995, val_loss: 0.4999\n",
      "Epoch 82, train_loss: 0.1908, val_loss: 0.5005\n",
      "Epoch 83, train_loss: 0.1941, val_loss: 0.5007\n",
      "Epoch 84, train_loss: 0.1943, val_loss: 0.4989\n",
      "Epoch 85, train_loss: 0.1891, val_loss: 0.5005\n",
      "Epoch 86, train_loss: 0.1891, val_loss: 0.5010\n",
      "Epoch 87, train_loss: 0.1864, val_loss: 0.5002\n",
      "Epoch 88, train_loss: 0.1851, val_loss: 0.4994\n",
      "Epoch 89, train_loss: 0.1807, val_loss: 0.5028\n",
      "Epoch 90, train_loss: 0.1815, val_loss: 0.5034\n",
      "Epoch 91, train_loss: 0.1898, val_loss: 0.4989\n",
      "Epoch 92, train_loss: 0.1867, val_loss: 0.5039\n",
      "Epoch 93, train_loss: 0.1830, val_loss: 0.5035\n",
      "Epoch 94, train_loss: 0.1711, val_loss: 0.5031\n",
      "Epoch 95, train_loss: 0.1812, val_loss: 0.5009\n",
      "Epoch 96, train_loss: 0.1757, val_loss: 0.5029\n",
      "Epoch 97, train_loss: 0.1725, val_loss: 0.5035\n",
      "Epoch 98, train_loss: 0.1642, val_loss: 0.5059\n",
      "Early Stopping!\n",
      "Test the model on fold 1:\n",
      "{'roc_auc': 0.8744588744588745, 'pr_auc': 0.9138528568353608, 'fmax': 0.8405747112248558}\n",
      "Starting Fold: 2\n",
      "Epoch 0, train_loss: 0.6897, val_loss: 0.6979\n",
      "Epoch 1, train_loss: 0.6700, val_loss: 0.6877\n",
      "Epoch 2, train_loss: 0.6644, val_loss: 0.6814\n",
      "Epoch 3, train_loss: 0.6475, val_loss: 0.6778\n",
      "Epoch 4, train_loss: 0.6398, val_loss: 0.6782\n",
      "Epoch 5, train_loss: 0.6332, val_loss: 0.6765\n",
      "Epoch 6, train_loss: 0.6293, val_loss: 0.6738\n",
      "Epoch 7, train_loss: 0.6220, val_loss: 0.6729\n",
      "Epoch 8, train_loss: 0.6144, val_loss: 0.6692\n",
      "Epoch 9, train_loss: 0.6090, val_loss: 0.6659\n",
      "Epoch 10, train_loss: 0.5966, val_loss: 0.6632\n",
      "Epoch 11, train_loss: 0.5897, val_loss: 0.6611\n",
      "Epoch 12, train_loss: 0.5860, val_loss: 0.6585\n",
      "Epoch 13, train_loss: 0.5769, val_loss: 0.6556\n",
      "Epoch 14, train_loss: 0.5816, val_loss: 0.6525\n",
      "Epoch 15, train_loss: 0.5603, val_loss: 0.6500\n",
      "Epoch 16, train_loss: 0.5596, val_loss: 0.6481\n",
      "Epoch 17, train_loss: 0.5556, val_loss: 0.6456\n",
      "Epoch 18, train_loss: 0.5486, val_loss: 0.6441\n",
      "Epoch 19, train_loss: 0.5400, val_loss: 0.6414\n",
      "Epoch 20, train_loss: 0.5236, val_loss: 0.6380\n",
      "Epoch 21, train_loss: 0.5222, val_loss: 0.6369\n",
      "Epoch 22, train_loss: 0.5253, val_loss: 0.6329\n",
      "Epoch 23, train_loss: 0.5184, val_loss: 0.6320\n",
      "Epoch 24, train_loss: 0.5042, val_loss: 0.6299\n",
      "Epoch 25, train_loss: 0.4954, val_loss: 0.6278\n",
      "Epoch 26, train_loss: 0.4827, val_loss: 0.6250\n",
      "Epoch 27, train_loss: 0.4849, val_loss: 0.6228\n",
      "Epoch 28, train_loss: 0.4782, val_loss: 0.6202\n",
      "Epoch 29, train_loss: 0.4700, val_loss: 0.6154\n",
      "Epoch 30, train_loss: 0.4586, val_loss: 0.6131\n",
      "Epoch 31, train_loss: 0.4481, val_loss: 0.6107\n",
      "Epoch 32, train_loss: 0.4379, val_loss: 0.6090\n",
      "Epoch 33, train_loss: 0.4355, val_loss: 0.6069\n",
      "Epoch 34, train_loss: 0.4342, val_loss: 0.6040\n",
      "Epoch 35, train_loss: 0.4168, val_loss: 0.5987\n",
      "Epoch 36, train_loss: 0.4115, val_loss: 0.5974\n",
      "Epoch 37, train_loss: 0.3990, val_loss: 0.5939\n",
      "Epoch 38, train_loss: 0.3915, val_loss: 0.5904\n",
      "Epoch 39, train_loss: 0.3739, val_loss: 0.5883\n",
      "Epoch 40, train_loss: 0.3757, val_loss: 0.5856\n",
      "Epoch 41, train_loss: 0.3638, val_loss: 0.5833\n",
      "Epoch 42, train_loss: 0.3592, val_loss: 0.5792\n",
      "Epoch 43, train_loss: 0.3517, val_loss: 0.5764\n",
      "Epoch 44, train_loss: 0.3405, val_loss: 0.5747\n",
      "Epoch 45, train_loss: 0.3368, val_loss: 0.5728\n",
      "Epoch 46, train_loss: 0.3189, val_loss: 0.5696\n",
      "Epoch 47, train_loss: 0.3426, val_loss: 0.5675\n",
      "Epoch 48, train_loss: 0.3130, val_loss: 0.5631\n",
      "Epoch 49, train_loss: 0.3120, val_loss: 0.5614\n",
      "Epoch 50, train_loss: 0.2847, val_loss: 0.5618\n",
      "Epoch 51, train_loss: 0.2850, val_loss: 0.5591\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 52, train_loss: 0.2764, val_loss: 0.5593\n",
      "Epoch 53, train_loss: 0.2806, val_loss: 0.5587\n",
      "Epoch 54, train_loss: 0.2730, val_loss: 0.5572\n",
      "Epoch 55, train_loss: 0.2716, val_loss: 0.5553\n",
      "Epoch 56, train_loss: 0.2537, val_loss: 0.5530\n",
      "Epoch 57, train_loss: 0.2559, val_loss: 0.5530\n",
      "Epoch 58, train_loss: 0.2457, val_loss: 0.5518\n",
      "Epoch 59, train_loss: 0.2404, val_loss: 0.5483\n",
      "Epoch 60, train_loss: 0.2412, val_loss: 0.5485\n",
      "Epoch 61, train_loss: 0.2308, val_loss: 0.5503\n",
      "Epoch 62, train_loss: 0.2334, val_loss: 0.5483\n",
      "Epoch 63, train_loss: 0.2338, val_loss: 0.5510\n",
      "Epoch 64, train_loss: 0.2189, val_loss: 0.5478\n",
      "Epoch 65, train_loss: 0.2117, val_loss: 0.5465\n",
      "Epoch 66, train_loss: 0.2183, val_loss: 0.5460\n",
      "Epoch 67, train_loss: 0.2104, val_loss: 0.5452\n",
      "Epoch 68, train_loss: 0.2036, val_loss: 0.5439\n",
      "Epoch 69, train_loss: 0.2004, val_loss: 0.5427\n",
      "Epoch 70, train_loss: 0.2018, val_loss: 0.5439\n",
      "Epoch 71, train_loss: 0.2016, val_loss: 0.5418\n",
      "Epoch 72, train_loss: 0.1958, val_loss: 0.5423\n",
      "Epoch 73, train_loss: 0.1928, val_loss: 0.5389\n",
      "Epoch 74, train_loss: 0.2059, val_loss: 0.5391\n",
      "Epoch 75, train_loss: 0.1861, val_loss: 0.5373\n",
      "Epoch 76, train_loss: 0.1877, val_loss: 0.5355\n",
      "Epoch 77, train_loss: 0.1853, val_loss: 0.5404\n",
      "Epoch 78, train_loss: 0.1869, val_loss: 0.5343\n",
      "Epoch 79, train_loss: 0.1803, val_loss: 0.5373\n",
      "Epoch 80, train_loss: 0.1845, val_loss: 0.5364\n",
      "Epoch 81, train_loss: 0.1765, val_loss: 0.5380\n",
      "Epoch 82, train_loss: 0.1740, val_loss: 0.5359\n",
      "Epoch 83, train_loss: 0.1736, val_loss: 0.5349\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16, train_loss: 0.5826, val_loss: 0.6583\n",
      "Epoch 17, train_loss: 0.5788, val_loss: 0.6569\n",
      "Epoch 18, train_loss: 0.5744, val_loss: 0.6562\n",
      "Epoch 19, train_loss: 0.5628, val_loss: 0.6539\n",
      "Epoch 20, train_loss: 0.5597, val_loss: 0.6529\n",
      "Epoch 21, train_loss: 0.5487, val_loss: 0.6514\n",
      "Epoch 22, train_loss: 0.5515, val_loss: 0.6503\n",
      "Epoch 23, train_loss: 0.5400, val_loss: 0.6486\n",
      "Epoch 24, train_loss: 0.5403, val_loss: 0.6479\n",
      "Epoch 25, train_loss: 0.5320, val_loss: 0.6475\n",
      "Epoch 26, train_loss: 0.5189, val_loss: 0.6467\n",
      "Epoch 27, train_loss: 0.5160, val_loss: 0.6459\n",
      "Epoch 28, train_loss: 0.5123, val_loss: 0.6446\n",
      "Epoch 29, train_loss: 0.5163, val_loss: 0.6435\n",
      "Epoch 30, train_loss: 0.5004, val_loss: 0.6434\n",
      "Epoch 31, train_loss: 0.5017, val_loss: 0.6434\n",
      "Epoch 32, train_loss: 0.4945, val_loss: 0.6435\n",
      "Epoch 33, train_loss: 0.4887, val_loss: 0.6418\n",
      "Epoch 34, train_loss: 0.4812, val_loss: 0.6419\n",
      "Epoch 35, train_loss: 0.4717, val_loss: 0.6417\n",
      "Epoch 36, train_loss: 0.4680, val_loss: 0.6423\n",
      "Epoch 37, train_loss: 0.4635, val_loss: 0.6419\n",
      "Epoch 38, train_loss: 0.4653, val_loss: 0.6431\n",
      "Epoch 39, train_loss: 0.4517, val_loss: 0.6432\n",
      "Epoch 40, train_loss: 0.4460, val_loss: 0.6431\n",
      "Epoch 41, train_loss: 0.4424, val_loss: 0.6435\n",
      "Epoch 42, train_loss: 0.4350, val_loss: 0.6428\n",
      "Epoch 43, train_loss: 0.4241, val_loss: 0.6438\n",
      "Epoch 44, train_loss: 0.4237, val_loss: 0.6438\n",
      "Early Stopping!\n",
      "Test the model on fold 1:\n",
      "{'roc_auc': 0.6086956521739131, 'pr_auc': 0.6894635516321703, 'fmax': 0.6857097796214481}\n",
      "Starting Fold: 2\n",
      "Epoch 0, train_loss: 0.6929, val_loss: 0.6857\n",
      "Epoch 1, train_loss: 0.6871, val_loss: 0.6831\n",
      "Epoch 2, train_loss: 0.6728, val_loss: 0.6853\n",
      "Epoch 3, train_loss: 0.6639, val_loss: 0.6849\n",
      "Epoch 4, train_loss: 0.6606, val_loss: 0.6858\n",
      "Epoch 5, train_loss: 0.6455, val_loss: 0.6866\n",
      "Epoch 6, train_loss: 0.6441, val_loss: 0.6854\n",
      "Epoch 7, train_loss: 0.6357, val_loss: 0.6850\n",
      "Epoch 8, train_loss: 0.6389, val_loss: 0.6856\n",
      "Epoch 9, train_loss: 0.6184, val_loss: 0.6854\n",
      "Epoch 10, train_loss: 0.6226, val_loss: 0.6854\n",
      "Epoch 11, train_loss: 0.6182, val_loss: 0.6844\n",
      "Epoch 12, train_loss: 0.6112, val_loss: 0.6837\n",
      "Epoch 13, train_loss: 0.6062, val_loss: 0.6822\n",
      "Epoch 14, train_loss: 0.6030, val_loss: 0.6821\n",
      "Epoch 15, train_loss: 0.5935, val_loss: 0.6822\n",
      "Epoch 16, train_loss: 0.5885, val_loss: 0.6817\n",
      "Epoch 17, train_loss: 0.5875, val_loss: 0.6828\n",
      "Epoch 18, train_loss: 0.5833, val_loss: 0.6826\n",
      "Epoch 19, train_loss: 0.5804, val_loss: 0.6833\n",
      "Epoch 20, train_loss: 0.5760, val_loss: 0.6838\n",
      "Epoch 21, train_loss: 0.5689, val_loss: 0.6836\n",
      "Epoch 22, train_loss: 0.5685, val_loss: 0.6831\n",
      "Epoch 23, train_loss: 0.5666, val_loss: 0.6837\n",
      "Epoch 24, train_loss: 0.5641, val_loss: 0.6848\n",
      "Epoch 25, train_loss: 0.5510, val_loss: 0.6847\n",
      "Epoch 26, train_loss: 0.5433, val_loss: 0.6859\n",
      "Early Stopping!\n",
      "Test the model on fold 2:\n",
      "{'roc_auc': 0.72, 'pr_auc': 0.8046657355795958, 'fmax': 0.716413232376281}\n",
      "Starting Fold: 3\n",
      "Epoch 0, train_loss: 0.6909, val_loss: 0.6854\n",
      "Epoch 1, train_loss: 0.6897, val_loss: 0.6798\n",
      "Epoch 2, train_loss: 0.6848, val_loss: 0.6790\n",
      "Epoch 3, train_loss: 0.6644, val_loss: 0.6760\n",
      "Epoch 4, train_loss: 0.6628, val_loss: 0.6735\n",
      "Epoch 5, train_loss: 0.6586, val_loss: 0.6719\n",
      "Epoch 6, train_loss: 0.6530, val_loss: 0.6722\n",
      "Epoch 7, train_loss: 0.6436, val_loss: 0.6694\n",
      "Epoch 8, train_loss: 0.6345, val_loss: 0.6679\n",
      "Epoch 9, train_loss: 0.6330, val_loss: 0.6663\n",
      "Epoch 10, train_loss: 0.6376, val_loss: 0.6664\n",
      "Epoch 11, train_loss: 0.6293, val_loss: 0.6666\n",
      "Epoch 12, train_loss: 0.6285, val_loss: 0.6645\n",
      "Epoch 13, train_loss: 0.6192, val_loss: 0.6640\n",
      "Epoch 14, train_loss: 0.6208, val_loss: 0.6619\n",
      "Epoch 15, train_loss: 0.6162, val_loss: 0.6612\n",
      "Epoch 16, train_loss: 0.6128, val_loss: 0.6596\n",
      "Epoch 17, train_loss: 0.6061, val_loss: 0.6590\n",
      "Epoch 18, train_loss: 0.6015, val_loss: 0.6594\n",
      "Epoch 19, train_loss: 0.5957, val_loss: 0.6570\n",
      "Epoch 20, train_loss: 0.5998, val_loss: 0.6540\n",
      "Epoch 21, train_loss: 0.5901, val_loss: 0.6523\n",
      "Epoch 22, train_loss: 0.5898, val_loss: 0.6518\n",
      "Epoch 23, train_loss: 0.5862, val_loss: 0.6504\n",
      "Epoch 24, train_loss: 0.5719, val_loss: 0.6506\n",
      "Epoch 25, train_loss: 0.5716, val_loss: 0.6504\n",
      "Epoch 26, train_loss: 0.5632, val_loss: 0.6502\n",
      "Epoch 27, train_loss: 0.5617, val_loss: 0.6468\n",
      "Epoch 28, train_loss: 0.5552, val_loss: 0.6465\n",
      "Epoch 29, train_loss: 0.5519, val_loss: 0.6457\n",
      "Epoch 30, train_loss: 0.5517, val_loss: 0.6439\n",
      "Epoch 31, train_loss: 0.5468, val_loss: 0.6424\n",
      "Epoch 32, train_loss: 0.5381, val_loss: 0.6419\n",
      "Epoch 33, train_loss: 0.5365, val_loss: 0.6435\n",
      "Epoch 34, train_loss: 0.5337, val_loss: 0.6434\n",
      "Epoch 35, train_loss: 0.5253, val_loss: 0.6426\n",
      "Epoch 36, train_loss: 0.5124, val_loss: 0.6427\n",
      "Epoch 37, train_loss: 0.5109, val_loss: 0.6428\n",
      "Epoch 38, train_loss: 0.5116, val_loss: 0.6420\n",
      "Epoch 39, train_loss: 0.5086, val_loss: 0.6415\n",
      "Epoch 40, train_loss: 0.4989, val_loss: 0.6405\n",
      "Epoch 41, train_loss: 0.4912, val_loss: 0.6399\n",
      "Epoch 42, train_loss: 0.4936, val_loss: 0.6388\n",
      "Epoch 43, train_loss: 0.4830, val_loss: 0.6381\n",
      "Epoch 44, train_loss: 0.4790, val_loss: 0.6380\n",
      "Epoch 45, train_loss: 0.4748, val_loss: 0.6363\n",
      "Epoch 46, train_loss: 0.4640, val_loss: 0.6344\n",
      "Epoch 47, train_loss: 0.4606, val_loss: 0.6356\n",
      "Epoch 48, train_loss: 0.4509, val_loss: 0.6374\n",
      "Epoch 49, train_loss: 0.4519, val_loss: 0.6377\n",
      "Epoch 50, train_loss: 0.4323, val_loss: 0.6368\n",
      "Epoch 51, train_loss: 0.4340, val_loss: 0.6356\n",
      "Epoch 52, train_loss: 0.4225, val_loss: 0.6338\n",
      "Epoch 53, train_loss: 0.4154, val_loss: 0.6339\n",
      "Epoch 54, train_loss: 0.4145, val_loss: 0.6348\n",
      "Epoch 55, train_loss: 0.4118, val_loss: 0.6324\n",
      "Epoch 56, train_loss: 0.4095, val_loss: 0.6334\n",
      "Epoch 57, train_loss: 0.4005, val_loss: 0.6332\n",
      "Epoch 58, train_loss: 0.3921, val_loss: 0.6320\n",
      "Epoch 59, train_loss: 0.3901, val_loss: 0.6328\n",
      "Epoch 60, train_loss: 0.3800, val_loss: 0.6334\n",
      "Epoch 61, train_loss: 0.3709, val_loss: 0.6334\n",
      "Epoch 62, train_loss: 0.3627, val_loss: 0.6328\n",
      "Epoch 63, train_loss: 0.3631, val_loss: 0.6329\n",
      "Epoch 64, train_loss: 0.3585, val_loss: 0.6341\n",
      "Epoch 65, train_loss: 0.3491, val_loss: 0.6323\n",
      "Epoch 66, train_loss: 0.3445, val_loss: 0.6299\n",
      "Epoch 67, train_loss: 0.3362, val_loss: 0.6319\n",
      "Epoch 68, train_loss: 0.3287, val_loss: 0.6315\n",
      "Epoch 69, train_loss: 0.3329, val_loss: 0.6333\n",
      "Epoch 70, train_loss: 0.3233, val_loss: 0.6336\n",
      "Epoch 71, train_loss: 0.3189, val_loss: 0.6336\n",
      "Epoch 72, train_loss: 0.3113, val_loss: 0.6328\n",
      "Epoch 73, train_loss: 0.3101, val_loss: 0.6349\n",
      "Early Stopping!\n",
      "Test the model on fold 3:\n",
      "{'roc_auc': 0.8581818181818182, 'pr_auc': 0.9060263748089623, 'fmax': 0.8163215327253099}\n",
      "Starting Fold: 4\n",
      "Epoch 0, train_loss: 0.6965, val_loss: 0.6916\n",
      "Epoch 1, train_loss: 0.6836, val_loss: 0.6857\n",
      "Epoch 2, train_loss: 0.6787, val_loss: 0.6810\n",
      "Epoch 3, train_loss: 0.6651, val_loss: 0.6775\n",
      "Epoch 4, train_loss: 0.6586, val_loss: 0.6752\n",
      "Epoch 5, train_loss: 0.6468, val_loss: 0.6724\n",
      "Epoch 6, train_loss: 0.6441, val_loss: 0.6702\n",
      "Epoch 7, train_loss: 0.6443, val_loss: 0.6692\n",
      "Epoch 8, train_loss: 0.6358, val_loss: 0.6669\n",
      "Epoch 9, train_loss: 0.6295, val_loss: 0.6641\n",
      "Epoch 10, train_loss: 0.6218, val_loss: 0.6611\n",
      "Epoch 11, train_loss: 0.6198, val_loss: 0.6599\n",
      "Epoch 12, train_loss: 0.6152, val_loss: 0.6583\n",
      "Epoch 13, train_loss: 0.6077, val_loss: 0.6572\n",
      "Epoch 14, train_loss: 0.6056, val_loss: 0.6554\n",
      "Epoch 15, train_loss: 0.6058, val_loss: 0.6552\n",
      "Epoch 16, train_loss: 0.5953, val_loss: 0.6546\n",
      "Epoch 17, train_loss: 0.5894, val_loss: 0.6533\n",
      "Epoch 18, train_loss: 0.5902, val_loss: 0.6524\n",
      "Epoch 19, train_loss: 0.5872, val_loss: 0.6505\n",
      "Epoch 20, train_loss: 0.5852, val_loss: 0.6492\n",
      "Epoch 21, train_loss: 0.5810, val_loss: 0.6480\n",
      "Epoch 22, train_loss: 0.5767, val_loss: 0.6468\n",
      "Epoch 23, train_loss: 0.5612, val_loss: 0.6459\n",
      "Epoch 24, train_loss: 0.5655, val_loss: 0.6447\n",
      "Epoch 25, train_loss: 0.5537, val_loss: 0.6437\n",
      "Epoch 26, train_loss: 0.5614, val_loss: 0.6431\n",
      "Epoch 27, train_loss: 0.5560, val_loss: 0.6422\n",
      "Epoch 28, train_loss: 0.5459, val_loss: 0.6424\n",
      "Epoch 29, train_loss: 0.5374, val_loss: 0.6417\n",
      "Epoch 30, train_loss: 0.5305, val_loss: 0.6412\n",
      "Epoch 31, train_loss: 0.5286, val_loss: 0.6403\n",
      "Epoch 32, train_loss: 0.5273, val_loss: 0.6396\n",
      "Epoch 33, train_loss: 0.5199, val_loss: 0.6384\n",
      "Epoch 34, train_loss: 0.5167, val_loss: 0.6378\n",
      "Epoch 35, train_loss: 0.5099, val_loss: 0.6376\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36, train_loss: 0.5072, val_loss: 0.6376\n",
      "Epoch 37, train_loss: 0.5004, val_loss: 0.6372\n",
      "Epoch 38, train_loss: 0.4963, val_loss: 0.6362\n",
      "Epoch 39, train_loss: 0.4779, val_loss: 0.6361\n",
      "Epoch 40, train_loss: 0.4784, val_loss: 0.6352\n",
      "Epoch 41, train_loss: 0.4761, val_loss: 0.6351\n",
      "Epoch 42, train_loss: 0.4763, val_loss: 0.6351\n",
      "Epoch 43, train_loss: 0.4679, val_loss: 0.6351\n",
      "Epoch 44, train_loss: 0.4600, val_loss: 0.6350\n",
      "Epoch 45, train_loss: 0.4593, val_loss: 0.6344\n",
      "Epoch 46, train_loss: 0.4447, val_loss: 0.6358\n",
      "Epoch 47, train_loss: 0.4475, val_loss: 0.6349\n",
      "Epoch 48, train_loss: 0.4416, val_loss: 0.6359\n",
      "Epoch 49, train_loss: 0.4349, val_loss: 0.6357\n",
      "Epoch 50, train_loss: 0.4252, val_loss: 0.6356\n",
      "Epoch 51, train_loss: 0.4193, val_loss: 0.6359\n",
      "Epoch 52, train_loss: 0.4110, val_loss: 0.6362\n",
      "Epoch 53, train_loss: 0.4072, val_loss: 0.6364\n",
      "Early Stopping!\n",
      "Test the model on fold 4:\n",
      "{'roc_auc': 0.7121212121212123, 'pr_auc': 0.7406395166946653, 'fmax': 0.6785666581968015}\n",
      "Evaluate pretrained model on disease class Dermatological (7/12)\n",
      "Starting Fold: 0\n",
      "Epoch 0, train_loss: 0.6866, val_loss: 0.6895\n",
      "Epoch 1, train_loss: 0.6857, val_loss: 0.6821\n",
      "Epoch 2, train_loss: 0.6681, val_loss: 0.6753\n",
      "Epoch 3, train_loss: 0.6652, val_loss: 0.6688\n",
      "Epoch 4, train_loss: 0.6496, val_loss: 0.6624\n",
      "Epoch 5, train_loss: 0.6557, val_loss: 0.6580\n",
      "Epoch 6, train_loss: 0.6411, val_loss: 0.6520\n",
      "Epoch 7, train_loss: 0.6315, val_loss: 0.6468\n",
      "Epoch 8, train_loss: 0.6344, val_loss: 0.6404\n",
      "Epoch 9, train_loss: 0.6237, val_loss: 0.6358\n",
      "Epoch 10, train_loss: 0.6036, val_loss: 0.6313\n",
      "Epoch 11, train_loss: 0.6172, val_loss: 0.6261\n",
      "Epoch 12, train_loss: 0.5945, val_loss: 0.6217\n",
      "Epoch 13, train_loss: 0.6093, val_loss: 0.6175\n",
      "Epoch 14, train_loss: 0.5941, val_loss: 0.6137\n",
      "Epoch 15, train_loss: 0.5924, val_loss: 0.6087\n",
      "Epoch 16, train_loss: 0.5821, val_loss: 0.6039\n",
      "Epoch 17, train_loss: 0.5682, val_loss: 0.5990\n",
      "Epoch 18, train_loss: 0.5748, val_loss: 0.5947\n",
      "Epoch 19, train_loss: 0.5559, val_loss: 0.5906\n",
      "Epoch 20, train_loss: 0.5525, val_loss: 0.5850\n",
      "Epoch 21, train_loss: 0.5559, val_loss: 0.5800\n",
      "Epoch 22, train_loss: 0.5424, val_loss: 0.5745\n",
      "Epoch 23, train_loss: 0.5095, val_loss: 0.5702\n",
      "Epoch 24, train_loss: 0.5341, val_loss: 0.5639\n",
      "Epoch 25, train_loss: 0.5259, val_loss: 0.5587\n",
      "Epoch 26, train_loss: 0.5086, val_loss: 0.5542\n",
      "Epoch 27, train_loss: 0.4823, val_loss: 0.5484\n",
      "Epoch 28, train_loss: 0.4894, val_loss: 0.5428\n",
      "Epoch 29, train_loss: 0.4772, val_loss: 0.5369\n",
      "Epoch 30, train_loss: 0.4654, val_loss: 0.5332\n",
      "Epoch 31, train_loss: 0.4730, val_loss: 0.5281\n",
      "Epoch 32, train_loss: 0.4385, val_loss: 0.5234\n",
      "Epoch 33, train_loss: 0.4396, val_loss: 0.5181\n",
      "Epoch 34, train_loss: 0.4202, val_loss: 0.5130\n",
      "Epoch 35, train_loss: 0.4609, val_loss: 0.5073\n",
      "Epoch 36, train_loss: 0.4347, val_loss: 0.5035\n",
      "Epoch 37, train_loss: 0.4041, val_loss: 0.4994\n",
      "Epoch 38, train_loss: 0.4038, val_loss: 0.4954\n",
      "Epoch 39, train_loss: 0.4120, val_loss: 0.4913\n",
      "Epoch 40, train_loss: 0.3885, val_loss: 0.4875\n",
      "Epoch 41, train_loss: 0.4043, val_loss: 0.4837\n",
      "Epoch 42, train_loss: 0.3894, val_loss: 0.4811\n",
      "Epoch 43, train_loss: 0.3712, val_loss: 0.4776\n",
      "Epoch 44, train_loss: 0.3771, val_loss: 0.4740\n",
      "Epoch 45, train_loss: 0.3563, val_loss: 0.4700\n",
      "Epoch 46, train_loss: 0.3475, val_loss: 0.4659\n",
      "Epoch 47, train_loss: 0.3524, val_loss: 0.4625\n",
      "Epoch 48, train_loss: 0.3605, val_loss: 0.4599\n",
      "Epoch 49, train_loss: 0.3418, val_loss: 0.4573\n",
      "Epoch 50, train_loss: 0.3031, val_loss: 0.4551\n",
      "Epoch 51, train_loss: 0.2940, val_loss: 0.4515\n",
      "Epoch 52, train_loss: 0.3262, val_loss: 0.4491\n",
      "Epoch 53, train_loss: 0.3164, val_loss: 0.4472\n",
      "Epoch 54, train_loss: 0.2789, val_loss: 0.4456\n",
      "Epoch 55, train_loss: 0.2883, val_loss: 0.4436\n",
      "Epoch 56, train_loss: 0.2667, val_loss: 0.4417\n",
      "Epoch 57, train_loss: 0.2741, val_loss: 0.4385\n",
      "Epoch 58, train_loss: 0.2866, val_loss: 0.4352\n",
      "Epoch 59, train_loss: 0.2586, val_loss: 0.4323\n",
      "Epoch 60, train_loss: 0.2514, val_loss: 0.4295\n",
      "Epoch 61, train_loss: 0.2605, val_loss: 0.4272\n",
      "Epoch 62, train_loss: 0.2749, val_loss: 0.4265\n",
      "Epoch 63, train_loss: 0.2569, val_loss: 0.4253\n",
      "Epoch 64, train_loss: 0.2512, val_loss: 0.4230\n",
      "Epoch 65, train_loss: 0.2501, val_loss: 0.4231\n",
      "Epoch 66, train_loss: 0.2443, val_loss: 0.4240\n",
      "Epoch 67, train_loss: 0.2422, val_loss: 0.4217\n",
      "Epoch 68, train_loss: 0.2251, val_loss: 0.4211\n",
      "Epoch 69, train_loss: 0.2093, val_loss: 0.4205\n",
      "Epoch 70, train_loss: 0.2169, val_loss: 0.4196\n",
      "Epoch 71, train_loss: 0.2329, val_loss: 0.4174\n",
      "Epoch 72, train_loss: 0.2101, val_loss: 0.4162\n",
      "Epoch 73, train_loss: 0.2174, val_loss: 0.4137\n",
      "Epoch 74, train_loss: 0.1999, val_loss: 0.4131\n",
      "Epoch 75, train_loss: 0.2024, val_loss: 0.4123\n",
      "Epoch 76, train_loss: 0.2126, val_loss: 0.4106\n",
      "Epoch 77, train_loss: 0.1975, val_loss: 0.4092\n",
      "Epoch 78, train_loss: 0.1794, val_loss: 0.4078\n",
      "Epoch 79, train_loss: 0.1942, val_loss: 0.4072\n",
      "Epoch 80, train_loss: 0.2206, val_loss: 0.4069\n",
      "Epoch 81, train_loss: 0.1816, val_loss: 0.4087\n",
      "Epoch 82, train_loss: 0.1895, val_loss: 0.4075\n",
      "Epoch 83, train_loss: 0.2090, val_loss: 0.4065\n",
      "Epoch 84, train_loss: 0.1897, val_loss: 0.4062\n",
      "Epoch 85, train_loss: 0.1988, val_loss: 0.4045\n",
      "Epoch 86, train_loss: 0.1830, val_loss: 0.4051\n",
      "Epoch 87, train_loss: 0.1754, val_loss: 0.4049\n",
      "Epoch 88, train_loss: 0.1835, val_loss: 0.4037\n",
      "Epoch 89, train_loss: 0.1980, val_loss: 0.4038\n",
      "Epoch 90, train_loss: 0.1971, val_loss: 0.4018\n",
      "Epoch 91, train_loss: 0.1551, val_loss: 0.4000\n",
      "Epoch 92, train_loss: 0.1570, val_loss: 0.3987\n",
      "Epoch 93, train_loss: 0.1821, val_loss: 0.3985\n",
      "Epoch 94, train_loss: 0.1582, val_loss: 0.3983\n",
      "Epoch 95, train_loss: 0.1635, val_loss: 0.3988\n",
      "Epoch 96, train_loss: 0.1590, val_loss: 0.3989\n",
      "Epoch 97, train_loss: 0.1554, val_loss: 0.3981\n",
      "Epoch 98, train_loss: 0.1455, val_loss: 0.3979\n",
      "Epoch 99, train_loss: 0.1556, val_loss: 0.3987\n",
      "Test the model on fold 0:\n",
      "{'roc_auc': 0.8301435406698564, 'pr_auc': 0.8689115142002667, 'fmax': 0.8499950500288262}\n",
      "Starting Fold: 1\n",
      "Epoch 0, train_loss: 0.6885, val_loss: 0.6892\n",
      "Epoch 1, train_loss: 0.6782, val_loss: 0.6826\n",
      "Epoch 2, train_loss: 0.6654, val_loss: 0.6781\n",
      "Epoch 3, train_loss: 0.6602, val_loss: 0.6745\n",
      "Epoch 4, train_loss: 0.6562, val_loss: 0.6708\n",
      "Epoch 5, train_loss: 0.6478, val_loss: 0.6671\n",
      "Epoch 6, train_loss: 0.6398, val_loss: 0.6647\n",
      "Epoch 7, train_loss: 0.6272, val_loss: 0.6622\n",
      "Epoch 8, train_loss: 0.6106, val_loss: 0.6597\n",
      "Epoch 9, train_loss: 0.6179, val_loss: 0.6557\n",
      "Epoch 10, train_loss: 0.6102, val_loss: 0.6538\n",
      "Epoch 11, train_loss: 0.6095, val_loss: 0.6515\n",
      "Epoch 12, train_loss: 0.6044, val_loss: 0.6493\n",
      "Epoch 13, train_loss: 0.5916, val_loss: 0.6470\n",
      "Epoch 14, train_loss: 0.5935, val_loss: 0.6465\n",
      "Epoch 15, train_loss: 0.6006, val_loss: 0.6448\n",
      "Epoch 16, train_loss: 0.5819, val_loss: 0.6417\n",
      "Epoch 17, train_loss: 0.5761, val_loss: 0.6380\n",
      "Epoch 18, train_loss: 0.5794, val_loss: 0.6352\n",
      "Epoch 19, train_loss: 0.5766, val_loss: 0.6313\n",
      "Epoch 20, train_loss: 0.5459, val_loss: 0.6291\n",
      "Epoch 21, train_loss: 0.5679, val_loss: 0.6263\n",
      "Epoch 22, train_loss: 0.5564, val_loss: 0.6234\n",
      "Epoch 23, train_loss: 0.5552, val_loss: 0.6208\n",
      "Epoch 24, train_loss: 0.5412, val_loss: 0.6178\n",
      "Epoch 25, train_loss: 0.5401, val_loss: 0.6155\n",
      "Epoch 26, train_loss: 0.5338, val_loss: 0.6121\n",
      "Epoch 27, train_loss: 0.5167, val_loss: 0.6102\n",
      "Epoch 28, train_loss: 0.5295, val_loss: 0.6080\n",
      "Epoch 29, train_loss: 0.5188, val_loss: 0.6047\n",
      "Epoch 30, train_loss: 0.4981, val_loss: 0.6010\n",
      "Epoch 31, train_loss: 0.4952, val_loss: 0.5972\n",
      "Epoch 32, train_loss: 0.5086, val_loss: 0.5935\n",
      "Epoch 33, train_loss: 0.5142, val_loss: 0.5910\n",
      "Epoch 34, train_loss: 0.4710, val_loss: 0.5884\n",
      "Epoch 35, train_loss: 0.4880, val_loss: 0.5842\n",
      "Epoch 36, train_loss: 0.4714, val_loss: 0.5804\n",
      "Epoch 37, train_loss: 0.4681, val_loss: 0.5775\n",
      "Epoch 38, train_loss: 0.4817, val_loss: 0.5741\n",
      "Epoch 39, train_loss: 0.4583, val_loss: 0.5720\n",
      "Epoch 40, train_loss: 0.4404, val_loss: 0.5681\n",
      "Epoch 41, train_loss: 0.4579, val_loss: 0.5667\n",
      "Epoch 42, train_loss: 0.4425, val_loss: 0.5636\n",
      "Epoch 43, train_loss: 0.4106, val_loss: 0.5600\n",
      "Epoch 44, train_loss: 0.4051, val_loss: 0.5566\n",
      "Epoch 45, train_loss: 0.4392, val_loss: 0.5531\n",
      "Epoch 46, train_loss: 0.4282, val_loss: 0.5502\n",
      "Epoch 47, train_loss: 0.3914, val_loss: 0.5461\n",
      "Epoch 48, train_loss: 0.4126, val_loss: 0.5440\n",
      "Epoch 49, train_loss: 0.4117, val_loss: 0.5419\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50, train_loss: 0.3884, val_loss: 0.5392\n",
      "Epoch 51, train_loss: 0.3680, val_loss: 0.5357\n",
      "Epoch 52, train_loss: 0.3690, val_loss: 0.5329\n",
      "Epoch 53, train_loss: 0.3657, val_loss: 0.5312\n",
      "Epoch 54, train_loss: 0.3663, val_loss: 0.5297\n",
      "Epoch 55, train_loss: 0.3784, val_loss: 0.5277\n",
      "Epoch 56, train_loss: 0.3611, val_loss: 0.5256\n",
      "Epoch 57, train_loss: 0.3567, val_loss: 0.5231\n",
      "Epoch 58, train_loss: 0.3496, val_loss: 0.5210\n",
      "Epoch 59, train_loss: 0.3174, val_loss: 0.5200\n",
      "Epoch 60, train_loss: 0.3393, val_loss: 0.5180\n",
      "Epoch 61, train_loss: 0.3594, val_loss: 0.5164\n",
      "Epoch 62, train_loss: 0.3230, val_loss: 0.5141\n",
      "Epoch 63, train_loss: 0.3176, val_loss: 0.5120\n",
      "Epoch 64, train_loss: 0.3233, val_loss: 0.5106\n",
      "Epoch 65, train_loss: 0.3315, val_loss: 0.5099\n",
      "Epoch 66, train_loss: 0.3021, val_loss: 0.5074\n",
      "Epoch 67, train_loss: 0.2866, val_loss: 0.5038\n",
      "Epoch 68, train_loss: 0.3139, val_loss: 0.5006\n",
      "Epoch 69, train_loss: 0.3058, val_loss: 0.4988\n",
      "Epoch 70, train_loss: 0.3037, val_loss: 0.4985\n",
      "Epoch 71, train_loss: 0.2681, val_loss: 0.4960\n",
      "Epoch 72, train_loss: 0.2974, val_loss: 0.4965\n",
      "Epoch 73, train_loss: 0.2831, val_loss: 0.4949\n",
      "Epoch 74, train_loss: 0.2951, val_loss: 0.4932\n",
      "Epoch 75, train_loss: 0.2772, val_loss: 0.4929\n",
      "Epoch 76, train_loss: 0.2605, val_loss: 0.4910\n",
      "Epoch 77, train_loss: 0.2483, val_loss: 0.4887\n",
      "Epoch 78, train_loss: 0.2513, val_loss: 0.4870\n",
      "Epoch 79, train_loss: 0.2795, val_loss: 0.4854\n",
      "Epoch 80, train_loss: 0.2738, val_loss: 0.4840\n",
      "Epoch 81, train_loss: 0.2476, val_loss: 0.4836\n",
      "Epoch 82, train_loss: 0.2328, val_loss: 0.4815\n",
      "Epoch 83, train_loss: 0.2453, val_loss: 0.4800\n",
      "Epoch 84, train_loss: 0.2495, val_loss: 0.4801\n",
      "Epoch 85, train_loss: 0.2504, val_loss: 0.4812\n",
      "Epoch 86, train_loss: 0.2192, val_loss: 0.4794\n",
      "Epoch 87, train_loss: 0.2229, val_loss: 0.4788\n",
      "Epoch 88, train_loss: 0.2261, val_loss: 0.4771\n",
      "Epoch 89, train_loss: 0.2102, val_loss: 0.4760\n",
      "Epoch 90, train_loss: 0.2229, val_loss: 0.4741\n",
      "Epoch 91, train_loss: 0.2152, val_loss: 0.4737\n",
      "Epoch 92, train_loss: 0.2028, val_loss: 0.4732\n",
      "Epoch 93, train_loss: 0.2276, val_loss: 0.4727\n",
      "Epoch 94, train_loss: 0.2114, val_loss: 0.4717\n",
      "Epoch 95, train_loss: 0.2107, val_loss: 0.4701\n",
      "Epoch 96, train_loss: 0.1979, val_loss: 0.4692\n",
      "Epoch 97, train_loss: 0.2104, val_loss: 0.4689\n",
      "Epoch 98, train_loss: 0.1951, val_loss: 0.4692\n",
      "Epoch 99, train_loss: 0.2143, val_loss: 0.4687\n",
      "Test the model on fold 1:\n",
      "{'roc_auc': 0.8976190476190475, 'pr_auc': 0.9383273717676799, 'fmax': 0.8837159329649813}\n",
      "Starting Fold: 2\n",
      "Epoch 0, train_loss: 0.7002, val_loss: 0.6865\n",
      "Epoch 1, train_loss: 0.6846, val_loss: 0.6729\n",
      "Epoch 2, train_loss: 0.6695, val_loss: 0.6644\n",
      "Epoch 3, train_loss: 0.6582, val_loss: 0.6593\n",
      "Epoch 4, train_loss: 0.6325, val_loss: 0.6521\n",
      "Epoch 5, train_loss: 0.6542, val_loss: 0.6461\n",
      "Epoch 6, train_loss: 0.6434, val_loss: 0.6412\n",
      "Epoch 7, train_loss: 0.6281, val_loss: 0.6377\n",
      "Epoch 8, train_loss: 0.6313, val_loss: 0.6340\n",
      "Epoch 9, train_loss: 0.6177, val_loss: 0.6300\n",
      "Epoch 10, train_loss: 0.6080, val_loss: 0.6249\n",
      "Epoch 11, train_loss: 0.5967, val_loss: 0.6177\n",
      "Epoch 12, train_loss: 0.5940, val_loss: 0.6135\n",
      "Epoch 13, train_loss: 0.5863, val_loss: 0.6100\n",
      "Epoch 14, train_loss: 0.5931, val_loss: 0.6050\n",
      "Epoch 15, train_loss: 0.5803, val_loss: 0.6003\n",
      "Epoch 16, train_loss: 0.5797, val_loss: 0.5954\n",
      "Epoch 17, train_loss: 0.5548, val_loss: 0.5907\n",
      "Epoch 18, train_loss: 0.5524, val_loss: 0.5849\n",
      "Epoch 19, train_loss: 0.5580, val_loss: 0.5806\n",
      "Epoch 20, train_loss: 0.5515, val_loss: 0.5770\n",
      "Epoch 21, train_loss: 0.5475, val_loss: 0.5739\n",
      "Epoch 22, train_loss: 0.5360, val_loss: 0.5697\n",
      "Epoch 23, train_loss: 0.5108, val_loss: 0.5665\n",
      "Epoch 24, train_loss: 0.5270, val_loss: 0.5617\n",
      "Epoch 25, train_loss: 0.5082, val_loss: 0.5570\n",
      "Epoch 26, train_loss: 0.5293, val_loss: 0.5514\n",
      "Epoch 27, train_loss: 0.5026, val_loss: 0.5478\n",
      "Epoch 28, train_loss: 0.5133, val_loss: 0.5437\n",
      "Epoch 29, train_loss: 0.4948, val_loss: 0.5407\n",
      "Epoch 30, train_loss: 0.4702, val_loss: 0.5378\n",
      "Epoch 31, train_loss: 0.4899, val_loss: 0.5340\n",
      "Epoch 32, train_loss: 0.4567, val_loss: 0.5308\n",
      "Epoch 33, train_loss: 0.4537, val_loss: 0.5254\n",
      "Epoch 34, train_loss: 0.4509, val_loss: 0.5217\n",
      "Epoch 35, train_loss: 0.4531, val_loss: 0.5182\n",
      "Epoch 36, train_loss: 0.4433, val_loss: 0.5142\n",
      "Epoch 37, train_loss: 0.4332, val_loss: 0.5113\n",
      "Epoch 38, train_loss: 0.4425, val_loss: 0.5087\n",
      "Epoch 39, train_loss: 0.4392, val_loss: 0.5060\n",
      "Epoch 40, train_loss: 0.4397, val_loss: 0.5039\n",
      "Epoch 41, train_loss: 0.4411, val_loss: 0.5008\n",
      "Epoch 42, train_loss: 0.4127, val_loss: 0.4993\n",
      "Epoch 43, train_loss: 0.4108, val_loss: 0.4964\n",
      "Epoch 44, train_loss: 0.3844, val_loss: 0.4937\n",
      "Epoch 45, train_loss: 0.3884, val_loss: 0.4910\n",
      "Epoch 46, train_loss: 0.3860, val_loss: 0.4869\n",
      "Epoch 47, train_loss: 0.3733, val_loss: 0.4839\n",
      "Epoch 48, train_loss: 0.3723, val_loss: 0.4819\n",
      "Epoch 49, train_loss: 0.3616, val_loss: 0.4793\n",
      "Epoch 50, train_loss: 0.3494, val_loss: 0.4771\n",
      "Epoch 51, train_loss: 0.3499, val_loss: 0.4738\n",
      "Epoch 52, train_loss: 0.3328, val_loss: 0.4709\n",
      "Epoch 53, train_loss: 0.3269, val_loss: 0.4686\n",
      "Epoch 54, train_loss: 0.3231, val_loss: 0.4683\n",
      "Epoch 55, train_loss: 0.3013, val_loss: 0.4680\n",
      "Epoch 56, train_loss: 0.2990, val_loss: 0.4684\n",
      "Epoch 57, train_loss: 0.3336, val_loss: 0.4671\n",
      "Epoch 58, train_loss: 0.3084, val_loss: 0.4649\n",
      "Epoch 59, train_loss: 0.2929, val_loss: 0.4611\n",
      "Epoch 60, train_loss: 0.3058, val_loss: 0.4593\n",
      "Epoch 61, train_loss: 0.2992, val_loss: 0.4570\n",
      "Epoch 62, train_loss: 0.2673, val_loss: 0.4574\n",
      "Epoch 63, train_loss: 0.2692, val_loss: 0.4603\n",
      "Epoch 64, train_loss: 0.3032, val_loss: 0.4601\n",
      "Epoch 65, train_loss: 0.2584, val_loss: 0.4585\n",
      "Epoch 66, train_loss: 0.2514, val_loss: 0.4556\n",
      "Epoch 67, train_loss: 0.2448, val_loss: 0.4543\n",
      "Epoch 68, train_loss: 0.2499, val_loss: 0.4541\n",
      "Epoch 69, train_loss: 0.2672, val_loss: 0.4526\n",
      "Epoch 70, train_loss: 0.2501, val_loss: 0.4502\n",
      "Epoch 71, train_loss: 0.2420, val_loss: 0.4473\n",
      "Epoch 72, train_loss: 0.2385, val_loss: 0.4456\n",
      "Epoch 73, train_loss: 0.2472, val_loss: 0.4436\n",
      "Epoch 74, train_loss: 0.2276, val_loss: 0.4437\n",
      "Epoch 75, train_loss: 0.2422, val_loss: 0.4432\n",
      "Epoch 76, train_loss: 0.2235, val_loss: 0.4425\n",
      "Epoch 77, train_loss: 0.2167, val_loss: 0.4410\n",
      "Epoch 78, train_loss: 0.2100, val_loss: 0.4403\n",
      "Epoch 79, train_loss: 0.2123, val_loss: 0.4368\n",
      "Epoch 80, train_loss: 0.1989, val_loss: 0.4354\n",
      "Epoch 81, train_loss: 0.2202, val_loss: 0.4353\n",
      "Epoch 82, train_loss: 0.1912, val_loss: 0.4361\n",
      "Epoch 83, train_loss: 0.1942, val_loss: 0.4355\n",
      "Epoch 84, train_loss: 0.1917, val_loss: 0.4345\n",
      "Epoch 85, train_loss: 0.2008, val_loss: 0.4336\n",
      "Epoch 86, train_loss: 0.2030, val_loss: 0.4340\n",
      "Epoch 87, train_loss: 0.1903, val_loss: 0.4352\n",
      "Epoch 88, train_loss: 0.1786, val_loss: 0.4371\n",
      "Epoch 89, train_loss: 0.1888, val_loss: 0.4373\n",
      "Epoch 90, train_loss: 0.1975, val_loss: 0.4398\n",
      "Epoch 91, train_loss: 0.1732, val_loss: 0.4394\n",
      "Epoch 92, train_loss: 0.1712, val_loss: 0.4381\n",
      "Epoch 93, train_loss: 0.1719, val_loss: 0.4355\n",
      "Epoch 94, train_loss: 0.1783, val_loss: 0.4360\n",
      "Epoch 95, train_loss: 0.1799, val_loss: 0.4362\n",
      "Epoch 96, train_loss: 0.1651, val_loss: 0.4357\n",
      "Epoch 97, train_loss: 0.1959, val_loss: 0.4375\n",
      "Epoch 98, train_loss: 0.1621, val_loss: 0.4402\n",
      "Early Stopping!\n",
      "Test the model on fold 2:\n",
      "{'roc_auc': 0.8904761904761904, 'pr_auc': 0.9158912248944653, 'fmax': 0.8571379592116616}\n",
      "Starting Fold: 3\n",
      "Epoch 0, train_loss: 0.6955, val_loss: 0.6892\n",
      "Epoch 1, train_loss: 0.6785, val_loss: 0.6811\n",
      "Epoch 2, train_loss: 0.6662, val_loss: 0.6762\n",
      "Epoch 3, train_loss: 0.6532, val_loss: 0.6692\n",
      "Epoch 4, train_loss: 0.6484, val_loss: 0.6662\n",
      "Epoch 5, train_loss: 0.6501, val_loss: 0.6629\n",
      "Epoch 6, train_loss: 0.6314, val_loss: 0.6603\n",
      "Epoch 7, train_loss: 0.6290, val_loss: 0.6558\n",
      "Epoch 8, train_loss: 0.6241, val_loss: 0.6531\n",
      "Epoch 9, train_loss: 0.6176, val_loss: 0.6489\n",
      "Epoch 10, train_loss: 0.6165, val_loss: 0.6454\n",
      "Epoch 11, train_loss: 0.6056, val_loss: 0.6424\n",
      "Epoch 12, train_loss: 0.5984, val_loss: 0.6396\n",
      "Epoch 13, train_loss: 0.5892, val_loss: 0.6370\n",
      "Epoch 14, train_loss: 0.5965, val_loss: 0.6334\n",
      "Epoch 15, train_loss: 0.5922, val_loss: 0.6294\n",
      "Epoch 16, train_loss: 0.5823, val_loss: 0.6254\n",
      "Epoch 17, train_loss: 0.5722, val_loss: 0.6226\n",
      "Epoch 18, train_loss: 0.5667, val_loss: 0.6189\n",
      "Epoch 19, train_loss: 0.5651, val_loss: 0.6152\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20, train_loss: 0.5540, val_loss: 0.6110\n",
      "Epoch 21, train_loss: 0.5551, val_loss: 0.6068\n",
      "Epoch 22, train_loss: 0.5482, val_loss: 0.6037\n",
      "Epoch 23, train_loss: 0.5433, val_loss: 0.5999\n",
      "Epoch 24, train_loss: 0.5327, val_loss: 0.5968\n",
      "Epoch 25, train_loss: 0.5321, val_loss: 0.5935\n",
      "Epoch 26, train_loss: 0.5152, val_loss: 0.5910\n",
      "Epoch 27, train_loss: 0.5176, val_loss: 0.5866\n",
      "Epoch 28, train_loss: 0.5144, val_loss: 0.5799\n",
      "Epoch 29, train_loss: 0.5006, val_loss: 0.5766\n",
      "Epoch 30, train_loss: 0.4859, val_loss: 0.5741\n",
      "Epoch 31, train_loss: 0.4967, val_loss: 0.5724\n",
      "Epoch 32, train_loss: 0.4703, val_loss: 0.5704\n",
      "Epoch 33, train_loss: 0.4889, val_loss: 0.5673\n",
      "Epoch 34, train_loss: 0.4742, val_loss: 0.5633\n",
      "Epoch 35, train_loss: 0.4704, val_loss: 0.5580\n",
      "Epoch 36, train_loss: 0.4581, val_loss: 0.5547\n",
      "Epoch 37, train_loss: 0.4501, val_loss: 0.5510\n",
      "Epoch 38, train_loss: 0.4567, val_loss: 0.5476\n",
      "Epoch 39, train_loss: 0.4396, val_loss: 0.5451\n",
      "Epoch 40, train_loss: 0.4385, val_loss: 0.5436\n",
      "Epoch 41, train_loss: 0.4298, val_loss: 0.5402\n",
      "Epoch 42, train_loss: 0.4217, val_loss: 0.5376\n",
      "Epoch 43, train_loss: 0.3899, val_loss: 0.5382\n",
      "Epoch 44, train_loss: 0.3911, val_loss: 0.5370\n",
      "Epoch 45, train_loss: 0.3870, val_loss: 0.5348\n",
      "Epoch 46, train_loss: 0.4084, val_loss: 0.5341\n",
      "Epoch 47, train_loss: 0.3636, val_loss: 0.5319\n",
      "Epoch 48, train_loss: 0.3826, val_loss: 0.5297\n",
      "Epoch 49, train_loss: 0.3615, val_loss: 0.5292\n",
      "Epoch 50, train_loss: 0.3627, val_loss: 0.5291\n",
      "Epoch 51, train_loss: 0.3615, val_loss: 0.5301\n",
      "Epoch 52, train_loss: 0.3663, val_loss: 0.5272\n",
      "Epoch 53, train_loss: 0.3366, val_loss: 0.5255\n",
      "Epoch 54, train_loss: 0.3280, val_loss: 0.5232\n",
      "Epoch 55, train_loss: 0.3459, val_loss: 0.5215\n",
      "Epoch 56, train_loss: 0.3183, val_loss: 0.5227\n",
      "Epoch 57, train_loss: 0.3208, val_loss: 0.5217\n",
      "Epoch 58, train_loss: 0.3050, val_loss: 0.5220\n",
      "Epoch 59, train_loss: 0.3017, val_loss: 0.5239\n",
      "Epoch 60, train_loss: 0.3014, val_loss: 0.5235\n",
      "Epoch 61, train_loss: 0.3063, val_loss: 0.5211\n",
      "Epoch 62, train_loss: 0.3073, val_loss: 0.5195\n",
      "Epoch 63, train_loss: 0.2992, val_loss: 0.5198\n",
      "Epoch 64, train_loss: 0.2711, val_loss: 0.5199\n",
      "Epoch 65, train_loss: 0.2598, val_loss: 0.5198\n",
      "Epoch 66, train_loss: 0.2791, val_loss: 0.5194\n",
      "Epoch 67, train_loss: 0.2574, val_loss: 0.5195\n",
      "Epoch 68, train_loss: 0.2518, val_loss: 0.5198\n",
      "Epoch 69, train_loss: 0.2395, val_loss: 0.5189\n",
      "Epoch 70, train_loss: 0.2678, val_loss: 0.5205\n",
      "Epoch 71, train_loss: 0.2371, val_loss: 0.5203\n",
      "Epoch 72, train_loss: 0.2245, val_loss: 0.5200\n",
      "Epoch 73, train_loss: 0.2292, val_loss: 0.5187\n",
      "Epoch 74, train_loss: 0.2257, val_loss: 0.5185\n",
      "Epoch 75, train_loss: 0.2390, val_loss: 0.5185\n",
      "Epoch 76, train_loss: 0.2272, val_loss: 0.5198\n",
      "Epoch 77, train_loss: 0.2105, val_loss: 0.5233\n",
      "Early Stopping!\n",
      "Test the model on fold 3:\n",
      "{'roc_auc': 0.8595238095238095, 'pr_auc': 0.8699026067797262, 'fmax': 0.8444394666960092}\n",
      "Starting Fold: 4\n",
      "Epoch 0, train_loss: 0.6989, val_loss: 0.6889\n",
      "Epoch 1, train_loss: 0.6904, val_loss: 0.6835\n",
      "Epoch 2, train_loss: 0.6719, val_loss: 0.6767\n",
      "Epoch 3, train_loss: 0.6666, val_loss: 0.6707\n",
      "Epoch 4, train_loss: 0.6551, val_loss: 0.6669\n",
      "Epoch 5, train_loss: 0.6502, val_loss: 0.6619\n",
      "Epoch 6, train_loss: 0.6368, val_loss: 0.6584\n",
      "Epoch 7, train_loss: 0.6355, val_loss: 0.6556\n",
      "Epoch 8, train_loss: 0.6252, val_loss: 0.6524\n",
      "Epoch 9, train_loss: 0.6324, val_loss: 0.6491\n",
      "Epoch 10, train_loss: 0.6099, val_loss: 0.6471\n",
      "Epoch 11, train_loss: 0.6164, val_loss: 0.6451\n",
      "Epoch 12, train_loss: 0.6083, val_loss: 0.6408\n",
      "Epoch 13, train_loss: 0.6056, val_loss: 0.6374\n",
      "Epoch 14, train_loss: 0.6074, val_loss: 0.6335\n",
      "Epoch 15, train_loss: 0.6044, val_loss: 0.6306\n",
      "Epoch 16, train_loss: 0.5864, val_loss: 0.6272\n",
      "Epoch 17, train_loss: 0.5867, val_loss: 0.6234\n",
      "Epoch 18, train_loss: 0.5905, val_loss: 0.6209\n",
      "Epoch 19, train_loss: 0.5794, val_loss: 0.6184\n",
      "Epoch 20, train_loss: 0.5795, val_loss: 0.6161\n",
      "Epoch 21, train_loss: 0.5687, val_loss: 0.6136\n",
      "Epoch 22, train_loss: 0.5604, val_loss: 0.6097\n",
      "Epoch 23, train_loss: 0.5573, val_loss: 0.6064\n",
      "Epoch 24, train_loss: 0.5580, val_loss: 0.6033\n",
      "Epoch 25, train_loss: 0.5506, val_loss: 0.6004\n",
      "Epoch 26, train_loss: 0.5384, val_loss: 0.5971\n",
      "Epoch 27, train_loss: 0.5411, val_loss: 0.5941\n",
      "Epoch 28, train_loss: 0.5370, val_loss: 0.5909\n",
      "Epoch 29, train_loss: 0.5268, val_loss: 0.5885\n",
      "Epoch 30, train_loss: 0.5211, val_loss: 0.5861\n",
      "Epoch 31, train_loss: 0.5155, val_loss: 0.5828\n",
      "Epoch 32, train_loss: 0.5109, val_loss: 0.5797\n",
      "Epoch 33, train_loss: 0.5054, val_loss: 0.5765\n",
      "Epoch 34, train_loss: 0.4887, val_loss: 0.5729\n",
      "Epoch 35, train_loss: 0.4962, val_loss: 0.5701\n",
      "Epoch 36, train_loss: 0.4839, val_loss: 0.5671\n",
      "Epoch 37, train_loss: 0.4698, val_loss: 0.5643\n",
      "Epoch 38, train_loss: 0.4806, val_loss: 0.5610\n",
      "Epoch 39, train_loss: 0.4661, val_loss: 0.5569\n",
      "Epoch 40, train_loss: 0.4490, val_loss: 0.5536\n",
      "Epoch 41, train_loss: 0.4451, val_loss: 0.5500\n",
      "Epoch 42, train_loss: 0.4409, val_loss: 0.5458\n",
      "Epoch 43, train_loss: 0.4343, val_loss: 0.5425\n",
      "Epoch 44, train_loss: 0.4257, val_loss: 0.5396\n",
      "Epoch 45, train_loss: 0.4290, val_loss: 0.5362\n",
      "Epoch 46, train_loss: 0.4291, val_loss: 0.5334\n",
      "Epoch 47, train_loss: 0.4174, val_loss: 0.5308\n",
      "Epoch 48, train_loss: 0.3994, val_loss: 0.5282\n",
      "Epoch 49, train_loss: 0.4167, val_loss: 0.5256\n",
      "Epoch 50, train_loss: 0.3835, val_loss: 0.5232\n",
      "Epoch 51, train_loss: 0.3955, val_loss: 0.5200\n",
      "Epoch 52, train_loss: 0.3716, val_loss: 0.5172\n",
      "Epoch 53, train_loss: 0.3705, val_loss: 0.5147\n",
      "Epoch 54, train_loss: 0.3627, val_loss: 0.5119\n",
      "Epoch 55, train_loss: 0.3469, val_loss: 0.5096\n",
      "Epoch 56, train_loss: 0.3585, val_loss: 0.5065\n",
      "Epoch 57, train_loss: 0.3380, val_loss: 0.5035\n",
      "Epoch 58, train_loss: 0.3300, val_loss: 0.5017\n",
      "Epoch 59, train_loss: 0.3352, val_loss: 0.5001\n",
      "Epoch 60, train_loss: 0.3181, val_loss: 0.4978\n",
      "Epoch 61, train_loss: 0.3089, val_loss: 0.4953\n",
      "Epoch 62, train_loss: 0.3330, val_loss: 0.4931\n",
      "Epoch 63, train_loss: 0.3112, val_loss: 0.4921\n",
      "Epoch 64, train_loss: 0.2930, val_loss: 0.4910\n",
      "Epoch 65, train_loss: 0.2904, val_loss: 0.4896\n",
      "Epoch 66, train_loss: 0.2851, val_loss: 0.4881\n",
      "Epoch 67, train_loss: 0.2891, val_loss: 0.4866\n",
      "Epoch 68, train_loss: 0.2709, val_loss: 0.4850\n",
      "Epoch 69, train_loss: 0.2787, val_loss: 0.4832\n",
      "Epoch 70, train_loss: 0.2842, val_loss: 0.4819\n",
      "Epoch 71, train_loss: 0.2561, val_loss: 0.4808\n",
      "Epoch 72, train_loss: 0.2514, val_loss: 0.4798\n",
      "Epoch 73, train_loss: 0.2662, val_loss: 0.4776\n",
      "Epoch 74, train_loss: 0.2432, val_loss: 0.4772\n",
      "Epoch 75, train_loss: 0.2509, val_loss: 0.4767\n",
      "Epoch 76, train_loss: 0.2358, val_loss: 0.4769\n",
      "Epoch 77, train_loss: 0.2489, val_loss: 0.4760\n",
      "Epoch 78, train_loss: 0.2433, val_loss: 0.4745\n",
      "Epoch 79, train_loss: 0.2392, val_loss: 0.4732\n",
      "Epoch 80, train_loss: 0.2445, val_loss: 0.4720\n",
      "Epoch 81, train_loss: 0.2266, val_loss: 0.4716\n",
      "Epoch 82, train_loss: 0.2434, val_loss: 0.4717\n",
      "Epoch 83, train_loss: 0.2257, val_loss: 0.4711\n",
      "Epoch 84, train_loss: 0.2130, val_loss: 0.4704\n",
      "Epoch 85, train_loss: 0.2098, val_loss: 0.4695\n",
      "Epoch 86, train_loss: 0.1981, val_loss: 0.4688\n",
      "Epoch 87, train_loss: 0.1990, val_loss: 0.4683\n",
      "Epoch 88, train_loss: 0.2013, val_loss: 0.4666\n",
      "Epoch 89, train_loss: 0.1865, val_loss: 0.4667\n",
      "Epoch 90, train_loss: 0.2024, val_loss: 0.4669\n",
      "Epoch 91, train_loss: 0.1916, val_loss: 0.4679\n",
      "Epoch 92, train_loss: 0.1963, val_loss: 0.4679\n",
      "Epoch 93, train_loss: 0.1893, val_loss: 0.4684\n",
      "Epoch 94, train_loss: 0.1791, val_loss: 0.4688\n",
      "Epoch 95, train_loss: 0.2044, val_loss: 0.4679\n",
      "Epoch 96, train_loss: 0.1755, val_loss: 0.4682\n",
      "Epoch 97, train_loss: 0.1746, val_loss: 0.4686\n",
      "Epoch 98, train_loss: 0.1698, val_loss: 0.4686\n",
      "Epoch 99, train_loss: 0.1814, val_loss: 0.4684\n",
      "Test the model on fold 4:\n",
      "{'roc_auc': 0.8939393939393939, 'pr_auc': 0.9176444845816043, 'fmax': 0.8749950781526854}\n",
      "Evaluate pretrained model on disease class Renal (8/12)\n",
      "Starting Fold: 0\n",
      "Epoch 0, train_loss: 0.6906, val_loss: 0.6896\n",
      "Epoch 1, train_loss: 0.6875, val_loss: 0.6886\n",
      "Epoch 2, train_loss: 0.6795, val_loss: 0.6885\n",
      "Epoch 3, train_loss: 0.6759, val_loss: 0.6887\n",
      "Epoch 4, train_loss: 0.6724, val_loss: 0.6889\n",
      "Epoch 5, train_loss: 0.6598, val_loss: 0.6887\n",
      "Epoch 6, train_loss: 0.6633, val_loss: 0.6879\n",
      "Epoch 7, train_loss: 0.6597, val_loss: 0.6872\n",
      "Epoch 8, train_loss: 0.6522, val_loss: 0.6863\n",
      "Epoch 9, train_loss: 0.6524, val_loss: 0.6853\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, train_loss: 0.6426, val_loss: 0.6841\n",
      "Epoch 11, train_loss: 0.6421, val_loss: 0.6827\n",
      "Epoch 12, train_loss: 0.6264, val_loss: 0.6818\n",
      "Epoch 13, train_loss: 0.6282, val_loss: 0.6808\n",
      "Epoch 14, train_loss: 0.6297, val_loss: 0.6801\n",
      "Epoch 15, train_loss: 0.6255, val_loss: 0.6790\n",
      "Epoch 16, train_loss: 0.6223, val_loss: 0.6783\n",
      "Epoch 17, train_loss: 0.6194, val_loss: 0.6770\n",
      "Epoch 18, train_loss: 0.6123, val_loss: 0.6753\n",
      "Epoch 19, train_loss: 0.5976, val_loss: 0.6739\n",
      "Epoch 20, train_loss: 0.5979, val_loss: 0.6728\n",
      "Epoch 21, train_loss: 0.5917, val_loss: 0.6717\n",
      "Epoch 22, train_loss: 0.5932, val_loss: 0.6711\n",
      "Epoch 23, train_loss: 0.5926, val_loss: 0.6697\n",
      "Epoch 24, train_loss: 0.5797, val_loss: 0.6686\n",
      "Epoch 25, train_loss: 0.5870, val_loss: 0.6675\n",
      "Epoch 26, train_loss: 0.5716, val_loss: 0.6666\n",
      "Epoch 27, train_loss: 0.5724, val_loss: 0.6654\n",
      "Epoch 28, train_loss: 0.5683, val_loss: 0.6647\n",
      "Epoch 29, train_loss: 0.5632, val_loss: 0.6631\n",
      "Epoch 30, train_loss: 0.5570, val_loss: 0.6611\n",
      "Epoch 31, train_loss: 0.5476, val_loss: 0.6598\n",
      "Epoch 32, train_loss: 0.5472, val_loss: 0.6589\n",
      "Epoch 33, train_loss: 0.5470, val_loss: 0.6577\n",
      "Epoch 34, train_loss: 0.5356, val_loss: 0.6560\n",
      "Epoch 35, train_loss: 0.5412, val_loss: 0.6548\n",
      "Epoch 36, train_loss: 0.5298, val_loss: 0.6532\n",
      "Epoch 37, train_loss: 0.5370, val_loss: 0.6518\n",
      "Epoch 38, train_loss: 0.5281, val_loss: 0.6506\n",
      "Epoch 39, train_loss: 0.5179, val_loss: 0.6489\n",
      "Epoch 40, train_loss: 0.5231, val_loss: 0.6478\n",
      "Epoch 41, train_loss: 0.5232, val_loss: 0.6471\n",
      "Epoch 42, train_loss: 0.5043, val_loss: 0.6465\n",
      "Epoch 43, train_loss: 0.4964, val_loss: 0.6453\n",
      "Epoch 44, train_loss: 0.4952, val_loss: 0.6441\n",
      "Epoch 45, train_loss: 0.4922, val_loss: 0.6434\n",
      "Epoch 46, train_loss: 0.4892, val_loss: 0.6425\n",
      "Epoch 47, train_loss: 0.4828, val_loss: 0.6410\n",
      "Epoch 48, train_loss: 0.4798, val_loss: 0.6393\n",
      "Epoch 49, train_loss: 0.4707, val_loss: 0.6378\n",
      "Epoch 50, train_loss: 0.4713, val_loss: 0.6367\n",
      "Epoch 51, train_loss: 0.4689, val_loss: 0.6359\n",
      "Epoch 52, train_loss: 0.4684, val_loss: 0.6348\n",
      "Epoch 53, train_loss: 0.4521, val_loss: 0.6331\n",
      "Epoch 54, train_loss: 0.4512, val_loss: 0.6317\n",
      "Epoch 55, train_loss: 0.4396, val_loss: 0.6312\n",
      "Epoch 56, train_loss: 0.4315, val_loss: 0.6309\n",
      "Epoch 57, train_loss: 0.4371, val_loss: 0.6303\n",
      "Epoch 58, train_loss: 0.4314, val_loss: 0.6294\n",
      "Epoch 59, train_loss: 0.4175, val_loss: 0.6283\n",
      "Epoch 60, train_loss: 0.4182, val_loss: 0.6271\n",
      "Epoch 61, train_loss: 0.4128, val_loss: 0.6253\n",
      "Epoch 62, train_loss: 0.4076, val_loss: 0.6239\n",
      "Epoch 63, train_loss: 0.4080, val_loss: 0.6230\n",
      "Epoch 64, train_loss: 0.4064, val_loss: 0.6218\n",
      "Epoch 65, train_loss: 0.3949, val_loss: 0.6206\n",
      "Epoch 66, train_loss: 0.3869, val_loss: 0.6198\n",
      "Epoch 67, train_loss: 0.3821, val_loss: 0.6195\n",
      "Epoch 68, train_loss: 0.3864, val_loss: 0.6184\n",
      "Epoch 69, train_loss: 0.3766, val_loss: 0.6172\n",
      "Epoch 70, train_loss: 0.3576, val_loss: 0.6159\n",
      "Epoch 71, train_loss: 0.3612, val_loss: 0.6151\n",
      "Epoch 72, train_loss: 0.3533, val_loss: 0.6145\n",
      "Epoch 73, train_loss: 0.3559, val_loss: 0.6137\n",
      "Epoch 74, train_loss: 0.3441, val_loss: 0.6128\n",
      "Epoch 75, train_loss: 0.3493, val_loss: 0.6116\n",
      "Epoch 76, train_loss: 0.3420, val_loss: 0.6110\n",
      "Epoch 77, train_loss: 0.3358, val_loss: 0.6106\n",
      "Epoch 78, train_loss: 0.3312, val_loss: 0.6108\n",
      "Epoch 79, train_loss: 0.3344, val_loss: 0.6099\n",
      "Epoch 80, train_loss: 0.3174, val_loss: 0.6085\n",
      "Epoch 81, train_loss: 0.3123, val_loss: 0.6081\n",
      "Epoch 82, train_loss: 0.3171, val_loss: 0.6074\n",
      "Epoch 83, train_loss: 0.3091, val_loss: 0.6066\n",
      "Epoch 84, train_loss: 0.2976, val_loss: 0.6058\n",
      "Epoch 85, train_loss: 0.3039, val_loss: 0.6050\n",
      "Epoch 86, train_loss: 0.2873, val_loss: 0.6054\n",
      "Epoch 87, train_loss: 0.2909, val_loss: 0.6051\n",
      "Epoch 88, train_loss: 0.2930, val_loss: 0.6050\n",
      "Epoch 89, train_loss: 0.2839, val_loss: 0.6049\n",
      "Epoch 90, train_loss: 0.2816, val_loss: 0.6045\n",
      "Epoch 91, train_loss: 0.2800, val_loss: 0.6036\n",
      "Epoch 92, train_loss: 0.2674, val_loss: 0.6027\n",
      "Epoch 93, train_loss: 0.2766, val_loss: 0.6014\n",
      "Epoch 94, train_loss: 0.2703, val_loss: 0.6008\n",
      "Epoch 95, train_loss: 0.2623, val_loss: 0.6010\n",
      "Epoch 96, train_loss: 0.2615, val_loss: 0.6013\n",
      "Epoch 97, train_loss: 0.2587, val_loss: 0.6012\n",
      "Epoch 98, train_loss: 0.2488, val_loss: 0.6011\n",
      "Epoch 99, train_loss: 0.2457, val_loss: 0.6009\n",
      "Test the model on fold 0:\n",
      "{'roc_auc': 0.7654320987654321, 'pr_auc': 0.8696839120639991, 'fmax': 0.8571379592116616}\n",
      "Starting Fold: 1\n",
      "Epoch 0, train_loss: 0.6999, val_loss: 0.6942\n",
      "Epoch 1, train_loss: 0.6931, val_loss: 0.6904\n",
      "Epoch 2, train_loss: 0.6787, val_loss: 0.6856\n",
      "Epoch 3, train_loss: 0.6746, val_loss: 0.6805\n",
      "Epoch 4, train_loss: 0.6714, val_loss: 0.6773\n",
      "Epoch 5, train_loss: 0.6643, val_loss: 0.6731\n",
      "Epoch 6, train_loss: 0.6508, val_loss: 0.6705\n",
      "Epoch 7, train_loss: 0.6542, val_loss: 0.6673\n",
      "Epoch 8, train_loss: 0.6448, val_loss: 0.6650\n",
      "Epoch 9, train_loss: 0.6396, val_loss: 0.6619\n",
      "Epoch 10, train_loss: 0.6415, val_loss: 0.6606\n",
      "Epoch 11, train_loss: 0.6398, val_loss: 0.6591\n",
      "Epoch 12, train_loss: 0.6340, val_loss: 0.6581\n",
      "Epoch 13, train_loss: 0.6295, val_loss: 0.6579\n",
      "Epoch 14, train_loss: 0.6200, val_loss: 0.6573\n",
      "Epoch 15, train_loss: 0.6187, val_loss: 0.6558\n",
      "Epoch 16, train_loss: 0.6109, val_loss: 0.6550\n",
      "Epoch 17, train_loss: 0.6094, val_loss: 0.6535\n",
      "Epoch 18, train_loss: 0.6100, val_loss: 0.6525\n",
      "Epoch 19, train_loss: 0.6079, val_loss: 0.6515\n",
      "Epoch 20, train_loss: 0.6049, val_loss: 0.6495\n",
      "Epoch 21, train_loss: 0.5931, val_loss: 0.6484\n",
      "Epoch 22, train_loss: 0.5975, val_loss: 0.6469\n",
      "Epoch 23, train_loss: 0.5974, val_loss: 0.6452\n",
      "Epoch 24, train_loss: 0.5918, val_loss: 0.6435\n",
      "Epoch 25, train_loss: 0.5810, val_loss: 0.6416\n",
      "Epoch 26, train_loss: 0.5834, val_loss: 0.6402\n",
      "Epoch 27, train_loss: 0.5843, val_loss: 0.6388\n",
      "Epoch 28, train_loss: 0.5770, val_loss: 0.6376\n",
      "Epoch 29, train_loss: 0.5688, val_loss: 0.6361\n",
      "Epoch 30, train_loss: 0.5662, val_loss: 0.6345\n",
      "Epoch 31, train_loss: 0.5613, val_loss: 0.6328\n",
      "Epoch 32, train_loss: 0.5648, val_loss: 0.6309\n",
      "Epoch 33, train_loss: 0.5517, val_loss: 0.6292\n",
      "Epoch 34, train_loss: 0.5568, val_loss: 0.6273\n",
      "Epoch 35, train_loss: 0.5523, val_loss: 0.6260\n",
      "Epoch 36, train_loss: 0.5544, val_loss: 0.6250\n",
      "Epoch 37, train_loss: 0.5489, val_loss: 0.6235\n",
      "Epoch 38, train_loss: 0.5433, val_loss: 0.6212\n",
      "Epoch 39, train_loss: 0.5424, val_loss: 0.6193\n",
      "Epoch 40, train_loss: 0.5403, val_loss: 0.6178\n",
      "Epoch 41, train_loss: 0.5361, val_loss: 0.6156\n",
      "Epoch 42, train_loss: 0.5292, val_loss: 0.6139\n",
      "Epoch 43, train_loss: 0.5290, val_loss: 0.6115\n",
      "Epoch 44, train_loss: 0.5219, val_loss: 0.6101\n",
      "Epoch 45, train_loss: 0.5129, val_loss: 0.6083\n",
      "Epoch 46, train_loss: 0.5123, val_loss: 0.6065\n",
      "Epoch 47, train_loss: 0.5048, val_loss: 0.6049\n",
      "Epoch 48, train_loss: 0.5022, val_loss: 0.6036\n",
      "Epoch 49, train_loss: 0.5003, val_loss: 0.6018\n",
      "Epoch 50, train_loss: 0.5057, val_loss: 0.5999\n",
      "Epoch 51, train_loss: 0.4963, val_loss: 0.5981\n",
      "Epoch 52, train_loss: 0.4860, val_loss: 0.5957\n",
      "Epoch 53, train_loss: 0.4830, val_loss: 0.5937\n",
      "Epoch 54, train_loss: 0.4806, val_loss: 0.5918\n",
      "Epoch 55, train_loss: 0.4732, val_loss: 0.5898\n",
      "Epoch 56, train_loss: 0.4636, val_loss: 0.5874\n",
      "Epoch 57, train_loss: 0.4657, val_loss: 0.5846\n",
      "Epoch 58, train_loss: 0.4674, val_loss: 0.5824\n",
      "Epoch 59, train_loss: 0.4623, val_loss: 0.5799\n",
      "Epoch 60, train_loss: 0.4415, val_loss: 0.5777\n",
      "Epoch 61, train_loss: 0.4469, val_loss: 0.5760\n",
      "Epoch 62, train_loss: 0.4501, val_loss: 0.5742\n",
      "Epoch 63, train_loss: 0.4422, val_loss: 0.5724\n",
      "Epoch 64, train_loss: 0.4417, val_loss: 0.5697\n",
      "Epoch 65, train_loss: 0.4246, val_loss: 0.5672\n",
      "Epoch 66, train_loss: 0.4297, val_loss: 0.5650\n",
      "Epoch 67, train_loss: 0.4300, val_loss: 0.5626\n",
      "Epoch 68, train_loss: 0.4230, val_loss: 0.5610\n",
      "Epoch 69, train_loss: 0.4105, val_loss: 0.5593\n",
      "Epoch 70, train_loss: 0.4142, val_loss: 0.5575\n",
      "Epoch 71, train_loss: 0.4090, val_loss: 0.5556\n",
      "Epoch 72, train_loss: 0.3958, val_loss: 0.5537\n",
      "Epoch 73, train_loss: 0.3950, val_loss: 0.5520\n",
      "Epoch 74, train_loss: 0.3920, val_loss: 0.5506\n",
      "Epoch 75, train_loss: 0.3972, val_loss: 0.5487\n",
      "Epoch 76, train_loss: 0.3865, val_loss: 0.5471\n",
      "Epoch 77, train_loss: 0.3781, val_loss: 0.5458\n",
      "Epoch 78, train_loss: 0.3736, val_loss: 0.5435\n",
      "Epoch 79, train_loss: 0.3801, val_loss: 0.5406\n",
      "Epoch 80, train_loss: 0.3669, val_loss: 0.5388\n",
      "Epoch 81, train_loss: 0.3675, val_loss: 0.5371\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 82, train_loss: 0.3617, val_loss: 0.5355\n",
      "Epoch 83, train_loss: 0.3563, val_loss: 0.5339\n",
      "Epoch 84, train_loss: 0.3497, val_loss: 0.5322\n",
      "Epoch 85, train_loss: 0.3521, val_loss: 0.5303\n",
      "Epoch 86, train_loss: 0.3414, val_loss: 0.5283\n",
      "Epoch 87, train_loss: 0.3443, val_loss: 0.5268\n",
      "Epoch 88, train_loss: 0.3301, val_loss: 0.5264\n",
      "Epoch 89, train_loss: 0.3366, val_loss: 0.5252\n",
      "Epoch 90, train_loss: 0.3250, val_loss: 0.5231\n",
      "Epoch 91, train_loss: 0.3222, val_loss: 0.5210\n",
      "Epoch 92, train_loss: 0.3178, val_loss: 0.5200\n",
      "Epoch 93, train_loss: 0.3201, val_loss: 0.5177\n",
      "Epoch 94, train_loss: 0.3070, val_loss: 0.5156\n",
      "Epoch 95, train_loss: 0.3005, val_loss: 0.5144\n",
      "Epoch 96, train_loss: 0.2982, val_loss: 0.5129\n",
      "Epoch 97, train_loss: 0.2999, val_loss: 0.5113\n",
      "Epoch 98, train_loss: 0.2939, val_loss: 0.5093\n",
      "Epoch 99, train_loss: 0.2940, val_loss: 0.5078\n",
      "Test the model on fold 1:\n",
      "{'roc_auc': 0.8388888888888889, 'pr_auc': 0.8020936132294493, 'fmax': 0.7741888033589471}\n",
      "Starting Fold: 2\n",
      "Epoch 0, train_loss: 0.6904, val_loss: 0.6862\n",
      "Epoch 1, train_loss: 0.6838, val_loss: 0.6851\n",
      "Epoch 2, train_loss: 0.6842, val_loss: 0.6854\n",
      "Epoch 3, train_loss: 0.6723, val_loss: 0.6860\n",
      "Epoch 4, train_loss: 0.6634, val_loss: 0.6862\n",
      "Epoch 5, train_loss: 0.6474, val_loss: 0.6870\n",
      "Epoch 6, train_loss: 0.6527, val_loss: 0.6871\n",
      "Epoch 7, train_loss: 0.6367, val_loss: 0.6877\n",
      "Epoch 8, train_loss: 0.6374, val_loss: 0.6884\n",
      "Epoch 9, train_loss: 0.6259, val_loss: 0.6890\n",
      "Epoch 10, train_loss: 0.6307, val_loss: 0.6888\n",
      "Epoch 11, train_loss: 0.6268, val_loss: 0.6885\n",
      "Epoch 12, train_loss: 0.6281, val_loss: 0.6887\n",
      "Epoch 13, train_loss: 0.6128, val_loss: 0.6888\n",
      "Epoch 14, train_loss: 0.6097, val_loss: 0.6880\n",
      "Epoch 15, train_loss: 0.6041, val_loss: 0.6876\n",
      "Epoch 16, train_loss: 0.5990, val_loss: 0.6877\n",
      "Epoch 17, train_loss: 0.5998, val_loss: 0.6874\n",
      "Epoch 18, train_loss: 0.5995, val_loss: 0.6871\n",
      "Epoch 19, train_loss: 0.5912, val_loss: 0.6865\n",
      "Epoch 20, train_loss: 0.5856, val_loss: 0.6867\n",
      "Epoch 21, train_loss: 0.5831, val_loss: 0.6873\n",
      "Epoch 22, train_loss: 0.5831, val_loss: 0.6875\n",
      "Epoch 23, train_loss: 0.5689, val_loss: 0.6871\n",
      "Epoch 24, train_loss: 0.5769, val_loss: 0.6867\n",
      "Epoch 25, train_loss: 0.5739, val_loss: 0.6865\n",
      "Epoch 26, train_loss: 0.5641, val_loss: 0.6861\n",
      "Epoch 27, train_loss: 0.5692, val_loss: 0.6862\n",
      "Epoch 28, train_loss: 0.5622, val_loss: 0.6862\n",
      "Epoch 29, train_loss: 0.5590, val_loss: 0.6860\n",
      "Epoch 30, train_loss: 0.5590, val_loss: 0.6854\n",
      "Epoch 31, train_loss: 0.5511, val_loss: 0.6859\n",
      "Epoch 32, train_loss: 0.5502, val_loss: 0.6864\n",
      "Epoch 33, train_loss: 0.5390, val_loss: 0.6871\n",
      "Epoch 34, train_loss: 0.5485, val_loss: 0.6880\n",
      "Early Stopping!\n",
      "Test the model on fold 2:\n",
      "{'roc_auc': 0.6666666666666666, 'pr_auc': 0.7773620999357302, 'fmax': 0.7428522449302533}\n",
      "Starting Fold: 3\n",
      "Epoch 0, train_loss: 0.6980, val_loss: 0.6903\n",
      "Epoch 1, train_loss: 0.6854, val_loss: 0.6889\n",
      "Epoch 2, train_loss: 0.6773, val_loss: 0.6885\n",
      "Epoch 3, train_loss: 0.6695, val_loss: 0.6875\n",
      "Epoch 4, train_loss: 0.6612, val_loss: 0.6869\n",
      "Epoch 5, train_loss: 0.6651, val_loss: 0.6874\n",
      "Epoch 6, train_loss: 0.6443, val_loss: 0.6885\n",
      "Epoch 7, train_loss: 0.6430, val_loss: 0.6891\n",
      "Epoch 8, train_loss: 0.6400, val_loss: 0.6896\n",
      "Epoch 9, train_loss: 0.6357, val_loss: 0.6899\n",
      "Epoch 10, train_loss: 0.6249, val_loss: 0.6907\n",
      "Epoch 11, train_loss: 0.6179, val_loss: 0.6910\n",
      "Epoch 12, train_loss: 0.6200, val_loss: 0.6902\n",
      "Epoch 13, train_loss: 0.6130, val_loss: 0.6896\n",
      "Epoch 14, train_loss: 0.6157, val_loss: 0.6895\n",
      "Epoch 15, train_loss: 0.6050, val_loss: 0.6896\n",
      "Epoch 16, train_loss: 0.6040, val_loss: 0.6901\n",
      "Epoch 17, train_loss: 0.5971, val_loss: 0.6900\n",
      "Epoch 18, train_loss: 0.5853, val_loss: 0.6899\n",
      "Epoch 19, train_loss: 0.5863, val_loss: 0.6898\n",
      "Epoch 20, train_loss: 0.5779, val_loss: 0.6906\n",
      "Epoch 21, train_loss: 0.5793, val_loss: 0.6910\n",
      "Epoch 22, train_loss: 0.5806, val_loss: 0.6913\n",
      "Early Stopping!\n",
      "Test the model on fold 3:\n",
      "{'roc_auc': 0.6363636363636364, 'pr_auc': 0.5850670677716713, 'fmax': 0.6896504637657992}\n",
      "Starting Fold: 4\n",
      "Epoch 0, train_loss: 0.6963, val_loss: 0.6957\n",
      "Epoch 1, train_loss: 0.6917, val_loss: 0.6939\n",
      "Epoch 2, train_loss: 0.6764, val_loss: 0.6922\n",
      "Epoch 3, train_loss: 0.6656, val_loss: 0.6900\n",
      "Epoch 4, train_loss: 0.6673, val_loss: 0.6874\n",
      "Epoch 5, train_loss: 0.6554, val_loss: 0.6856\n",
      "Epoch 6, train_loss: 0.6484, val_loss: 0.6835\n",
      "Epoch 7, train_loss: 0.6376, val_loss: 0.6812\n",
      "Epoch 8, train_loss: 0.6454, val_loss: 0.6798\n",
      "Epoch 9, train_loss: 0.6401, val_loss: 0.6787\n",
      "Epoch 10, train_loss: 0.6277, val_loss: 0.6776\n",
      "Epoch 11, train_loss: 0.6214, val_loss: 0.6765\n",
      "Epoch 12, train_loss: 0.6268, val_loss: 0.6748\n",
      "Epoch 13, train_loss: 0.6153, val_loss: 0.6732\n",
      "Epoch 14, train_loss: 0.6132, val_loss: 0.6718\n",
      "Epoch 15, train_loss: 0.6092, val_loss: 0.6708\n",
      "Epoch 16, train_loss: 0.6035, val_loss: 0.6699\n",
      "Epoch 17, train_loss: 0.5988, val_loss: 0.6686\n",
      "Epoch 18, train_loss: 0.5967, val_loss: 0.6675\n",
      "Epoch 19, train_loss: 0.5921, val_loss: 0.6665\n",
      "Epoch 20, train_loss: 0.5941, val_loss: 0.6653\n",
      "Epoch 21, train_loss: 0.5840, val_loss: 0.6645\n",
      "Epoch 22, train_loss: 0.5827, val_loss: 0.6633\n",
      "Epoch 23, train_loss: 0.5771, val_loss: 0.6622\n",
      "Epoch 24, train_loss: 0.5754, val_loss: 0.6615\n",
      "Epoch 25, train_loss: 0.5682, val_loss: 0.6612\n",
      "Epoch 26, train_loss: 0.5660, val_loss: 0.6605\n",
      "Epoch 27, train_loss: 0.5566, val_loss: 0.6600\n",
      "Epoch 28, train_loss: 0.5492, val_loss: 0.6594\n",
      "Epoch 29, train_loss: 0.5483, val_loss: 0.6583\n",
      "Epoch 30, train_loss: 0.5504, val_loss: 0.6574\n",
      "Epoch 31, train_loss: 0.5377, val_loss: 0.6564\n",
      "Epoch 32, train_loss: 0.5369, val_loss: 0.6552\n",
      "Epoch 33, train_loss: 0.5385, val_loss: 0.6539\n",
      "Epoch 34, train_loss: 0.5368, val_loss: 0.6529\n",
      "Epoch 35, train_loss: 0.5303, val_loss: 0.6521\n",
      "Epoch 36, train_loss: 0.5196, val_loss: 0.6509\n",
      "Epoch 37, train_loss: 0.5199, val_loss: 0.6499\n",
      "Epoch 38, train_loss: 0.5138, val_loss: 0.6488\n",
      "Epoch 39, train_loss: 0.5098, val_loss: 0.6477\n",
      "Epoch 40, train_loss: 0.5086, val_loss: 0.6463\n",
      "Epoch 41, train_loss: 0.5046, val_loss: 0.6453\n",
      "Epoch 42, train_loss: 0.4994, val_loss: 0.6441\n",
      "Epoch 43, train_loss: 0.4978, val_loss: 0.6429\n",
      "Epoch 44, train_loss: 0.4868, val_loss: 0.6414\n",
      "Epoch 45, train_loss: 0.4809, val_loss: 0.6400\n",
      "Epoch 46, train_loss: 0.4792, val_loss: 0.6387\n",
      "Epoch 47, train_loss: 0.4701, val_loss: 0.6374\n",
      "Epoch 48, train_loss: 0.4687, val_loss: 0.6362\n",
      "Epoch 49, train_loss: 0.4595, val_loss: 0.6348\n",
      "Epoch 50, train_loss: 0.4527, val_loss: 0.6332\n",
      "Epoch 51, train_loss: 0.4508, val_loss: 0.6320\n",
      "Epoch 52, train_loss: 0.4384, val_loss: 0.6305\n",
      "Epoch 53, train_loss: 0.4454, val_loss: 0.6292\n",
      "Epoch 54, train_loss: 0.4345, val_loss: 0.6280\n",
      "Epoch 55, train_loss: 0.4323, val_loss: 0.6267\n",
      "Epoch 56, train_loss: 0.4268, val_loss: 0.6252\n",
      "Epoch 57, train_loss: 0.4273, val_loss: 0.6237\n",
      "Epoch 58, train_loss: 0.4096, val_loss: 0.6224\n",
      "Epoch 59, train_loss: 0.4145, val_loss: 0.6208\n",
      "Epoch 60, train_loss: 0.4004, val_loss: 0.6190\n",
      "Epoch 61, train_loss: 0.4080, val_loss: 0.6172\n",
      "Epoch 62, train_loss: 0.3957, val_loss: 0.6154\n",
      "Epoch 63, train_loss: 0.3854, val_loss: 0.6136\n",
      "Epoch 64, train_loss: 0.3850, val_loss: 0.6121\n",
      "Epoch 65, train_loss: 0.3730, val_loss: 0.6105\n",
      "Epoch 66, train_loss: 0.3711, val_loss: 0.6088\n",
      "Epoch 67, train_loss: 0.3678, val_loss: 0.6076\n",
      "Epoch 68, train_loss: 0.3576, val_loss: 0.6056\n",
      "Epoch 69, train_loss: 0.3554, val_loss: 0.6037\n",
      "Epoch 70, train_loss: 0.3568, val_loss: 0.6020\n",
      "Epoch 71, train_loss: 0.3527, val_loss: 0.6002\n",
      "Epoch 72, train_loss: 0.3463, val_loss: 0.5990\n",
      "Epoch 73, train_loss: 0.3462, val_loss: 0.5984\n",
      "Epoch 74, train_loss: 0.3371, val_loss: 0.5974\n",
      "Epoch 75, train_loss: 0.3399, val_loss: 0.5959\n",
      "Epoch 76, train_loss: 0.3332, val_loss: 0.5946\n",
      "Epoch 77, train_loss: 0.3168, val_loss: 0.5927\n",
      "Epoch 78, train_loss: 0.3158, val_loss: 0.5910\n",
      "Epoch 79, train_loss: 0.3134, val_loss: 0.5893\n",
      "Epoch 80, train_loss: 0.3098, val_loss: 0.5874\n",
      "Epoch 81, train_loss: 0.3014, val_loss: 0.5862\n",
      "Epoch 82, train_loss: 0.3016, val_loss: 0.5846\n",
      "Epoch 83, train_loss: 0.2882, val_loss: 0.5834\n",
      "Epoch 84, train_loss: 0.2994, val_loss: 0.5818\n",
      "Epoch 85, train_loss: 0.2816, val_loss: 0.5806\n",
      "Epoch 86, train_loss: 0.2906, val_loss: 0.5790\n",
      "Epoch 87, train_loss: 0.2749, val_loss: 0.5785\n",
      "Epoch 88, train_loss: 0.2784, val_loss: 0.5774\n",
      "Epoch 89, train_loss: 0.2724, val_loss: 0.5760\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 90, train_loss: 0.2742, val_loss: 0.5748\n",
      "Epoch 91, train_loss: 0.2687, val_loss: 0.5732\n",
      "Epoch 92, train_loss: 0.2657, val_loss: 0.5724\n",
      "Epoch 93, train_loss: 0.2517, val_loss: 0.5723\n",
      "Epoch 94, train_loss: 0.2544, val_loss: 0.5716\n",
      "Epoch 95, train_loss: 0.2543, val_loss: 0.5702\n",
      "Epoch 96, train_loss: 0.2540, val_loss: 0.5689\n",
      "Epoch 97, train_loss: 0.2460, val_loss: 0.5675\n",
      "Epoch 98, train_loss: 0.2413, val_loss: 0.5662\n",
      "Epoch 99, train_loss: 0.2380, val_loss: 0.5654\n",
      "Test the model on fold 4:\n",
      "{'roc_auc': 0.65, 'pr_auc': 0.6800589736350605, 'fmax': 0.6363586777245892}\n",
      "Evaluate pretrained model on disease class Hematological (9/12)\n",
      "Starting Fold: 0\n",
      "Epoch 0, train_loss: 0.6890, val_loss: 0.6840\n",
      "Epoch 1, train_loss: 0.6719, val_loss: 0.6742\n",
      "Epoch 2, train_loss: 0.6626, val_loss: 0.6660\n",
      "Epoch 3, train_loss: 0.6568, val_loss: 0.6577\n",
      "Epoch 4, train_loss: 0.6529, val_loss: 0.6501\n",
      "Epoch 5, train_loss: 0.6426, val_loss: 0.6429\n",
      "Epoch 6, train_loss: 0.6284, val_loss: 0.6351\n",
      "Epoch 7, train_loss: 0.6268, val_loss: 0.6264\n",
      "Epoch 8, train_loss: 0.6189, val_loss: 0.6198\n",
      "Epoch 9, train_loss: 0.6082, val_loss: 0.6131\n",
      "Epoch 10, train_loss: 0.6023, val_loss: 0.6054\n",
      "Epoch 11, train_loss: 0.5923, val_loss: 0.5987\n",
      "Epoch 12, train_loss: 0.5852, val_loss: 0.5917\n",
      "Epoch 13, train_loss: 0.5762, val_loss: 0.5846\n",
      "Epoch 14, train_loss: 0.5648, val_loss: 0.5790\n",
      "Epoch 15, train_loss: 0.5582, val_loss: 0.5723\n",
      "Epoch 16, train_loss: 0.5447, val_loss: 0.5672\n",
      "Epoch 17, train_loss: 0.5345, val_loss: 0.5605\n",
      "Epoch 18, train_loss: 0.5258, val_loss: 0.5546\n",
      "Epoch 19, train_loss: 0.5166, val_loss: 0.5482\n",
      "Epoch 20, train_loss: 0.5117, val_loss: 0.5418\n",
      "Epoch 21, train_loss: 0.5001, val_loss: 0.5371\n",
      "Epoch 22, train_loss: 0.4916, val_loss: 0.5332\n",
      "Epoch 23, train_loss: 0.4790, val_loss: 0.5300\n",
      "Epoch 24, train_loss: 0.4691, val_loss: 0.5253\n",
      "Epoch 25, train_loss: 0.4572, val_loss: 0.5192\n",
      "Epoch 26, train_loss: 0.4505, val_loss: 0.5155\n",
      "Epoch 27, train_loss: 0.4460, val_loss: 0.5123\n",
      "Epoch 28, train_loss: 0.4348, val_loss: 0.5094\n",
      "Epoch 29, train_loss: 0.4321, val_loss: 0.5067\n",
      "Epoch 30, train_loss: 0.4198, val_loss: 0.5045\n",
      "Epoch 31, train_loss: 0.4046, val_loss: 0.5012\n",
      "Epoch 32, train_loss: 0.3993, val_loss: 0.5002\n",
      "Epoch 33, train_loss: 0.3889, val_loss: 0.4977\n",
      "Epoch 34, train_loss: 0.3755, val_loss: 0.4955\n",
      "Epoch 35, train_loss: 0.3773, val_loss: 0.4942\n",
      "Epoch 36, train_loss: 0.3643, val_loss: 0.4907\n",
      "Epoch 37, train_loss: 0.3534, val_loss: 0.4905\n",
      "Epoch 38, train_loss: 0.3504, val_loss: 0.4878\n",
      "Epoch 39, train_loss: 0.3407, val_loss: 0.4842\n",
      "Epoch 40, train_loss: 0.3307, val_loss: 0.4838\n",
      "Epoch 41, train_loss: 0.3204, val_loss: 0.4819\n",
      "Epoch 42, train_loss: 0.3179, val_loss: 0.4823\n",
      "Epoch 43, train_loss: 0.3094, val_loss: 0.4806\n",
      "Epoch 44, train_loss: 0.3038, val_loss: 0.4787\n",
      "Epoch 45, train_loss: 0.2953, val_loss: 0.4765\n",
      "Epoch 46, train_loss: 0.2901, val_loss: 0.4777\n",
      "Epoch 47, train_loss: 0.2859, val_loss: 0.4759\n",
      "Epoch 48, train_loss: 0.2767, val_loss: 0.4756\n",
      "Epoch 49, train_loss: 0.2700, val_loss: 0.4746\n",
      "Epoch 50, train_loss: 0.2677, val_loss: 0.4756\n",
      "Epoch 51, train_loss: 0.2656, val_loss: 0.4738\n",
      "Epoch 52, train_loss: 0.2571, val_loss: 0.4742\n",
      "Epoch 53, train_loss: 0.2504, val_loss: 0.4736\n",
      "Epoch 54, train_loss: 0.2466, val_loss: 0.4724\n",
      "Epoch 55, train_loss: 0.2444, val_loss: 0.4735\n",
      "Epoch 56, train_loss: 0.2348, val_loss: 0.4715\n",
      "Epoch 57, train_loss: 0.2333, val_loss: 0.4731\n",
      "Epoch 58, train_loss: 0.2303, val_loss: 0.4723\n",
      "Epoch 59, train_loss: 0.2260, val_loss: 0.4704\n",
      "Epoch 60, train_loss: 0.2218, val_loss: 0.4735\n",
      "Epoch 61, train_loss: 0.2150, val_loss: 0.4734\n",
      "Epoch 62, train_loss: 0.2136, val_loss: 0.4726\n",
      "Epoch 63, train_loss: 0.2094, val_loss: 0.4743\n",
      "Epoch 64, train_loss: 0.2077, val_loss: 0.4766\n",
      "Early Stopping!\n",
      "Test the model on fold 0:\n",
      "{'roc_auc': 0.833096590909091, 'pr_auc': 0.8871672238471264, 'fmax': 0.8171993155582219}\n",
      "Starting Fold: 1\n",
      "Epoch 0, train_loss: 0.6930, val_loss: 0.6822\n",
      "Epoch 1, train_loss: 0.6755, val_loss: 0.6729\n",
      "Epoch 2, train_loss: 0.6612, val_loss: 0.6670\n",
      "Epoch 3, train_loss: 0.6537, val_loss: 0.6636\n",
      "Epoch 4, train_loss: 0.6444, val_loss: 0.6590\n",
      "Epoch 5, train_loss: 0.6332, val_loss: 0.6539\n",
      "Epoch 6, train_loss: 0.6266, val_loss: 0.6479\n",
      "Epoch 7, train_loss: 0.6173, val_loss: 0.6424\n",
      "Epoch 8, train_loss: 0.6089, val_loss: 0.6381\n",
      "Epoch 9, train_loss: 0.6007, val_loss: 0.6348\n",
      "Epoch 10, train_loss: 0.6010, val_loss: 0.6304\n",
      "Epoch 11, train_loss: 0.5938, val_loss: 0.6256\n",
      "Epoch 12, train_loss: 0.5824, val_loss: 0.6210\n",
      "Epoch 13, train_loss: 0.5743, val_loss: 0.6163\n",
      "Epoch 14, train_loss: 0.5665, val_loss: 0.6116\n",
      "Epoch 15, train_loss: 0.5654, val_loss: 0.6090\n",
      "Epoch 16, train_loss: 0.5581, val_loss: 0.6067\n",
      "Epoch 17, train_loss: 0.5520, val_loss: 0.6026\n",
      "Epoch 18, train_loss: 0.5413, val_loss: 0.5991\n",
      "Epoch 19, train_loss: 0.5363, val_loss: 0.5944\n",
      "Epoch 20, train_loss: 0.5331, val_loss: 0.5912\n",
      "Epoch 21, train_loss: 0.5229, val_loss: 0.5884\n",
      "Epoch 22, train_loss: 0.5143, val_loss: 0.5845\n",
      "Epoch 23, train_loss: 0.5109, val_loss: 0.5799\n",
      "Epoch 24, train_loss: 0.5018, val_loss: 0.5761\n",
      "Epoch 25, train_loss: 0.4931, val_loss: 0.5744\n",
      "Epoch 26, train_loss: 0.4861, val_loss: 0.5718\n",
      "Epoch 27, train_loss: 0.4792, val_loss: 0.5679\n",
      "Epoch 28, train_loss: 0.4769, val_loss: 0.5639\n",
      "Epoch 29, train_loss: 0.4721, val_loss: 0.5599\n",
      "Epoch 30, train_loss: 0.4621, val_loss: 0.5575\n",
      "Epoch 31, train_loss: 0.4554, val_loss: 0.5557\n",
      "Epoch 32, train_loss: 0.4474, val_loss: 0.5520\n",
      "Epoch 33, train_loss: 0.4381, val_loss: 0.5489\n",
      "Epoch 34, train_loss: 0.4307, val_loss: 0.5464\n",
      "Epoch 35, train_loss: 0.4284, val_loss: 0.5433\n",
      "Epoch 36, train_loss: 0.4169, val_loss: 0.5398\n",
      "Epoch 37, train_loss: 0.4151, val_loss: 0.5381\n",
      "Epoch 38, train_loss: 0.4035, val_loss: 0.5368\n",
      "Epoch 39, train_loss: 0.3961, val_loss: 0.5329\n",
      "Epoch 40, train_loss: 0.3878, val_loss: 0.5310\n",
      "Epoch 41, train_loss: 0.3840, val_loss: 0.5265\n",
      "Epoch 42, train_loss: 0.3729, val_loss: 0.5251\n",
      "Epoch 43, train_loss: 0.3742, val_loss: 0.5235\n",
      "Epoch 44, train_loss: 0.3585, val_loss: 0.5211\n",
      "Epoch 45, train_loss: 0.3569, val_loss: 0.5188\n",
      "Epoch 46, train_loss: 0.3495, val_loss: 0.5184\n",
      "Epoch 47, train_loss: 0.3434, val_loss: 0.5167\n",
      "Epoch 48, train_loss: 0.3391, val_loss: 0.5152\n",
      "Epoch 49, train_loss: 0.3262, val_loss: 0.5128\n",
      "Epoch 50, train_loss: 0.3252, val_loss: 0.5138\n",
      "Epoch 51, train_loss: 0.3158, val_loss: 0.5125\n",
      "Epoch 52, train_loss: 0.3128, val_loss: 0.5114\n",
      "Epoch 53, train_loss: 0.3014, val_loss: 0.5112\n",
      "Epoch 54, train_loss: 0.2963, val_loss: 0.5095\n",
      "Epoch 55, train_loss: 0.2914, val_loss: 0.5069\n",
      "Epoch 56, train_loss: 0.2865, val_loss: 0.5068\n",
      "Epoch 57, train_loss: 0.2861, val_loss: 0.5065\n",
      "Epoch 58, train_loss: 0.2771, val_loss: 0.5067\n",
      "Epoch 59, train_loss: 0.2723, val_loss: 0.5049\n",
      "Epoch 60, train_loss: 0.2675, val_loss: 0.5072\n",
      "Epoch 61, train_loss: 0.2659, val_loss: 0.5056\n",
      "Epoch 62, train_loss: 0.2580, val_loss: 0.5052\n",
      "Epoch 63, train_loss: 0.2530, val_loss: 0.5045\n",
      "Epoch 64, train_loss: 0.2447, val_loss: 0.5041\n",
      "Epoch 65, train_loss: 0.2457, val_loss: 0.5042\n",
      "Epoch 66, train_loss: 0.2408, val_loss: 0.5046\n",
      "Epoch 67, train_loss: 0.2369, val_loss: 0.5060\n",
      "Epoch 68, train_loss: 0.2288, val_loss: 0.5059\n",
      "Epoch 69, train_loss: 0.2288, val_loss: 0.5037\n",
      "Epoch 70, train_loss: 0.2268, val_loss: 0.5043\n",
      "Epoch 71, train_loss: 0.2234, val_loss: 0.5082\n",
      "Early Stopping!\n",
      "Test the model on fold 1:\n",
      "{'roc_auc': 0.8905058905058905, 'pr_auc': 0.9017704658561589, 'fmax': 0.8249950281549628}\n",
      "Starting Fold: 2\n",
      "Epoch 0, train_loss: 0.6872, val_loss: 0.6859\n",
      "Epoch 1, train_loss: 0.6653, val_loss: 0.6796\n",
      "Epoch 2, train_loss: 0.6548, val_loss: 0.6738\n",
      "Epoch 3, train_loss: 0.6432, val_loss: 0.6678\n",
      "Epoch 4, train_loss: 0.6336, val_loss: 0.6647\n",
      "Epoch 5, train_loss: 0.6214, val_loss: 0.6606\n",
      "Epoch 6, train_loss: 0.6139, val_loss: 0.6565\n",
      "Epoch 7, train_loss: 0.6029, val_loss: 0.6526\n",
      "Epoch 8, train_loss: 0.5944, val_loss: 0.6487\n",
      "Epoch 9, train_loss: 0.5887, val_loss: 0.6449\n",
      "Epoch 10, train_loss: 0.5801, val_loss: 0.6419\n",
      "Epoch 11, train_loss: 0.5715, val_loss: 0.6384\n",
      "Epoch 12, train_loss: 0.5604, val_loss: 0.6357\n",
      "Epoch 13, train_loss: 0.5549, val_loss: 0.6337\n",
      "Epoch 14, train_loss: 0.5479, val_loss: 0.6308\n",
      "Epoch 15, train_loss: 0.5418, val_loss: 0.6277\n",
      "Epoch 16, train_loss: 0.5345, val_loss: 0.6252\n",
      "Epoch 17, train_loss: 0.5234, val_loss: 0.6226\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18, train_loss: 0.5193, val_loss: 0.6205\n",
      "Epoch 19, train_loss: 0.5101, val_loss: 0.6171\n",
      "Epoch 20, train_loss: 0.5043, val_loss: 0.6146\n",
      "Epoch 21, train_loss: 0.4993, val_loss: 0.6115\n",
      "Epoch 22, train_loss: 0.4894, val_loss: 0.6092\n",
      "Epoch 23, train_loss: 0.4858, val_loss: 0.6076\n",
      "Epoch 24, train_loss: 0.4745, val_loss: 0.6058\n",
      "Epoch 25, train_loss: 0.4680, val_loss: 0.6031\n",
      "Epoch 26, train_loss: 0.4604, val_loss: 0.6004\n",
      "Epoch 27, train_loss: 0.4548, val_loss: 0.5992\n",
      "Epoch 28, train_loss: 0.4411, val_loss: 0.5959\n",
      "Epoch 29, train_loss: 0.4370, val_loss: 0.5938\n",
      "Epoch 30, train_loss: 0.4349, val_loss: 0.5908\n",
      "Epoch 31, train_loss: 0.4260, val_loss: 0.5876\n",
      "Epoch 32, train_loss: 0.4158, val_loss: 0.5870\n",
      "Epoch 33, train_loss: 0.4088, val_loss: 0.5849\n",
      "Epoch 34, train_loss: 0.3993, val_loss: 0.5821\n",
      "Epoch 35, train_loss: 0.3965, val_loss: 0.5795\n",
      "Epoch 36, train_loss: 0.3865, val_loss: 0.5766\n",
      "Epoch 37, train_loss: 0.3819, val_loss: 0.5752\n",
      "Epoch 38, train_loss: 0.3657, val_loss: 0.5719\n",
      "Epoch 39, train_loss: 0.3637, val_loss: 0.5710\n",
      "Epoch 40, train_loss: 0.3576, val_loss: 0.5697\n",
      "Epoch 41, train_loss: 0.3499, val_loss: 0.5687\n",
      "Epoch 42, train_loss: 0.3410, val_loss: 0.5658\n",
      "Epoch 43, train_loss: 0.3359, val_loss: 0.5624\n",
      "Epoch 44, train_loss: 0.3273, val_loss: 0.5610\n",
      "Epoch 45, train_loss: 0.3227, val_loss: 0.5599\n",
      "Epoch 46, train_loss: 0.3141, val_loss: 0.5583\n",
      "Epoch 47, train_loss: 0.3067, val_loss: 0.5559\n",
      "Epoch 48, train_loss: 0.3050, val_loss: 0.5542\n",
      "Epoch 49, train_loss: 0.2997, val_loss: 0.5534\n",
      "Epoch 50, train_loss: 0.2940, val_loss: 0.5518\n",
      "Epoch 51, train_loss: 0.2870, val_loss: 0.5502\n",
      "Epoch 52, train_loss: 0.2812, val_loss: 0.5486\n",
      "Epoch 53, train_loss: 0.2744, val_loss: 0.5452\n",
      "Epoch 54, train_loss: 0.2694, val_loss: 0.5462\n",
      "Epoch 55, train_loss: 0.2602, val_loss: 0.5439\n",
      "Epoch 56, train_loss: 0.2577, val_loss: 0.5431\n",
      "Epoch 57, train_loss: 0.2538, val_loss: 0.5420\n",
      "Epoch 58, train_loss: 0.2477, val_loss: 0.5398\n",
      "Epoch 59, train_loss: 0.2419, val_loss: 0.5378\n",
      "Epoch 60, train_loss: 0.2431, val_loss: 0.5378\n",
      "Epoch 61, train_loss: 0.2360, val_loss: 0.5356\n",
      "Epoch 62, train_loss: 0.2296, val_loss: 0.5344\n",
      "Epoch 63, train_loss: 0.2263, val_loss: 0.5356\n",
      "Epoch 64, train_loss: 0.2233, val_loss: 0.5328\n",
      "Epoch 65, train_loss: 0.2212, val_loss: 0.5330\n",
      "Epoch 66, train_loss: 0.2180, val_loss: 0.5324\n",
      "Epoch 67, train_loss: 0.2140, val_loss: 0.5319\n",
      "Epoch 68, train_loss: 0.2078, val_loss: 0.5316\n",
      "Epoch 69, train_loss: 0.2112, val_loss: 0.5293\n",
      "Epoch 70, train_loss: 0.2031, val_loss: 0.5282\n",
      "Epoch 71, train_loss: 0.2074, val_loss: 0.5266\n",
      "Epoch 72, train_loss: 0.1991, val_loss: 0.5261\n",
      "Epoch 73, train_loss: 0.1955, val_loss: 0.5261\n",
      "Epoch 74, train_loss: 0.1918, val_loss: 0.5247\n",
      "Epoch 75, train_loss: 0.1928, val_loss: 0.5241\n",
      "Epoch 76, train_loss: 0.1865, val_loss: 0.5238\n",
      "Epoch 77, train_loss: 0.1856, val_loss: 0.5228\n",
      "Epoch 78, train_loss: 0.1808, val_loss: 0.5246\n",
      "Epoch 79, train_loss: 0.1820, val_loss: 0.5223\n",
      "Epoch 80, train_loss: 0.1789, val_loss: 0.5217\n",
      "Epoch 81, train_loss: 0.1766, val_loss: 0.5177\n",
      "Epoch 82, train_loss: 0.1784, val_loss: 0.5208\n",
      "Epoch 83, train_loss: 0.1729, val_loss: 0.5198\n",
      "Epoch 84, train_loss: 0.1696, val_loss: 0.5202\n",
      "Epoch 85, train_loss: 0.1694, val_loss: 0.5190\n",
      "Epoch 86, train_loss: 0.1683, val_loss: 0.5191\n",
      "Epoch 87, train_loss: 0.1672, val_loss: 0.5173\n",
      "Epoch 88, train_loss: 0.1673, val_loss: 0.5182\n",
      "Epoch 89, train_loss: 0.1639, val_loss: 0.5167\n",
      "Epoch 90, train_loss: 0.1633, val_loss: 0.5193\n",
      "Epoch 91, train_loss: 0.1615, val_loss: 0.5156\n",
      "Epoch 92, train_loss: 0.1586, val_loss: 0.5156\n",
      "Epoch 93, train_loss: 0.1594, val_loss: 0.5172\n",
      "Epoch 94, train_loss: 0.1584, val_loss: 0.5166\n",
      "Epoch 95, train_loss: 0.1555, val_loss: 0.5169\n",
      "Epoch 96, train_loss: 0.1565, val_loss: 0.5160\n",
      "Epoch 97, train_loss: 0.1532, val_loss: 0.5160\n",
      "Epoch 98, train_loss: 0.1527, val_loss: 0.5174\n",
      "Epoch 99, train_loss: 0.1509, val_loss: 0.5110\n",
      "Test the model on fold 2:\n",
      "{'roc_auc': 0.8329868329868331, 'pr_auc': 0.8516498822167983, 'fmax': 0.7816043070724541}\n",
      "Starting Fold: 3\n",
      "Epoch 0, train_loss: 0.6825, val_loss: 0.6836\n",
      "Epoch 1, train_loss: 0.6655, val_loss: 0.6757\n",
      "Epoch 2, train_loss: 0.6501, val_loss: 0.6675\n",
      "Epoch 3, train_loss: 0.6372, val_loss: 0.6621\n",
      "Epoch 4, train_loss: 0.6211, val_loss: 0.6577\n",
      "Epoch 5, train_loss: 0.6146, val_loss: 0.6519\n",
      "Epoch 6, train_loss: 0.6053, val_loss: 0.6476\n",
      "Epoch 7, train_loss: 0.5929, val_loss: 0.6416\n",
      "Epoch 8, train_loss: 0.5846, val_loss: 0.6384\n",
      "Epoch 9, train_loss: 0.5713, val_loss: 0.6340\n",
      "Epoch 10, train_loss: 0.5632, val_loss: 0.6298\n",
      "Epoch 11, train_loss: 0.5567, val_loss: 0.6262\n",
      "Epoch 12, train_loss: 0.5485, val_loss: 0.6229\n",
      "Epoch 13, train_loss: 0.5375, val_loss: 0.6186\n",
      "Epoch 14, train_loss: 0.5364, val_loss: 0.6156\n",
      "Epoch 15, train_loss: 0.5259, val_loss: 0.6124\n",
      "Epoch 16, train_loss: 0.5162, val_loss: 0.6094\n",
      "Epoch 17, train_loss: 0.5064, val_loss: 0.6058\n",
      "Epoch 18, train_loss: 0.5014, val_loss: 0.6027\n",
      "Epoch 19, train_loss: 0.4892, val_loss: 0.6000\n",
      "Epoch 20, train_loss: 0.4851, val_loss: 0.5977\n",
      "Epoch 21, train_loss: 0.4777, val_loss: 0.5938\n",
      "Epoch 22, train_loss: 0.4694, val_loss: 0.5917\n",
      "Epoch 23, train_loss: 0.4540, val_loss: 0.5871\n",
      "Epoch 24, train_loss: 0.4488, val_loss: 0.5842\n",
      "Epoch 25, train_loss: 0.4454, val_loss: 0.5824\n",
      "Epoch 26, train_loss: 0.4370, val_loss: 0.5798\n",
      "Epoch 27, train_loss: 0.4335, val_loss: 0.5781\n",
      "Epoch 28, train_loss: 0.4203, val_loss: 0.5759\n",
      "Epoch 29, train_loss: 0.4134, val_loss: 0.5720\n",
      "Epoch 30, train_loss: 0.4037, val_loss: 0.5697\n",
      "Epoch 31, train_loss: 0.3954, val_loss: 0.5694\n",
      "Epoch 32, train_loss: 0.3883, val_loss: 0.5662\n",
      "Epoch 33, train_loss: 0.3819, val_loss: 0.5645\n",
      "Epoch 34, train_loss: 0.3740, val_loss: 0.5623\n",
      "Epoch 35, train_loss: 0.3689, val_loss: 0.5613\n",
      "Epoch 36, train_loss: 0.3613, val_loss: 0.5606\n",
      "Epoch 37, train_loss: 0.3491, val_loss: 0.5565\n",
      "Epoch 38, train_loss: 0.3454, val_loss: 0.5572\n",
      "Epoch 39, train_loss: 0.3391, val_loss: 0.5568\n",
      "Epoch 40, train_loss: 0.3326, val_loss: 0.5549\n",
      "Epoch 41, train_loss: 0.3246, val_loss: 0.5543\n",
      "Epoch 42, train_loss: 0.3195, val_loss: 0.5521\n",
      "Epoch 43, train_loss: 0.3140, val_loss: 0.5531\n",
      "Epoch 44, train_loss: 0.3098, val_loss: 0.5520\n",
      "Epoch 45, train_loss: 0.2988, val_loss: 0.5532\n",
      "Epoch 46, train_loss: 0.2940, val_loss: 0.5506\n",
      "Epoch 47, train_loss: 0.2894, val_loss: 0.5495\n",
      "Epoch 48, train_loss: 0.2869, val_loss: 0.5481\n",
      "Epoch 49, train_loss: 0.2804, val_loss: 0.5493\n",
      "Epoch 50, train_loss: 0.2700, val_loss: 0.5510\n",
      "Epoch 51, train_loss: 0.2683, val_loss: 0.5520\n",
      "Epoch 52, train_loss: 0.2624, val_loss: 0.5523\n",
      "Epoch 53, train_loss: 0.2557, val_loss: 0.5503\n",
      "Epoch 54, train_loss: 0.2570, val_loss: 0.5520\n",
      "Epoch 55, train_loss: 0.2504, val_loss: 0.5513\n",
      "Epoch 56, train_loss: 0.2403, val_loss: 0.5516\n",
      "Epoch 57, train_loss: 0.2397, val_loss: 0.5532\n",
      "Early Stopping!\n",
      "Test the model on fold 3:\n",
      "{'roc_auc': 0.8897581792318635, 'pr_auc': 0.889566783348298, 'fmax': 0.8235244955310788}\n",
      "Starting Fold: 4\n",
      "Epoch 0, train_loss: 0.6902, val_loss: 0.6793\n",
      "Epoch 1, train_loss: 0.6755, val_loss: 0.6668\n",
      "Epoch 2, train_loss: 0.6521, val_loss: 0.6552\n",
      "Epoch 3, train_loss: 0.6406, val_loss: 0.6465\n",
      "Epoch 4, train_loss: 0.6233, val_loss: 0.6373\n",
      "Epoch 5, train_loss: 0.6202, val_loss: 0.6289\n",
      "Epoch 6, train_loss: 0.6096, val_loss: 0.6212\n",
      "Epoch 7, train_loss: 0.5988, val_loss: 0.6132\n",
      "Epoch 8, train_loss: 0.5867, val_loss: 0.6067\n",
      "Epoch 9, train_loss: 0.5822, val_loss: 0.6009\n",
      "Epoch 10, train_loss: 0.5750, val_loss: 0.5945\n",
      "Epoch 11, train_loss: 0.5652, val_loss: 0.5890\n",
      "Epoch 12, train_loss: 0.5582, val_loss: 0.5838\n",
      "Epoch 13, train_loss: 0.5501, val_loss: 0.5796\n",
      "Epoch 14, train_loss: 0.5412, val_loss: 0.5760\n",
      "Epoch 15, train_loss: 0.5350, val_loss: 0.5723\n",
      "Epoch 16, train_loss: 0.5250, val_loss: 0.5699\n",
      "Epoch 17, train_loss: 0.5176, val_loss: 0.5662\n",
      "Epoch 18, train_loss: 0.5118, val_loss: 0.5628\n",
      "Epoch 19, train_loss: 0.5057, val_loss: 0.5595\n",
      "Epoch 20, train_loss: 0.4957, val_loss: 0.5558\n",
      "Epoch 21, train_loss: 0.4923, val_loss: 0.5524\n",
      "Epoch 22, train_loss: 0.4820, val_loss: 0.5491\n",
      "Epoch 23, train_loss: 0.4708, val_loss: 0.5456\n",
      "Epoch 24, train_loss: 0.4640, val_loss: 0.5429\n",
      "Epoch 25, train_loss: 0.4569, val_loss: 0.5408\n",
      "Epoch 26, train_loss: 0.4509, val_loss: 0.5379\n",
      "Epoch 27, train_loss: 0.4449, val_loss: 0.5337\n",
      "Epoch 28, train_loss: 0.4334, val_loss: 0.5322\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29, train_loss: 0.4283, val_loss: 0.5297\n",
      "Epoch 30, train_loss: 0.4173, val_loss: 0.5276\n",
      "Epoch 31, train_loss: 0.4071, val_loss: 0.5246\n",
      "Epoch 32, train_loss: 0.4010, val_loss: 0.5225\n",
      "Epoch 33, train_loss: 0.3986, val_loss: 0.5209\n",
      "Epoch 34, train_loss: 0.3842, val_loss: 0.5180\n",
      "Epoch 35, train_loss: 0.3791, val_loss: 0.5163\n",
      "Epoch 36, train_loss: 0.3681, val_loss: 0.5148\n",
      "Epoch 37, train_loss: 0.3623, val_loss: 0.5119\n",
      "Epoch 38, train_loss: 0.3578, val_loss: 0.5103\n",
      "Epoch 39, train_loss: 0.3477, val_loss: 0.5081\n",
      "Epoch 40, train_loss: 0.3439, val_loss: 0.5072\n",
      "Epoch 41, train_loss: 0.3296, val_loss: 0.5057\n",
      "Epoch 42, train_loss: 0.3288, val_loss: 0.5034\n",
      "Epoch 43, train_loss: 0.3209, val_loss: 0.5015\n",
      "Epoch 44, train_loss: 0.3173, val_loss: 0.5008\n",
      "Epoch 45, train_loss: 0.3031, val_loss: 0.4996\n",
      "Epoch 46, train_loss: 0.3007, val_loss: 0.4982\n",
      "Epoch 47, train_loss: 0.2917, val_loss: 0.4972\n",
      "Epoch 48, train_loss: 0.2912, val_loss: 0.4971\n",
      "Epoch 49, train_loss: 0.2854, val_loss: 0.4963\n",
      "Epoch 50, train_loss: 0.2762, val_loss: 0.4961\n",
      "Epoch 51, train_loss: 0.2671, val_loss: 0.4960\n",
      "Epoch 52, train_loss: 0.2648, val_loss: 0.4950\n",
      "Epoch 53, train_loss: 0.2630, val_loss: 0.4949\n",
      "Epoch 54, train_loss: 0.2536, val_loss: 0.4936\n",
      "Epoch 55, train_loss: 0.2487, val_loss: 0.4945\n",
      "Epoch 56, train_loss: 0.2447, val_loss: 0.4934\n",
      "Epoch 57, train_loss: 0.2416, val_loss: 0.4931\n",
      "Epoch 58, train_loss: 0.2386, val_loss: 0.4929\n",
      "Epoch 59, train_loss: 0.2254, val_loss: 0.4941\n",
      "Epoch 60, train_loss: 0.2272, val_loss: 0.4928\n",
      "Epoch 61, train_loss: 0.2274, val_loss: 0.4930\n",
      "Epoch 62, train_loss: 0.2200, val_loss: 0.4938\n",
      "Epoch 63, train_loss: 0.2137, val_loss: 0.4944\n",
      "Epoch 64, train_loss: 0.2121, val_loss: 0.4946\n",
      "Epoch 65, train_loss: 0.2105, val_loss: 0.4929\n",
      "Epoch 66, train_loss: 0.2092, val_loss: 0.4936\n",
      "Epoch 67, train_loss: 0.2028, val_loss: 0.4954\n",
      "Early Stopping!\n",
      "Test the model on fold 4:\n",
      "{'roc_auc': 0.8149210903873745, 'pr_auc': 0.8345287660675561, 'fmax': 0.7435848258050424}\n",
      "Evaluate pretrained model on disease class Immunological (10/12)\n",
      "Starting Fold: 0\n",
      "Epoch 0, train_loss: 0.6876, val_loss: 0.6771\n",
      "Epoch 1, train_loss: 0.6829, val_loss: 0.6709\n",
      "Epoch 2, train_loss: 0.6686, val_loss: 0.6658\n",
      "Epoch 3, train_loss: 0.6629, val_loss: 0.6591\n",
      "Epoch 4, train_loss: 0.6548, val_loss: 0.6526\n",
      "Epoch 5, train_loss: 0.6481, val_loss: 0.6467\n",
      "Epoch 6, train_loss: 0.6365, val_loss: 0.6420\n",
      "Epoch 7, train_loss: 0.6250, val_loss: 0.6359\n",
      "Epoch 8, train_loss: 0.6167, val_loss: 0.6304\n",
      "Epoch 9, train_loss: 0.6127, val_loss: 0.6220\n",
      "Epoch 10, train_loss: 0.6033, val_loss: 0.6152\n",
      "Epoch 11, train_loss: 0.5972, val_loss: 0.6080\n",
      "Epoch 12, train_loss: 0.5853, val_loss: 0.6018\n",
      "Epoch 13, train_loss: 0.5694, val_loss: 0.5952\n",
      "Epoch 14, train_loss: 0.5747, val_loss: 0.5885\n",
      "Epoch 15, train_loss: 0.5539, val_loss: 0.5816\n",
      "Epoch 16, train_loss: 0.5505, val_loss: 0.5718\n",
      "Epoch 17, train_loss: 0.5404, val_loss: 0.5648\n",
      "Epoch 18, train_loss: 0.5340, val_loss: 0.5565\n",
      "Epoch 19, train_loss: 0.5320, val_loss: 0.5469\n",
      "Epoch 20, train_loss: 0.5143, val_loss: 0.5410\n",
      "Epoch 21, train_loss: 0.5024, val_loss: 0.5341\n",
      "Epoch 22, train_loss: 0.4985, val_loss: 0.5264\n",
      "Epoch 23, train_loss: 0.4891, val_loss: 0.5193\n",
      "Epoch 24, train_loss: 0.4880, val_loss: 0.5125\n",
      "Epoch 25, train_loss: 0.4684, val_loss: 0.5065\n",
      "Epoch 26, train_loss: 0.4675, val_loss: 0.5007\n",
      "Epoch 27, train_loss: 0.4634, val_loss: 0.4930\n",
      "Epoch 28, train_loss: 0.4450, val_loss: 0.4850\n",
      "Epoch 29, train_loss: 0.4371, val_loss: 0.4801\n",
      "Epoch 30, train_loss: 0.4365, val_loss: 0.4736\n",
      "Epoch 31, train_loss: 0.4276, val_loss: 0.4670\n",
      "Epoch 32, train_loss: 0.4226, val_loss: 0.4627\n",
      "Epoch 33, train_loss: 0.4078, val_loss: 0.4580\n",
      "Epoch 34, train_loss: 0.3999, val_loss: 0.4521\n",
      "Epoch 35, train_loss: 0.3996, val_loss: 0.4476\n",
      "Epoch 36, train_loss: 0.3785, val_loss: 0.4428\n",
      "Epoch 37, train_loss: 0.3725, val_loss: 0.4381\n",
      "Epoch 38, train_loss: 0.3636, val_loss: 0.4325\n",
      "Epoch 39, train_loss: 0.3670, val_loss: 0.4291\n",
      "Epoch 40, train_loss: 0.3472, val_loss: 0.4240\n",
      "Epoch 41, train_loss: 0.3548, val_loss: 0.4219\n",
      "Epoch 42, train_loss: 0.3442, val_loss: 0.4173\n",
      "Epoch 43, train_loss: 0.3398, val_loss: 0.4146\n",
      "Epoch 44, train_loss: 0.3269, val_loss: 0.4090\n",
      "Epoch 45, train_loss: 0.3217, val_loss: 0.4088\n",
      "Epoch 46, train_loss: 0.3221, val_loss: 0.4043\n",
      "Epoch 47, train_loss: 0.3043, val_loss: 0.4017\n",
      "Epoch 48, train_loss: 0.3123, val_loss: 0.3997\n",
      "Epoch 49, train_loss: 0.3002, val_loss: 0.3941\n",
      "Epoch 50, train_loss: 0.3029, val_loss: 0.3899\n",
      "Epoch 51, train_loss: 0.2906, val_loss: 0.3898\n",
      "Epoch 52, train_loss: 0.2880, val_loss: 0.3870\n",
      "Epoch 53, train_loss: 0.2799, val_loss: 0.3850\n",
      "Epoch 54, train_loss: 0.2831, val_loss: 0.3815\n",
      "Epoch 55, train_loss: 0.2729, val_loss: 0.3787\n",
      "Epoch 56, train_loss: 0.2650, val_loss: 0.3784\n",
      "Epoch 57, train_loss: 0.2686, val_loss: 0.3754\n",
      "Epoch 58, train_loss: 0.2605, val_loss: 0.3733\n",
      "Epoch 59, train_loss: 0.2473, val_loss: 0.3730\n",
      "Epoch 60, train_loss: 0.2435, val_loss: 0.3704\n",
      "Epoch 61, train_loss: 0.2515, val_loss: 0.3693\n",
      "Epoch 62, train_loss: 0.2392, val_loss: 0.3689\n",
      "Epoch 63, train_loss: 0.2384, val_loss: 0.3677\n",
      "Epoch 64, train_loss: 0.2337, val_loss: 0.3634\n",
      "Epoch 65, train_loss: 0.2270, val_loss: 0.3634\n",
      "Epoch 66, train_loss: 0.2262, val_loss: 0.3628\n",
      "Epoch 67, train_loss: 0.2237, val_loss: 0.3623\n",
      "Epoch 68, train_loss: 0.2244, val_loss: 0.3620\n",
      "Epoch 69, train_loss: 0.2191, val_loss: 0.3607\n",
      "Epoch 70, train_loss: 0.2081, val_loss: 0.3611\n",
      "Epoch 71, train_loss: 0.2176, val_loss: 0.3587\n",
      "Epoch 72, train_loss: 0.2071, val_loss: 0.3583\n",
      "Epoch 73, train_loss: 0.2093, val_loss: 0.3570\n",
      "Epoch 74, train_loss: 0.1973, val_loss: 0.3553\n",
      "Epoch 75, train_loss: 0.1999, val_loss: 0.3538\n",
      "Epoch 76, train_loss: 0.2009, val_loss: 0.3531\n",
      "Epoch 77, train_loss: 0.2034, val_loss: 0.3549\n",
      "Epoch 78, train_loss: 0.2024, val_loss: 0.3504\n",
      "Epoch 79, train_loss: 0.1867, val_loss: 0.3519\n",
      "Epoch 80, train_loss: 0.1961, val_loss: 0.3519\n",
      "Epoch 81, train_loss: 0.1812, val_loss: 0.3507\n",
      "Epoch 82, train_loss: 0.1915, val_loss: 0.3489\n",
      "Epoch 83, train_loss: 0.1841, val_loss: 0.3486\n",
      "Epoch 84, train_loss: 0.1850, val_loss: 0.3490\n",
      "Epoch 85, train_loss: 0.1801, val_loss: 0.3502\n",
      "Epoch 86, train_loss: 0.1783, val_loss: 0.3503\n",
      "Epoch 87, train_loss: 0.1840, val_loss: 0.3506\n",
      "Epoch 88, train_loss: 0.1703, val_loss: 0.3505\n",
      "Epoch 89, train_loss: 0.1724, val_loss: 0.3485\n",
      "Epoch 90, train_loss: 0.1755, val_loss: 0.3481\n",
      "Epoch 91, train_loss: 0.1737, val_loss: 0.3491\n",
      "Epoch 92, train_loss: 0.1656, val_loss: 0.3482\n",
      "Epoch 93, train_loss: 0.1590, val_loss: 0.3460\n",
      "Epoch 94, train_loss: 0.1837, val_loss: 0.3476\n",
      "Epoch 95, train_loss: 0.1657, val_loss: 0.3503\n",
      "Epoch 96, train_loss: 0.1590, val_loss: 0.3517\n",
      "Early Stopping!\n",
      "Test the model on fold 0:\n",
      "{'roc_auc': 0.9010416666666666, 'pr_auc': 0.9137487909713194, 'fmax': 0.8399950080296665}\n",
      "Starting Fold: 1\n",
      "Epoch 0, train_loss: 0.6883, val_loss: 0.6850\n",
      "Epoch 1, train_loss: 0.6704, val_loss: 0.6778\n",
      "Epoch 2, train_loss: 0.6532, val_loss: 0.6760\n",
      "Epoch 3, train_loss: 0.6452, val_loss: 0.6678\n",
      "Epoch 4, train_loss: 0.6405, val_loss: 0.6652\n",
      "Epoch 5, train_loss: 0.6312, val_loss: 0.6626\n",
      "Epoch 6, train_loss: 0.6189, val_loss: 0.6602\n",
      "Epoch 7, train_loss: 0.6102, val_loss: 0.6559\n",
      "Epoch 8, train_loss: 0.6098, val_loss: 0.6509\n",
      "Epoch 9, train_loss: 0.5996, val_loss: 0.6481\n",
      "Epoch 10, train_loss: 0.5921, val_loss: 0.6442\n",
      "Epoch 11, train_loss: 0.5774, val_loss: 0.6453\n",
      "Epoch 12, train_loss: 0.5738, val_loss: 0.6454\n",
      "Epoch 13, train_loss: 0.5597, val_loss: 0.6444\n",
      "Epoch 14, train_loss: 0.5569, val_loss: 0.6409\n",
      "Epoch 15, train_loss: 0.5538, val_loss: 0.6390\n",
      "Epoch 16, train_loss: 0.5438, val_loss: 0.6376\n",
      "Epoch 17, train_loss: 0.5317, val_loss: 0.6337\n",
      "Epoch 18, train_loss: 0.5262, val_loss: 0.6299\n",
      "Epoch 19, train_loss: 0.5222, val_loss: 0.6276\n",
      "Epoch 20, train_loss: 0.5096, val_loss: 0.6262\n",
      "Epoch 21, train_loss: 0.5066, val_loss: 0.6208\n",
      "Epoch 22, train_loss: 0.4928, val_loss: 0.6178\n",
      "Epoch 23, train_loss: 0.4822, val_loss: 0.6191\n",
      "Epoch 24, train_loss: 0.4781, val_loss: 0.6193\n",
      "Epoch 25, train_loss: 0.4779, val_loss: 0.6187\n",
      "Epoch 26, train_loss: 0.4733, val_loss: 0.6147\n",
      "Epoch 27, train_loss: 0.4597, val_loss: 0.6127\n",
      "Epoch 28, train_loss: 0.4407, val_loss: 0.6120\n",
      "Epoch 29, train_loss: 0.4384, val_loss: 0.6101\n",
      "Epoch 30, train_loss: 0.4350, val_loss: 0.6118\n",
      "Epoch 31, train_loss: 0.4269, val_loss: 0.6117\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32, train_loss: 0.4171, val_loss: 0.6109\n",
      "Epoch 33, train_loss: 0.4084, val_loss: 0.6069\n",
      "Epoch 34, train_loss: 0.4046, val_loss: 0.6019\n",
      "Epoch 35, train_loss: 0.3943, val_loss: 0.6004\n",
      "Epoch 36, train_loss: 0.3867, val_loss: 0.5979\n",
      "Epoch 37, train_loss: 0.3834, val_loss: 0.6015\n",
      "Epoch 38, train_loss: 0.3794, val_loss: 0.6026\n",
      "Epoch 39, train_loss: 0.3633, val_loss: 0.5983\n",
      "Epoch 40, train_loss: 0.3563, val_loss: 0.5964\n",
      "Epoch 41, train_loss: 0.3535, val_loss: 0.5951\n",
      "Epoch 42, train_loss: 0.3454, val_loss: 0.5977\n",
      "Epoch 43, train_loss: 0.3386, val_loss: 0.5981\n",
      "Epoch 44, train_loss: 0.3373, val_loss: 0.5938\n",
      "Epoch 45, train_loss: 0.3249, val_loss: 0.5947\n",
      "Epoch 46, train_loss: 0.3230, val_loss: 0.5968\n",
      "Epoch 47, train_loss: 0.3187, val_loss: 0.5967\n",
      "Epoch 48, train_loss: 0.3070, val_loss: 0.5953\n",
      "Epoch 49, train_loss: 0.2998, val_loss: 0.5965\n",
      "Epoch 50, train_loss: 0.2986, val_loss: 0.5990\n",
      "Epoch 51, train_loss: 0.2891, val_loss: 0.5982\n",
      "Epoch 52, train_loss: 0.2882, val_loss: 0.5978\n",
      "Epoch 53, train_loss: 0.2830, val_loss: 0.6001\n",
      "Epoch 54, train_loss: 0.2827, val_loss: 0.6008\n",
      "Early Stopping!\n",
      "Test the model on fold 1:\n",
      "{'roc_auc': 0.9000000000000001, 'pr_auc': 0.9515791926112114, 'fmax': 0.8965467479467637}\n",
      "Starting Fold: 2\n",
      "Epoch 0, train_loss: 0.6981, val_loss: 0.6912\n",
      "Epoch 1, train_loss: 0.6742, val_loss: 0.6877\n",
      "Epoch 2, train_loss: 0.6583, val_loss: 0.6815\n",
      "Epoch 3, train_loss: 0.6555, val_loss: 0.6758\n",
      "Epoch 4, train_loss: 0.6434, val_loss: 0.6705\n",
      "Epoch 5, train_loss: 0.6354, val_loss: 0.6656\n",
      "Epoch 6, train_loss: 0.6306, val_loss: 0.6608\n",
      "Epoch 7, train_loss: 0.6202, val_loss: 0.6565\n",
      "Epoch 8, train_loss: 0.6155, val_loss: 0.6548\n",
      "Epoch 9, train_loss: 0.6051, val_loss: 0.6514\n",
      "Epoch 10, train_loss: 0.6021, val_loss: 0.6477\n",
      "Epoch 11, train_loss: 0.5943, val_loss: 0.6442\n",
      "Epoch 12, train_loss: 0.5851, val_loss: 0.6407\n",
      "Epoch 13, train_loss: 0.5802, val_loss: 0.6369\n",
      "Epoch 14, train_loss: 0.5729, val_loss: 0.6334\n",
      "Epoch 15, train_loss: 0.5698, val_loss: 0.6289\n",
      "Epoch 16, train_loss: 0.5629, val_loss: 0.6230\n",
      "Epoch 17, train_loss: 0.5507, val_loss: 0.6185\n",
      "Epoch 18, train_loss: 0.5416, val_loss: 0.6151\n",
      "Epoch 19, train_loss: 0.5378, val_loss: 0.6106\n",
      "Epoch 20, train_loss: 0.5280, val_loss: 0.6062\n",
      "Epoch 21, train_loss: 0.5226, val_loss: 0.6028\n",
      "Epoch 22, train_loss: 0.5104, val_loss: 0.5990\n",
      "Epoch 23, train_loss: 0.5087, val_loss: 0.5944\n",
      "Epoch 24, train_loss: 0.5002, val_loss: 0.5889\n",
      "Epoch 25, train_loss: 0.4977, val_loss: 0.5857\n",
      "Epoch 26, train_loss: 0.4861, val_loss: 0.5836\n",
      "Epoch 27, train_loss: 0.4738, val_loss: 0.5795\n",
      "Epoch 28, train_loss: 0.4724, val_loss: 0.5745\n",
      "Epoch 29, train_loss: 0.4586, val_loss: 0.5709\n",
      "Epoch 30, train_loss: 0.4555, val_loss: 0.5672\n",
      "Epoch 31, train_loss: 0.4529, val_loss: 0.5665\n",
      "Epoch 32, train_loss: 0.4399, val_loss: 0.5627\n",
      "Epoch 33, train_loss: 0.4394, val_loss: 0.5593\n",
      "Epoch 34, train_loss: 0.4224, val_loss: 0.5571\n",
      "Epoch 35, train_loss: 0.4254, val_loss: 0.5517\n",
      "Epoch 36, train_loss: 0.4190, val_loss: 0.5493\n",
      "Epoch 37, train_loss: 0.4051, val_loss: 0.5484\n",
      "Epoch 38, train_loss: 0.4046, val_loss: 0.5456\n",
      "Epoch 39, train_loss: 0.3994, val_loss: 0.5451\n",
      "Epoch 40, train_loss: 0.3966, val_loss: 0.5432\n",
      "Epoch 41, train_loss: 0.3792, val_loss: 0.5408\n",
      "Epoch 42, train_loss: 0.3823, val_loss: 0.5360\n",
      "Epoch 43, train_loss: 0.3726, val_loss: 0.5307\n",
      "Epoch 44, train_loss: 0.3667, val_loss: 0.5279\n",
      "Epoch 45, train_loss: 0.3608, val_loss: 0.5291\n",
      "Epoch 46, train_loss: 0.3579, val_loss: 0.5280\n",
      "Epoch 47, train_loss: 0.3492, val_loss: 0.5259\n",
      "Epoch 48, train_loss: 0.3430, val_loss: 0.5240\n",
      "Epoch 49, train_loss: 0.3404, val_loss: 0.5212\n",
      "Epoch 50, train_loss: 0.3321, val_loss: 0.5178\n",
      "Epoch 51, train_loss: 0.3245, val_loss: 0.5175\n",
      "Epoch 52, train_loss: 0.3168, val_loss: 0.5155\n",
      "Epoch 53, train_loss: 0.3110, val_loss: 0.5157\n",
      "Epoch 54, train_loss: 0.3097, val_loss: 0.5157\n",
      "Epoch 55, train_loss: 0.3065, val_loss: 0.5152\n",
      "Epoch 56, train_loss: 0.3024, val_loss: 0.5140\n",
      "Epoch 57, train_loss: 0.2960, val_loss: 0.5094\n",
      "Epoch 58, train_loss: 0.2938, val_loss: 0.5073\n",
      "Epoch 59, train_loss: 0.2916, val_loss: 0.5065\n",
      "Epoch 60, train_loss: 0.2798, val_loss: 0.5094\n",
      "Epoch 61, train_loss: 0.2802, val_loss: 0.5090\n",
      "Epoch 62, train_loss: 0.2757, val_loss: 0.5078\n",
      "Epoch 63, train_loss: 0.2717, val_loss: 0.5082\n",
      "Epoch 64, train_loss: 0.2642, val_loss: 0.5092\n",
      "Epoch 65, train_loss: 0.2664, val_loss: 0.5099\n",
      "Epoch 66, train_loss: 0.2560, val_loss: 0.5072\n",
      "Epoch 67, train_loss: 0.2641, val_loss: 0.5046\n",
      "Epoch 68, train_loss: 0.2531, val_loss: 0.5035\n",
      "Epoch 69, train_loss: 0.2484, val_loss: 0.5046\n",
      "Epoch 70, train_loss: 0.2561, val_loss: 0.5027\n",
      "Epoch 71, train_loss: 0.2443, val_loss: 0.5055\n",
      "Epoch 72, train_loss: 0.2408, val_loss: 0.5108\n",
      "Early Stopping!\n",
      "Test the model on fold 2:\n",
      "{'roc_auc': 0.9567901234567902, 'pr_auc': 0.9634569864094216, 'fmax': 0.9130384877399427}\n",
      "Starting Fold: 3\n",
      "Epoch 0, train_loss: 0.6883, val_loss: 0.6919\n",
      "Epoch 1, train_loss: 0.6733, val_loss: 0.6872\n",
      "Epoch 2, train_loss: 0.6652, val_loss: 0.6792\n",
      "Epoch 3, train_loss: 0.6563, val_loss: 0.6759\n",
      "Epoch 4, train_loss: 0.6457, val_loss: 0.6709\n",
      "Epoch 5, train_loss: 0.6374, val_loss: 0.6622\n",
      "Epoch 6, train_loss: 0.6258, val_loss: 0.6576\n",
      "Epoch 7, train_loss: 0.6207, val_loss: 0.6559\n",
      "Epoch 8, train_loss: 0.6094, val_loss: 0.6536\n",
      "Epoch 9, train_loss: 0.6050, val_loss: 0.6456\n",
      "Epoch 10, train_loss: 0.6063, val_loss: 0.6380\n",
      "Epoch 11, train_loss: 0.5888, val_loss: 0.6353\n",
      "Epoch 12, train_loss: 0.5854, val_loss: 0.6312\n",
      "Epoch 13, train_loss: 0.5682, val_loss: 0.6247\n",
      "Epoch 14, train_loss: 0.5638, val_loss: 0.6201\n",
      "Epoch 15, train_loss: 0.5611, val_loss: 0.6153\n",
      "Epoch 16, train_loss: 0.5571, val_loss: 0.6116\n",
      "Epoch 17, train_loss: 0.5524, val_loss: 0.6064\n",
      "Epoch 18, train_loss: 0.5437, val_loss: 0.6015\n",
      "Epoch 19, train_loss: 0.5287, val_loss: 0.5971\n",
      "Epoch 20, train_loss: 0.5149, val_loss: 0.5956\n",
      "Epoch 21, train_loss: 0.5150, val_loss: 0.5916\n",
      "Epoch 22, train_loss: 0.5084, val_loss: 0.5891\n",
      "Epoch 23, train_loss: 0.5049, val_loss: 0.5866\n",
      "Epoch 24, train_loss: 0.4976, val_loss: 0.5828\n",
      "Epoch 25, train_loss: 0.4852, val_loss: 0.5768\n",
      "Epoch 26, train_loss: 0.4687, val_loss: 0.5745\n",
      "Epoch 27, train_loss: 0.4678, val_loss: 0.5735\n",
      "Epoch 28, train_loss: 0.4735, val_loss: 0.5690\n",
      "Epoch 29, train_loss: 0.4483, val_loss: 0.5661\n",
      "Epoch 30, train_loss: 0.4543, val_loss: 0.5598\n",
      "Epoch 31, train_loss: 0.4334, val_loss: 0.5549\n",
      "Epoch 32, train_loss: 0.4235, val_loss: 0.5507\n",
      "Epoch 33, train_loss: 0.4158, val_loss: 0.5492\n",
      "Epoch 34, train_loss: 0.4053, val_loss: 0.5430\n",
      "Epoch 35, train_loss: 0.4037, val_loss: 0.5404\n",
      "Epoch 36, train_loss: 0.3985, val_loss: 0.5378\n",
      "Epoch 37, train_loss: 0.3949, val_loss: 0.5363\n",
      "Epoch 38, train_loss: 0.3889, val_loss: 0.5310\n",
      "Epoch 39, train_loss: 0.3802, val_loss: 0.5288\n",
      "Epoch 40, train_loss: 0.3669, val_loss: 0.5265\n",
      "Epoch 41, train_loss: 0.3653, val_loss: 0.5234\n",
      "Epoch 42, train_loss: 0.3526, val_loss: 0.5161\n",
      "Epoch 43, train_loss: 0.3542, val_loss: 0.5141\n",
      "Epoch 44, train_loss: 0.3511, val_loss: 0.5186\n",
      "Epoch 45, train_loss: 0.3394, val_loss: 0.5143\n",
      "Epoch 46, train_loss: 0.3318, val_loss: 0.5136\n",
      "Epoch 47, train_loss: 0.3244, val_loss: 0.5079\n",
      "Epoch 48, train_loss: 0.3184, val_loss: 0.5053\n",
      "Epoch 49, train_loss: 0.3190, val_loss: 0.5029\n",
      "Epoch 50, train_loss: 0.3022, val_loss: 0.4976\n",
      "Epoch 51, train_loss: 0.3078, val_loss: 0.4998\n",
      "Epoch 52, train_loss: 0.3031, val_loss: 0.4904\n",
      "Epoch 53, train_loss: 0.2912, val_loss: 0.4917\n",
      "Epoch 54, train_loss: 0.2876, val_loss: 0.4910\n",
      "Epoch 55, train_loss: 0.2819, val_loss: 0.4918\n",
      "Epoch 56, train_loss: 0.2792, val_loss: 0.4920\n",
      "Epoch 57, train_loss: 0.2771, val_loss: 0.4918\n",
      "Epoch 58, train_loss: 0.2707, val_loss: 0.4914\n",
      "Epoch 59, train_loss: 0.2563, val_loss: 0.4864\n",
      "Epoch 60, train_loss: 0.2590, val_loss: 0.4822\n",
      "Epoch 61, train_loss: 0.2530, val_loss: 0.4822\n",
      "Epoch 62, train_loss: 0.2530, val_loss: 0.4830\n",
      "Epoch 63, train_loss: 0.2554, val_loss: 0.4795\n",
      "Epoch 64, train_loss: 0.2505, val_loss: 0.4753\n",
      "Epoch 65, train_loss: 0.2405, val_loss: 0.4763\n",
      "Epoch 66, train_loss: 0.2330, val_loss: 0.4783\n",
      "Epoch 67, train_loss: 0.2367, val_loss: 0.4785\n",
      "Epoch 68, train_loss: 0.2272, val_loss: 0.4774\n",
      "Epoch 69, train_loss: 0.2371, val_loss: 0.4674\n",
      "Epoch 70, train_loss: 0.2236, val_loss: 0.4698\n",
      "Epoch 71, train_loss: 0.2202, val_loss: 0.4689\n",
      "Epoch 72, train_loss: 0.2228, val_loss: 0.4676\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 73, train_loss: 0.2163, val_loss: 0.4656\n",
      "Epoch 74, train_loss: 0.2112, val_loss: 0.4615\n",
      "Epoch 75, train_loss: 0.2072, val_loss: 0.4643\n",
      "Epoch 76, train_loss: 0.2138, val_loss: 0.4639\n",
      "Epoch 77, train_loss: 0.2021, val_loss: 0.4638\n",
      "Epoch 78, train_loss: 0.2024, val_loss: 0.4618\n",
      "Epoch 79, train_loss: 0.2058, val_loss: 0.4660\n",
      "Epoch 80, train_loss: 0.1950, val_loss: 0.4636\n",
      "Epoch 81, train_loss: 0.1966, val_loss: 0.4592\n",
      "Epoch 82, train_loss: 0.1963, val_loss: 0.4629\n",
      "Epoch 83, train_loss: 0.1918, val_loss: 0.4585\n",
      "Epoch 84, train_loss: 0.1882, val_loss: 0.4609\n",
      "Epoch 85, train_loss: 0.1878, val_loss: 0.4528\n",
      "Epoch 86, train_loss: 0.1847, val_loss: 0.4535\n",
      "Epoch 87, train_loss: 0.1938, val_loss: 0.4563\n",
      "Epoch 88, train_loss: 0.1816, val_loss: 0.4549\n",
      "Epoch 89, train_loss: 0.1830, val_loss: 0.4575\n",
      "Epoch 90, train_loss: 0.1776, val_loss: 0.4572\n",
      "Epoch 91, train_loss: 0.1854, val_loss: 0.4579\n",
      "Epoch 92, train_loss: 0.1763, val_loss: 0.4564\n",
      "Epoch 93, train_loss: 0.1683, val_loss: 0.4506\n",
      "Epoch 94, train_loss: 0.1784, val_loss: 0.4496\n",
      "Epoch 95, train_loss: 0.1737, val_loss: 0.4489\n",
      "Epoch 96, train_loss: 0.1701, val_loss: 0.4463\n",
      "Epoch 97, train_loss: 0.1739, val_loss: 0.4492\n",
      "Epoch 98, train_loss: 0.1683, val_loss: 0.4540\n",
      "Epoch 99, train_loss: 0.1656, val_loss: 0.4527\n",
      "Test the model on fold 3:\n",
      "{'roc_auc': 0.8692307692307691, 'pr_auc': 0.9049387474673609, 'fmax': 0.8181769111864542}\n",
      "Starting Fold: 4\n",
      "Epoch 0, train_loss: 0.6893, val_loss: 0.6819\n",
      "Epoch 1, train_loss: 0.6737, val_loss: 0.6663\n",
      "Epoch 2, train_loss: 0.6603, val_loss: 0.6525\n",
      "Epoch 3, train_loss: 0.6490, val_loss: 0.6481\n",
      "Epoch 4, train_loss: 0.6419, val_loss: 0.6420\n",
      "Epoch 5, train_loss: 0.6319, val_loss: 0.6342\n",
      "Epoch 6, train_loss: 0.6236, val_loss: 0.6277\n",
      "Epoch 7, train_loss: 0.6195, val_loss: 0.6204\n",
      "Epoch 8, train_loss: 0.6075, val_loss: 0.6137\n",
      "Epoch 9, train_loss: 0.5956, val_loss: 0.6073\n",
      "Epoch 10, train_loss: 0.5884, val_loss: 0.6000\n",
      "Epoch 11, train_loss: 0.5795, val_loss: 0.5932\n",
      "Epoch 12, train_loss: 0.5742, val_loss: 0.5860\n",
      "Epoch 13, train_loss: 0.5654, val_loss: 0.5798\n",
      "Epoch 14, train_loss: 0.5504, val_loss: 0.5721\n",
      "Epoch 15, train_loss: 0.5504, val_loss: 0.5651\n",
      "Epoch 16, train_loss: 0.5317, val_loss: 0.5596\n",
      "Epoch 17, train_loss: 0.5352, val_loss: 0.5540\n",
      "Epoch 18, train_loss: 0.5223, val_loss: 0.5473\n",
      "Epoch 19, train_loss: 0.5086, val_loss: 0.5420\n",
      "Epoch 20, train_loss: 0.5077, val_loss: 0.5381\n",
      "Epoch 21, train_loss: 0.5036, val_loss: 0.5316\n",
      "Epoch 22, train_loss: 0.4928, val_loss: 0.5260\n",
      "Epoch 23, train_loss: 0.4853, val_loss: 0.5208\n",
      "Epoch 24, train_loss: 0.4760, val_loss: 0.5166\n",
      "Epoch 25, train_loss: 0.4742, val_loss: 0.5126\n",
      "Epoch 26, train_loss: 0.4614, val_loss: 0.5077\n",
      "Epoch 27, train_loss: 0.4603, val_loss: 0.5039\n",
      "Epoch 28, train_loss: 0.4516, val_loss: 0.5009\n",
      "Epoch 29, train_loss: 0.4400, val_loss: 0.4960\n",
      "Epoch 30, train_loss: 0.4324, val_loss: 0.4920\n",
      "Epoch 31, train_loss: 0.4236, val_loss: 0.4881\n",
      "Epoch 32, train_loss: 0.4151, val_loss: 0.4858\n",
      "Epoch 33, train_loss: 0.4019, val_loss: 0.4821\n",
      "Epoch 34, train_loss: 0.4052, val_loss: 0.4806\n",
      "Epoch 35, train_loss: 0.3977, val_loss: 0.4771\n",
      "Epoch 36, train_loss: 0.3838, val_loss: 0.4742\n",
      "Epoch 37, train_loss: 0.3758, val_loss: 0.4724\n",
      "Epoch 38, train_loss: 0.3651, val_loss: 0.4712\n",
      "Epoch 39, train_loss: 0.3665, val_loss: 0.4691\n",
      "Epoch 40, train_loss: 0.3599, val_loss: 0.4662\n",
      "Epoch 41, train_loss: 0.3511, val_loss: 0.4654\n",
      "Epoch 42, train_loss: 0.3414, val_loss: 0.4650\n",
      "Epoch 43, train_loss: 0.3407, val_loss: 0.4621\n",
      "Epoch 44, train_loss: 0.3343, val_loss: 0.4620\n",
      "Epoch 45, train_loss: 0.3321, val_loss: 0.4600\n",
      "Epoch 46, train_loss: 0.3271, val_loss: 0.4611\n",
      "Epoch 47, train_loss: 0.3105, val_loss: 0.4615\n",
      "Epoch 48, train_loss: 0.3045, val_loss: 0.4558\n",
      "Epoch 49, train_loss: 0.3077, val_loss: 0.4542\n",
      "Epoch 50, train_loss: 0.3037, val_loss: 0.4568\n",
      "Epoch 51, train_loss: 0.2963, val_loss: 0.4537\n",
      "Epoch 52, train_loss: 0.2881, val_loss: 0.4562\n",
      "Epoch 53, train_loss: 0.2799, val_loss: 0.4563\n",
      "Epoch 54, train_loss: 0.2717, val_loss: 0.4559\n",
      "Epoch 55, train_loss: 0.2666, val_loss: 0.4535\n",
      "Epoch 56, train_loss: 0.2696, val_loss: 0.4543\n",
      "Epoch 57, train_loss: 0.2619, val_loss: 0.4551\n",
      "Epoch 58, train_loss: 0.2541, val_loss: 0.4519\n",
      "Epoch 59, train_loss: 0.2574, val_loss: 0.4512\n",
      "Epoch 60, train_loss: 0.2472, val_loss: 0.4518\n",
      "Epoch 61, train_loss: 0.2479, val_loss: 0.4549\n",
      "Epoch 62, train_loss: 0.2494, val_loss: 0.4535\n",
      "Epoch 63, train_loss: 0.2378, val_loss: 0.4531\n",
      "Epoch 64, train_loss: 0.2384, val_loss: 0.4545\n",
      "Epoch 65, train_loss: 0.2304, val_loss: 0.4545\n",
      "Epoch 66, train_loss: 0.2231, val_loss: 0.4565\n",
      "Early Stopping!\n",
      "Test the model on fold 4:\n",
      "{'roc_auc': 0.8194444444444444, 'pr_auc': 0.8252988874632342, 'fmax': 0.7999950080311499}\n",
      "Evaluate pretrained model on disease class Muscular (11/12)\n",
      "Starting Fold: 0\n",
      "Epoch 0, train_loss: 0.6872, val_loss: 0.6947\n",
      "Epoch 1, train_loss: 0.6810, val_loss: 0.6928\n",
      "Epoch 2, train_loss: 0.6690, val_loss: 0.6905\n",
      "Epoch 3, train_loss: 0.6731, val_loss: 0.6878\n",
      "Epoch 4, train_loss: 0.6587, val_loss: 0.6844\n",
      "Epoch 5, train_loss: 0.6544, val_loss: 0.6800\n",
      "Epoch 6, train_loss: 0.6422, val_loss: 0.6771\n",
      "Epoch 7, train_loss: 0.6394, val_loss: 0.6741\n",
      "Epoch 8, train_loss: 0.6254, val_loss: 0.6696\n",
      "Epoch 9, train_loss: 0.6304, val_loss: 0.6658\n",
      "Epoch 10, train_loss: 0.6198, val_loss: 0.6634\n",
      "Epoch 11, train_loss: 0.6159, val_loss: 0.6608\n",
      "Epoch 12, train_loss: 0.6118, val_loss: 0.6576\n",
      "Epoch 13, train_loss: 0.5933, val_loss: 0.6539\n",
      "Epoch 14, train_loss: 0.6005, val_loss: 0.6502\n",
      "Epoch 15, train_loss: 0.5829, val_loss: 0.6465\n",
      "Epoch 16, train_loss: 0.5815, val_loss: 0.6431\n",
      "Epoch 17, train_loss: 0.5854, val_loss: 0.6407\n",
      "Epoch 18, train_loss: 0.5723, val_loss: 0.6379\n",
      "Epoch 19, train_loss: 0.5638, val_loss: 0.6341\n",
      "Epoch 20, train_loss: 0.5590, val_loss: 0.6303\n",
      "Epoch 21, train_loss: 0.5496, val_loss: 0.6261\n",
      "Epoch 22, train_loss: 0.5351, val_loss: 0.6223\n",
      "Epoch 23, train_loss: 0.5318, val_loss: 0.6189\n",
      "Epoch 24, train_loss: 0.5320, val_loss: 0.6160\n",
      "Epoch 25, train_loss: 0.5276, val_loss: 0.6126\n",
      "Epoch 26, train_loss: 0.5146, val_loss: 0.6096\n",
      "Epoch 27, train_loss: 0.5060, val_loss: 0.6065\n",
      "Epoch 28, train_loss: 0.5012, val_loss: 0.6028\n",
      "Epoch 29, train_loss: 0.4972, val_loss: 0.5999\n",
      "Epoch 30, train_loss: 0.4949, val_loss: 0.5967\n",
      "Epoch 31, train_loss: 0.4907, val_loss: 0.5933\n",
      "Epoch 32, train_loss: 0.4766, val_loss: 0.5906\n",
      "Epoch 33, train_loss: 0.4636, val_loss: 0.5877\n",
      "Epoch 34, train_loss: 0.4679, val_loss: 0.5843\n",
      "Epoch 35, train_loss: 0.4613, val_loss: 0.5807\n",
      "Epoch 36, train_loss: 0.4529, val_loss: 0.5777\n",
      "Epoch 37, train_loss: 0.4446, val_loss: 0.5744\n",
      "Epoch 38, train_loss: 0.4447, val_loss: 0.5713\n",
      "Epoch 39, train_loss: 0.4459, val_loss: 0.5681\n",
      "Epoch 40, train_loss: 0.4360, val_loss: 0.5650\n",
      "Epoch 41, train_loss: 0.4298, val_loss: 0.5615\n",
      "Epoch 42, train_loss: 0.4191, val_loss: 0.5579\n",
      "Epoch 43, train_loss: 0.4161, val_loss: 0.5556\n",
      "Epoch 44, train_loss: 0.4047, val_loss: 0.5529\n",
      "Epoch 45, train_loss: 0.3928, val_loss: 0.5500\n",
      "Epoch 46, train_loss: 0.3987, val_loss: 0.5460\n",
      "Epoch 47, train_loss: 0.3937, val_loss: 0.5428\n",
      "Epoch 48, train_loss: 0.3859, val_loss: 0.5383\n",
      "Epoch 49, train_loss: 0.3830, val_loss: 0.5355\n",
      "Epoch 50, train_loss: 0.3724, val_loss: 0.5317\n",
      "Epoch 51, train_loss: 0.3600, val_loss: 0.5290\n",
      "Epoch 52, train_loss: 0.3577, val_loss: 0.5255\n",
      "Epoch 53, train_loss: 0.3571, val_loss: 0.5235\n",
      "Epoch 54, train_loss: 0.3415, val_loss: 0.5202\n",
      "Epoch 55, train_loss: 0.3416, val_loss: 0.5169\n",
      "Epoch 56, train_loss: 0.3385, val_loss: 0.5127\n",
      "Epoch 57, train_loss: 0.3253, val_loss: 0.5099\n",
      "Epoch 58, train_loss: 0.3365, val_loss: 0.5075\n",
      "Epoch 59, train_loss: 0.3165, val_loss: 0.5051\n",
      "Epoch 60, train_loss: 0.3062, val_loss: 0.5022\n",
      "Epoch 61, train_loss: 0.3184, val_loss: 0.4988\n",
      "Epoch 62, train_loss: 0.3167, val_loss: 0.4966\n",
      "Epoch 63, train_loss: 0.3108, val_loss: 0.4945\n",
      "Epoch 64, train_loss: 0.2930, val_loss: 0.4913\n",
      "Epoch 65, train_loss: 0.2858, val_loss: 0.4894\n",
      "Epoch 66, train_loss: 0.2781, val_loss: 0.4866\n",
      "Epoch 67, train_loss: 0.2784, val_loss: 0.4839\n",
      "Epoch 68, train_loss: 0.2821, val_loss: 0.4815\n",
      "Epoch 69, train_loss: 0.2733, val_loss: 0.4783\n",
      "Epoch 70, train_loss: 0.2698, val_loss: 0.4757\n",
      "Epoch 71, train_loss: 0.2659, val_loss: 0.4730\n",
      "Epoch 72, train_loss: 0.2639, val_loss: 0.4718\n",
      "Epoch 73, train_loss: 0.2608, val_loss: 0.4687\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 74, train_loss: 0.2515, val_loss: 0.4667\n",
      "Epoch 75, train_loss: 0.2546, val_loss: 0.4643\n",
      "Epoch 76, train_loss: 0.2450, val_loss: 0.4617\n",
      "Epoch 77, train_loss: 0.2412, val_loss: 0.4587\n",
      "Epoch 78, train_loss: 0.2442, val_loss: 0.4571\n",
      "Epoch 79, train_loss: 0.2425, val_loss: 0.4560\n",
      "Epoch 80, train_loss: 0.2388, val_loss: 0.4555\n",
      "Epoch 81, train_loss: 0.2372, val_loss: 0.4541\n",
      "Epoch 82, train_loss: 0.2261, val_loss: 0.4534\n",
      "Epoch 83, train_loss: 0.2210, val_loss: 0.4516\n",
      "Epoch 84, train_loss: 0.2142, val_loss: 0.4503\n",
      "Epoch 85, train_loss: 0.2143, val_loss: 0.4468\n",
      "Epoch 86, train_loss: 0.2265, val_loss: 0.4458\n",
      "Epoch 87, train_loss: 0.2142, val_loss: 0.4445\n",
      "Epoch 88, train_loss: 0.2090, val_loss: 0.4424\n",
      "Epoch 89, train_loss: 0.2025, val_loss: 0.4409\n",
      "Epoch 90, train_loss: 0.2087, val_loss: 0.4397\n",
      "Epoch 91, train_loss: 0.1978, val_loss: 0.4376\n",
      "Epoch 92, train_loss: 0.2115, val_loss: 0.4355\n",
      "Epoch 93, train_loss: 0.2087, val_loss: 0.4330\n",
      "Epoch 94, train_loss: 0.1934, val_loss: 0.4334\n",
      "Epoch 95, train_loss: 0.1990, val_loss: 0.4335\n",
      "Epoch 96, train_loss: 0.1966, val_loss: 0.4329\n",
      "Epoch 97, train_loss: 0.2034, val_loss: 0.4310\n",
      "Epoch 98, train_loss: 0.1967, val_loss: 0.4301\n",
      "Epoch 99, train_loss: 0.1930, val_loss: 0.4293\n",
      "Test the model on fold 0:\n",
      "{'roc_auc': 0.8035714285714285, 'pr_auc': 0.8979677942677456, 'fmax': 0.833328395090992}\n",
      "Starting Fold: 1\n",
      "Epoch 0, train_loss: 0.6954, val_loss: 0.6895\n",
      "Epoch 1, train_loss: 0.6817, val_loss: 0.6767\n",
      "Epoch 2, train_loss: 0.6715, val_loss: 0.6650\n",
      "Epoch 3, train_loss: 0.6562, val_loss: 0.6534\n",
      "Epoch 4, train_loss: 0.6549, val_loss: 0.6441\n",
      "Epoch 5, train_loss: 0.6483, val_loss: 0.6330\n",
      "Epoch 6, train_loss: 0.6339, val_loss: 0.6266\n",
      "Epoch 7, train_loss: 0.6246, val_loss: 0.6197\n",
      "Epoch 8, train_loss: 0.6236, val_loss: 0.6119\n",
      "Epoch 9, train_loss: 0.6140, val_loss: 0.6048\n",
      "Epoch 10, train_loss: 0.6095, val_loss: 0.5961\n",
      "Epoch 11, train_loss: 0.5996, val_loss: 0.5877\n",
      "Epoch 12, train_loss: 0.5957, val_loss: 0.5811\n",
      "Epoch 13, train_loss: 0.5974, val_loss: 0.5764\n",
      "Epoch 14, train_loss: 0.5809, val_loss: 0.5718\n",
      "Epoch 15, train_loss: 0.5811, val_loss: 0.5649\n",
      "Epoch 16, train_loss: 0.5728, val_loss: 0.5589\n",
      "Epoch 17, train_loss: 0.5656, val_loss: 0.5521\n",
      "Epoch 18, train_loss: 0.5717, val_loss: 0.5449\n",
      "Epoch 19, train_loss: 0.5592, val_loss: 0.5387\n",
      "Epoch 20, train_loss: 0.5541, val_loss: 0.5342\n",
      "Epoch 21, train_loss: 0.5557, val_loss: 0.5296\n",
      "Epoch 22, train_loss: 0.5432, val_loss: 0.5258\n",
      "Epoch 23, train_loss: 0.5382, val_loss: 0.5209\n",
      "Epoch 24, train_loss: 0.5423, val_loss: 0.5163\n",
      "Epoch 25, train_loss: 0.5334, val_loss: 0.5115\n",
      "Epoch 26, train_loss: 0.5208, val_loss: 0.5049\n",
      "Epoch 27, train_loss: 0.5130, val_loss: 0.5002\n",
      "Epoch 28, train_loss: 0.5159, val_loss: 0.4946\n",
      "Epoch 29, train_loss: 0.5156, val_loss: 0.4898\n",
      "Epoch 30, train_loss: 0.5085, val_loss: 0.4850\n",
      "Epoch 31, train_loss: 0.5027, val_loss: 0.4822\n",
      "Epoch 32, train_loss: 0.4981, val_loss: 0.4792\n",
      "Epoch 33, train_loss: 0.4832, val_loss: 0.4763\n",
      "Epoch 34, train_loss: 0.4890, val_loss: 0.4718\n",
      "Epoch 35, train_loss: 0.4710, val_loss: 0.4682\n",
      "Epoch 36, train_loss: 0.4696, val_loss: 0.4637\n",
      "Epoch 37, train_loss: 0.4687, val_loss: 0.4583\n",
      "Epoch 38, train_loss: 0.4616, val_loss: 0.4541\n",
      "Epoch 39, train_loss: 0.4634, val_loss: 0.4503\n",
      "Epoch 40, train_loss: 0.4586, val_loss: 0.4467\n",
      "Epoch 41, train_loss: 0.4400, val_loss: 0.4425\n",
      "Epoch 42, train_loss: 0.4463, val_loss: 0.4389\n",
      "Epoch 43, train_loss: 0.4398, val_loss: 0.4352\n",
      "Epoch 44, train_loss: 0.4270, val_loss: 0.4328\n",
      "Epoch 45, train_loss: 0.4289, val_loss: 0.4297\n",
      "Epoch 46, train_loss: 0.4239, val_loss: 0.4282\n",
      "Epoch 47, train_loss: 0.4067, val_loss: 0.4256\n",
      "Epoch 48, train_loss: 0.4097, val_loss: 0.4219\n",
      "Epoch 49, train_loss: 0.4035, val_loss: 0.4184\n",
      "Epoch 50, train_loss: 0.4054, val_loss: 0.4149\n",
      "Epoch 51, train_loss: 0.3918, val_loss: 0.4135\n",
      "Epoch 52, train_loss: 0.3846, val_loss: 0.4103\n",
      "Epoch 53, train_loss: 0.3862, val_loss: 0.4088\n",
      "Epoch 54, train_loss: 0.3855, val_loss: 0.4050\n",
      "Epoch 55, train_loss: 0.3697, val_loss: 0.4047\n",
      "Epoch 56, train_loss: 0.3646, val_loss: 0.4009\n",
      "Epoch 57, train_loss: 0.3641, val_loss: 0.3979\n",
      "Epoch 58, train_loss: 0.3522, val_loss: 0.3951\n",
      "Epoch 59, train_loss: 0.3529, val_loss: 0.3923\n",
      "Epoch 60, train_loss: 0.3498, val_loss: 0.3889\n",
      "Epoch 61, train_loss: 0.3395, val_loss: 0.3872\n",
      "Epoch 62, train_loss: 0.3396, val_loss: 0.3844\n",
      "Epoch 63, train_loss: 0.3316, val_loss: 0.3822\n",
      "Epoch 64, train_loss: 0.3256, val_loss: 0.3784\n",
      "Epoch 65, train_loss: 0.3224, val_loss: 0.3768\n",
      "Epoch 66, train_loss: 0.3184, val_loss: 0.3736\n",
      "Epoch 67, train_loss: 0.3183, val_loss: 0.3719\n",
      "Epoch 68, train_loss: 0.3122, val_loss: 0.3698\n",
      "Epoch 69, train_loss: 0.3069, val_loss: 0.3672\n",
      "Epoch 70, train_loss: 0.3010, val_loss: 0.3672\n",
      "Epoch 71, train_loss: 0.2961, val_loss: 0.3659\n",
      "Epoch 72, train_loss: 0.2971, val_loss: 0.3635\n",
      "Epoch 73, train_loss: 0.2901, val_loss: 0.3613\n",
      "Epoch 74, train_loss: 0.2832, val_loss: 0.3595\n",
      "Epoch 75, train_loss: 0.2814, val_loss: 0.3577\n",
      "Epoch 76, train_loss: 0.2821, val_loss: 0.3577\n",
      "Epoch 77, train_loss: 0.2691, val_loss: 0.3576\n",
      "Epoch 78, train_loss: 0.2640, val_loss: 0.3562\n",
      "Epoch 79, train_loss: 0.2624, val_loss: 0.3526\n",
      "Epoch 80, train_loss: 0.2694, val_loss: 0.3511\n",
      "Epoch 81, train_loss: 0.2421, val_loss: 0.3502\n",
      "Epoch 82, train_loss: 0.2531, val_loss: 0.3489\n",
      "Epoch 83, train_loss: 0.2501, val_loss: 0.3478\n",
      "Epoch 84, train_loss: 0.2510, val_loss: 0.3466\n",
      "Epoch 85, train_loss: 0.2385, val_loss: 0.3444\n",
      "Epoch 86, train_loss: 0.2402, val_loss: 0.3435\n",
      "Epoch 87, train_loss: 0.2417, val_loss: 0.3434\n",
      "Epoch 88, train_loss: 0.2349, val_loss: 0.3426\n",
      "Epoch 89, train_loss: 0.2268, val_loss: 0.3429\n",
      "Epoch 90, train_loss: 0.2348, val_loss: 0.3393\n",
      "Epoch 91, train_loss: 0.2217, val_loss: 0.3369\n",
      "Epoch 92, train_loss: 0.2179, val_loss: 0.3364\n",
      "Epoch 93, train_loss: 0.2178, val_loss: 0.3359\n",
      "Epoch 94, train_loss: 0.2224, val_loss: 0.3332\n",
      "Epoch 95, train_loss: 0.2075, val_loss: 0.3351\n",
      "Epoch 96, train_loss: 0.2143, val_loss: 0.3347\n",
      "Epoch 97, train_loss: 0.2132, val_loss: 0.3336\n",
      "Epoch 98, train_loss: 0.2076, val_loss: 0.3326\n",
      "Epoch 99, train_loss: 0.2011, val_loss: 0.3314\n",
      "Test the model on fold 1:\n",
      "{'roc_auc': 0.8038194444444444, 'pr_auc': 0.7628872246856261, 'fmax': 0.7999950367654861}\n",
      "Starting Fold: 2\n",
      "Epoch 0, train_loss: 0.6947, val_loss: 0.6888\n",
      "Epoch 1, train_loss: 0.6792, val_loss: 0.6759\n",
      "Epoch 2, train_loss: 0.6744, val_loss: 0.6601\n",
      "Epoch 3, train_loss: 0.6619, val_loss: 0.6502\n",
      "Epoch 4, train_loss: 0.6511, val_loss: 0.6415\n",
      "Epoch 5, train_loss: 0.6412, val_loss: 0.6317\n",
      "Epoch 6, train_loss: 0.6378, val_loss: 0.6235\n",
      "Epoch 7, train_loss: 0.6328, val_loss: 0.6141\n",
      "Epoch 8, train_loss: 0.6239, val_loss: 0.6064\n",
      "Epoch 9, train_loss: 0.6236, val_loss: 0.5968\n",
      "Epoch 10, train_loss: 0.6159, val_loss: 0.5912\n",
      "Epoch 11, train_loss: 0.6073, val_loss: 0.5848\n",
      "Epoch 12, train_loss: 0.6062, val_loss: 0.5780\n",
      "Epoch 13, train_loss: 0.5860, val_loss: 0.5724\n",
      "Epoch 14, train_loss: 0.5896, val_loss: 0.5657\n",
      "Epoch 15, train_loss: 0.5856, val_loss: 0.5592\n",
      "Epoch 16, train_loss: 0.5779, val_loss: 0.5523\n",
      "Epoch 17, train_loss: 0.5689, val_loss: 0.5472\n",
      "Epoch 18, train_loss: 0.5642, val_loss: 0.5393\n",
      "Epoch 19, train_loss: 0.5668, val_loss: 0.5329\n",
      "Epoch 20, train_loss: 0.5545, val_loss: 0.5282\n",
      "Epoch 21, train_loss: 0.5499, val_loss: 0.5242\n",
      "Epoch 22, train_loss: 0.5407, val_loss: 0.5204\n",
      "Epoch 23, train_loss: 0.5453, val_loss: 0.5164\n",
      "Epoch 24, train_loss: 0.5271, val_loss: 0.5128\n",
      "Epoch 25, train_loss: 0.5304, val_loss: 0.5078\n",
      "Epoch 26, train_loss: 0.5219, val_loss: 0.5039\n",
      "Epoch 27, train_loss: 0.5152, val_loss: 0.4998\n",
      "Epoch 28, train_loss: 0.5130, val_loss: 0.4958\n",
      "Epoch 29, train_loss: 0.5021, val_loss: 0.4915\n",
      "Epoch 30, train_loss: 0.4903, val_loss: 0.4884\n",
      "Epoch 31, train_loss: 0.4920, val_loss: 0.4846\n",
      "Epoch 32, train_loss: 0.4820, val_loss: 0.4825\n",
      "Epoch 33, train_loss: 0.4788, val_loss: 0.4795\n",
      "Epoch 34, train_loss: 0.4870, val_loss: 0.4759\n",
      "Epoch 35, train_loss: 0.4693, val_loss: 0.4730\n",
      "Epoch 36, train_loss: 0.4704, val_loss: 0.4702\n",
      "Epoch 37, train_loss: 0.4579, val_loss: 0.4681\n",
      "Epoch 38, train_loss: 0.4557, val_loss: 0.4657\n",
      "Epoch 39, train_loss: 0.4406, val_loss: 0.4625\n",
      "Epoch 40, train_loss: 0.4395, val_loss: 0.4608\n",
      "Epoch 41, train_loss: 0.4373, val_loss: 0.4586\n",
      "Epoch 42, train_loss: 0.4371, val_loss: 0.4556\n",
      "Epoch 43, train_loss: 0.4173, val_loss: 0.4532\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 44, train_loss: 0.4186, val_loss: 0.4504\n",
      "Epoch 45, train_loss: 0.4108, val_loss: 0.4478\n",
      "Epoch 46, train_loss: 0.4098, val_loss: 0.4445\n",
      "Epoch 47, train_loss: 0.3990, val_loss: 0.4423\n",
      "Epoch 48, train_loss: 0.4009, val_loss: 0.4389\n",
      "Epoch 49, train_loss: 0.3840, val_loss: 0.4375\n",
      "Epoch 50, train_loss: 0.3874, val_loss: 0.4349\n",
      "Epoch 51, train_loss: 0.3712, val_loss: 0.4319\n",
      "Epoch 52, train_loss: 0.3673, val_loss: 0.4293\n",
      "Epoch 53, train_loss: 0.3598, val_loss: 0.4286\n",
      "Epoch 54, train_loss: 0.3512, val_loss: 0.4266\n",
      "Epoch 55, train_loss: 0.3496, val_loss: 0.4246\n",
      "Epoch 56, train_loss: 0.3436, val_loss: 0.4221\n",
      "Epoch 57, train_loss: 0.3385, val_loss: 0.4207\n",
      "Epoch 58, train_loss: 0.3256, val_loss: 0.4184\n",
      "Epoch 59, train_loss: 0.3174, val_loss: 0.4164\n",
      "Epoch 60, train_loss: 0.3170, val_loss: 0.4141\n",
      "Epoch 61, train_loss: 0.3116, val_loss: 0.4135\n",
      "Epoch 62, train_loss: 0.3068, val_loss: 0.4119\n",
      "Epoch 63, train_loss: 0.2998, val_loss: 0.4098\n",
      "Epoch 64, train_loss: 0.2945, val_loss: 0.4071\n",
      "Epoch 65, train_loss: 0.2983, val_loss: 0.4044\n",
      "Epoch 66, train_loss: 0.2904, val_loss: 0.4021\n",
      "Epoch 67, train_loss: 0.2884, val_loss: 0.4009\n",
      "Epoch 68, train_loss: 0.2785, val_loss: 0.4003\n",
      "Epoch 69, train_loss: 0.2742, val_loss: 0.3978\n",
      "Epoch 70, train_loss: 0.2786, val_loss: 0.3961\n",
      "Epoch 71, train_loss: 0.2641, val_loss: 0.3956\n",
      "Epoch 72, train_loss: 0.2614, val_loss: 0.3947\n",
      "Epoch 73, train_loss: 0.2604, val_loss: 0.3921\n",
      "Epoch 74, train_loss: 0.2542, val_loss: 0.3920\n",
      "Epoch 75, train_loss: 0.2516, val_loss: 0.3912\n",
      "Epoch 76, train_loss: 0.2466, val_loss: 0.3902\n",
      "Epoch 77, train_loss: 0.2410, val_loss: 0.3891\n",
      "Epoch 78, train_loss: 0.2436, val_loss: 0.3901\n",
      "Epoch 79, train_loss: 0.2294, val_loss: 0.3893\n",
      "Epoch 80, train_loss: 0.2351, val_loss: 0.3887\n",
      "Epoch 81, train_loss: 0.2358, val_loss: 0.3880\n",
      "Epoch 82, train_loss: 0.2185, val_loss: 0.3872\n",
      "Epoch 83, train_loss: 0.2218, val_loss: 0.3865\n",
      "Epoch 84, train_loss: 0.2152, val_loss: 0.3846\n",
      "Epoch 85, train_loss: 0.2198, val_loss: 0.3838\n",
      "Epoch 86, train_loss: 0.2116, val_loss: 0.3832\n",
      "Epoch 87, train_loss: 0.2156, val_loss: 0.3828\n",
      "Epoch 88, train_loss: 0.2093, val_loss: 0.3827\n",
      "Epoch 89, train_loss: 0.2129, val_loss: 0.3828\n",
      "Epoch 90, train_loss: 0.2058, val_loss: 0.3838\n",
      "Epoch 91, train_loss: 0.2013, val_loss: 0.3842\n",
      "Epoch 92, train_loss: 0.2066, val_loss: 0.3840\n",
      "Epoch 93, train_loss: 0.1983, val_loss: 0.3820\n",
      "Epoch 94, train_loss: 0.2041, val_loss: 0.3798\n",
      "Epoch 95, train_loss: 0.1877, val_loss: 0.3794\n",
      "Epoch 96, train_loss: 0.1956, val_loss: 0.3788\n",
      "Epoch 97, train_loss: 0.1977, val_loss: 0.3791\n",
      "Epoch 98, train_loss: 0.1858, val_loss: 0.3779\n",
      "Epoch 99, train_loss: 0.1851, val_loss: 0.3777\n",
      "Test the model on fold 2:\n",
      "{'roc_auc': 0.8719723183391004, 'pr_auc': 0.8383705072061712, 'fmax': 0.8648598977641007}\n",
      "Starting Fold: 3\n",
      "Epoch 0, train_loss: 0.6950, val_loss: 0.6862\n",
      "Epoch 1, train_loss: 0.6865, val_loss: 0.6722\n",
      "Epoch 2, train_loss: 0.6711, val_loss: 0.6614\n",
      "Epoch 3, train_loss: 0.6605, val_loss: 0.6511\n",
      "Epoch 4, train_loss: 0.6504, val_loss: 0.6405\n",
      "Epoch 5, train_loss: 0.6449, val_loss: 0.6314\n",
      "Epoch 6, train_loss: 0.6408, val_loss: 0.6245\n",
      "Epoch 7, train_loss: 0.6316, val_loss: 0.6178\n",
      "Epoch 8, train_loss: 0.6292, val_loss: 0.6091\n",
      "Epoch 9, train_loss: 0.6245, val_loss: 0.6012\n",
      "Epoch 10, train_loss: 0.6123, val_loss: 0.5940\n",
      "Epoch 11, train_loss: 0.6053, val_loss: 0.5873\n",
      "Epoch 12, train_loss: 0.6085, val_loss: 0.5803\n",
      "Epoch 13, train_loss: 0.5984, val_loss: 0.5745\n",
      "Epoch 14, train_loss: 0.6054, val_loss: 0.5687\n",
      "Epoch 15, train_loss: 0.5899, val_loss: 0.5631\n",
      "Epoch 16, train_loss: 0.5805, val_loss: 0.5554\n",
      "Epoch 17, train_loss: 0.5859, val_loss: 0.5487\n",
      "Epoch 18, train_loss: 0.5696, val_loss: 0.5422\n",
      "Epoch 19, train_loss: 0.5659, val_loss: 0.5371\n",
      "Epoch 20, train_loss: 0.5623, val_loss: 0.5332\n",
      "Epoch 21, train_loss: 0.5712, val_loss: 0.5297\n",
      "Epoch 22, train_loss: 0.5510, val_loss: 0.5264\n",
      "Epoch 23, train_loss: 0.5569, val_loss: 0.5228\n",
      "Epoch 24, train_loss: 0.5439, val_loss: 0.5189\n",
      "Epoch 25, train_loss: 0.5464, val_loss: 0.5163\n",
      "Epoch 26, train_loss: 0.5328, val_loss: 0.5135\n",
      "Epoch 27, train_loss: 0.5381, val_loss: 0.5092\n",
      "Epoch 28, train_loss: 0.5356, val_loss: 0.5054\n",
      "Epoch 29, train_loss: 0.5179, val_loss: 0.5032\n",
      "Epoch 30, train_loss: 0.5217, val_loss: 0.5006\n",
      "Epoch 31, train_loss: 0.5156, val_loss: 0.4978\n",
      "Epoch 32, train_loss: 0.5071, val_loss: 0.4947\n",
      "Epoch 33, train_loss: 0.5154, val_loss: 0.4908\n",
      "Epoch 34, train_loss: 0.4986, val_loss: 0.4886\n",
      "Epoch 35, train_loss: 0.4981, val_loss: 0.4847\n",
      "Epoch 36, train_loss: 0.4994, val_loss: 0.4835\n",
      "Epoch 37, train_loss: 0.4915, val_loss: 0.4808\n",
      "Epoch 38, train_loss: 0.4813, val_loss: 0.4777\n",
      "Epoch 39, train_loss: 0.4776, val_loss: 0.4757\n",
      "Epoch 40, train_loss: 0.4737, val_loss: 0.4731\n",
      "Epoch 41, train_loss: 0.4649, val_loss: 0.4706\n",
      "Epoch 42, train_loss: 0.4674, val_loss: 0.4669\n",
      "Epoch 43, train_loss: 0.4475, val_loss: 0.4638\n",
      "Epoch 44, train_loss: 0.4475, val_loss: 0.4602\n",
      "Epoch 45, train_loss: 0.4428, val_loss: 0.4569\n",
      "Epoch 46, train_loss: 0.4442, val_loss: 0.4537\n",
      "Epoch 47, train_loss: 0.4357, val_loss: 0.4509\n",
      "Epoch 48, train_loss: 0.4267, val_loss: 0.4479\n",
      "Epoch 49, train_loss: 0.4139, val_loss: 0.4454\n",
      "Epoch 50, train_loss: 0.4164, val_loss: 0.4430\n",
      "Epoch 51, train_loss: 0.4206, val_loss: 0.4409\n",
      "Epoch 52, train_loss: 0.4041, val_loss: 0.4408\n",
      "Epoch 53, train_loss: 0.3965, val_loss: 0.4387\n",
      "Epoch 54, train_loss: 0.3886, val_loss: 0.4367\n",
      "Epoch 55, train_loss: 0.3853, val_loss: 0.4356\n",
      "Epoch 56, train_loss: 0.3824, val_loss: 0.4323\n",
      "Epoch 57, train_loss: 0.3749, val_loss: 0.4297\n",
      "Epoch 58, train_loss: 0.3830, val_loss: 0.4286\n",
      "Epoch 59, train_loss: 0.3648, val_loss: 0.4263\n",
      "Epoch 60, train_loss: 0.3651, val_loss: 0.4233\n",
      "Epoch 61, train_loss: 0.3566, val_loss: 0.4220\n",
      "Epoch 62, train_loss: 0.3511, val_loss: 0.4191\n",
      "Epoch 63, train_loss: 0.3532, val_loss: 0.4159\n",
      "Epoch 64, train_loss: 0.3341, val_loss: 0.4153\n",
      "Epoch 65, train_loss: 0.3367, val_loss: 0.4136\n",
      "Epoch 66, train_loss: 0.3352, val_loss: 0.4115\n",
      "Epoch 67, train_loss: 0.3194, val_loss: 0.4099\n",
      "Epoch 68, train_loss: 0.3241, val_loss: 0.4075\n",
      "Epoch 69, train_loss: 0.3122, val_loss: 0.4058\n",
      "Epoch 70, train_loss: 0.3026, val_loss: 0.4035\n",
      "Epoch 71, train_loss: 0.3135, val_loss: 0.4026\n",
      "Epoch 72, train_loss: 0.3008, val_loss: 0.4011\n",
      "Epoch 73, train_loss: 0.3016, val_loss: 0.3993\n",
      "Epoch 74, train_loss: 0.2977, val_loss: 0.3971\n",
      "Epoch 75, train_loss: 0.2974, val_loss: 0.3941\n",
      "Epoch 76, train_loss: 0.2940, val_loss: 0.3919\n",
      "Epoch 77, train_loss: 0.2867, val_loss: 0.3904\n",
      "Epoch 78, train_loss: 0.2759, val_loss: 0.3890\n",
      "Epoch 79, train_loss: 0.2726, val_loss: 0.3884\n",
      "Epoch 80, train_loss: 0.2795, val_loss: 0.3863\n",
      "Epoch 81, train_loss: 0.2727, val_loss: 0.3848\n",
      "Epoch 82, train_loss: 0.2746, val_loss: 0.3830\n",
      "Epoch 83, train_loss: 0.2676, val_loss: 0.3821\n",
      "Epoch 84, train_loss: 0.2532, val_loss: 0.3811\n",
      "Epoch 85, train_loss: 0.2622, val_loss: 0.3789\n",
      "Epoch 86, train_loss: 0.2461, val_loss: 0.3782\n",
      "Epoch 87, train_loss: 0.2458, val_loss: 0.3774\n",
      "Epoch 88, train_loss: 0.2429, val_loss: 0.3772\n",
      "Epoch 89, train_loss: 0.2483, val_loss: 0.3755\n",
      "Epoch 90, train_loss: 0.2390, val_loss: 0.3745\n",
      "Epoch 91, train_loss: 0.2300, val_loss: 0.3737\n",
      "Epoch 92, train_loss: 0.2230, val_loss: 0.3729\n",
      "Epoch 93, train_loss: 0.2272, val_loss: 0.3712\n",
      "Epoch 94, train_loss: 0.2376, val_loss: 0.3693\n",
      "Epoch 95, train_loss: 0.2340, val_loss: 0.3682\n",
      "Epoch 96, train_loss: 0.2305, val_loss: 0.3673\n",
      "Epoch 97, train_loss: 0.2164, val_loss: 0.3658\n",
      "Epoch 98, train_loss: 0.2325, val_loss: 0.3652\n",
      "Epoch 99, train_loss: 0.2130, val_loss: 0.3649\n",
      "Test the model on fold 3:\n",
      "{'roc_auc': 0.8993055555555556, 'pr_auc': 0.935396393680441, 'fmax': 0.8571378612536087}\n",
      "Starting Fold: 4\n",
      "Epoch 0, train_loss: 0.6968, val_loss: 0.6906\n",
      "Epoch 1, train_loss: 0.6870, val_loss: 0.6765\n",
      "Epoch 2, train_loss: 0.6667, val_loss: 0.6604\n",
      "Epoch 3, train_loss: 0.6644, val_loss: 0.6497\n",
      "Epoch 4, train_loss: 0.6447, val_loss: 0.6413\n",
      "Epoch 5, train_loss: 0.6357, val_loss: 0.6308\n",
      "Epoch 6, train_loss: 0.6336, val_loss: 0.6207\n",
      "Epoch 7, train_loss: 0.6177, val_loss: 0.6103\n",
      "Epoch 8, train_loss: 0.6196, val_loss: 0.6008\n",
      "Epoch 9, train_loss: 0.6025, val_loss: 0.5942\n",
      "Epoch 10, train_loss: 0.6051, val_loss: 0.5871\n",
      "Epoch 11, train_loss: 0.5982, val_loss: 0.5785\n",
      "Epoch 12, train_loss: 0.5898, val_loss: 0.5704\n",
      "Epoch 13, train_loss: 0.5836, val_loss: 0.5637\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14, train_loss: 0.5791, val_loss: 0.5582\n",
      "Epoch 15, train_loss: 0.5775, val_loss: 0.5540\n",
      "Epoch 16, train_loss: 0.5690, val_loss: 0.5489\n",
      "Epoch 17, train_loss: 0.5686, val_loss: 0.5457\n",
      "Epoch 18, train_loss: 0.5514, val_loss: 0.5403\n",
      "Epoch 19, train_loss: 0.5444, val_loss: 0.5348\n",
      "Epoch 20, train_loss: 0.5479, val_loss: 0.5299\n",
      "Epoch 21, train_loss: 0.5438, val_loss: 0.5266\n",
      "Epoch 22, train_loss: 0.5421, val_loss: 0.5240\n",
      "Epoch 23, train_loss: 0.5288, val_loss: 0.5209\n",
      "Epoch 24, train_loss: 0.5362, val_loss: 0.5172\n",
      "Epoch 25, train_loss: 0.5160, val_loss: 0.5136\n",
      "Epoch 26, train_loss: 0.5107, val_loss: 0.5106\n",
      "Epoch 27, train_loss: 0.5110, val_loss: 0.5085\n",
      "Epoch 28, train_loss: 0.5004, val_loss: 0.5035\n",
      "Epoch 29, train_loss: 0.4885, val_loss: 0.5006\n",
      "Epoch 30, train_loss: 0.4890, val_loss: 0.4974\n",
      "Epoch 31, train_loss: 0.4903, val_loss: 0.4949\n",
      "Epoch 32, train_loss: 0.4837, val_loss: 0.4925\n",
      "Epoch 33, train_loss: 0.4697, val_loss: 0.4902\n",
      "Epoch 34, train_loss: 0.4782, val_loss: 0.4869\n",
      "Epoch 35, train_loss: 0.4648, val_loss: 0.4850\n",
      "Epoch 36, train_loss: 0.4600, val_loss: 0.4831\n",
      "Epoch 37, train_loss: 0.4477, val_loss: 0.4820\n",
      "Epoch 38, train_loss: 0.4460, val_loss: 0.4793\n",
      "Epoch 39, train_loss: 0.4420, val_loss: 0.4766\n",
      "Epoch 40, train_loss: 0.4425, val_loss: 0.4741\n",
      "Epoch 41, train_loss: 0.4278, val_loss: 0.4719\n",
      "Epoch 42, train_loss: 0.4247, val_loss: 0.4700\n",
      "Epoch 43, train_loss: 0.4189, val_loss: 0.4678\n",
      "Epoch 44, train_loss: 0.4119, val_loss: 0.4649\n",
      "Epoch 45, train_loss: 0.4052, val_loss: 0.4643\n",
      "Epoch 46, train_loss: 0.4037, val_loss: 0.4616\n",
      "Epoch 47, train_loss: 0.3906, val_loss: 0.4600\n",
      "Epoch 48, train_loss: 0.3847, val_loss: 0.4573\n",
      "Epoch 49, train_loss: 0.3921, val_loss: 0.4546\n",
      "Epoch 50, train_loss: 0.3781, val_loss: 0.4530\n",
      "Epoch 51, train_loss: 0.3753, val_loss: 0.4523\n",
      "Epoch 52, train_loss: 0.3724, val_loss: 0.4510\n",
      "Epoch 53, train_loss: 0.3612, val_loss: 0.4506\n",
      "Epoch 54, train_loss: 0.3545, val_loss: 0.4496\n",
      "Epoch 55, train_loss: 0.3502, val_loss: 0.4481\n",
      "Epoch 56, train_loss: 0.3489, val_loss: 0.4442\n",
      "Epoch 57, train_loss: 0.3434, val_loss: 0.4427\n",
      "Epoch 58, train_loss: 0.3315, val_loss: 0.4401\n",
      "Epoch 59, train_loss: 0.3378, val_loss: 0.4368\n",
      "Epoch 60, train_loss: 0.3214, val_loss: 0.4352\n",
      "Epoch 61, train_loss: 0.3168, val_loss: 0.4350\n",
      "Epoch 62, train_loss: 0.3145, val_loss: 0.4367\n",
      "Epoch 63, train_loss: 0.3046, val_loss: 0.4356\n",
      "Epoch 64, train_loss: 0.3083, val_loss: 0.4350\n",
      "Epoch 65, train_loss: 0.2935, val_loss: 0.4336\n",
      "Epoch 66, train_loss: 0.2970, val_loss: 0.4291\n",
      "Epoch 67, train_loss: 0.2895, val_loss: 0.4281\n",
      "Epoch 68, train_loss: 0.2831, val_loss: 0.4275\n",
      "Epoch 69, train_loss: 0.2786, val_loss: 0.4253\n",
      "Epoch 70, train_loss: 0.2741, val_loss: 0.4233\n",
      "Epoch 71, train_loss: 0.2708, val_loss: 0.4223\n",
      "Epoch 72, train_loss: 0.2687, val_loss: 0.4216\n",
      "Epoch 73, train_loss: 0.2614, val_loss: 0.4214\n",
      "Epoch 74, train_loss: 0.2560, val_loss: 0.4195\n",
      "Epoch 75, train_loss: 0.2620, val_loss: 0.4189\n",
      "Epoch 76, train_loss: 0.2501, val_loss: 0.4171\n",
      "Epoch 77, train_loss: 0.2492, val_loss: 0.4175\n",
      "Epoch 78, train_loss: 0.2404, val_loss: 0.4157\n",
      "Epoch 79, train_loss: 0.2380, val_loss: 0.4143\n",
      "Epoch 80, train_loss: 0.2337, val_loss: 0.4126\n",
      "Epoch 81, train_loss: 0.2281, val_loss: 0.4116\n",
      "Epoch 82, train_loss: 0.2381, val_loss: 0.4113\n",
      "Epoch 83, train_loss: 0.2252, val_loss: 0.4102\n",
      "Epoch 84, train_loss: 0.2218, val_loss: 0.4109\n",
      "Epoch 85, train_loss: 0.2223, val_loss: 0.4074\n",
      "Epoch 86, train_loss: 0.2204, val_loss: 0.4097\n",
      "Epoch 87, train_loss: 0.2134, val_loss: 0.4094\n",
      "Epoch 88, train_loss: 0.2056, val_loss: 0.4092\n",
      "Epoch 89, train_loss: 0.2048, val_loss: 0.4084\n",
      "Epoch 90, train_loss: 0.2068, val_loss: 0.4065\n",
      "Epoch 91, train_loss: 0.2046, val_loss: 0.4047\n",
      "Epoch 92, train_loss: 0.2016, val_loss: 0.4059\n",
      "Epoch 93, train_loss: 0.1871, val_loss: 0.4046\n",
      "Epoch 94, train_loss: 0.1955, val_loss: 0.4038\n",
      "Epoch 95, train_loss: 0.1856, val_loss: 0.4047\n",
      "Epoch 96, train_loss: 0.1877, val_loss: 0.4041\n",
      "Epoch 97, train_loss: 0.1981, val_loss: 0.4034\n",
      "Epoch 98, train_loss: 0.1853, val_loss: 0.4027\n",
      "Epoch 99, train_loss: 0.1818, val_loss: 0.4020\n",
      "Test the model on fold 4:\n",
      "{'roc_auc': 0.8071428571428572, 'pr_auc': 0.8547972192535347, 'fmax': 0.8148098217041229}\n",
      "Evaluate pretrained model on disease class Developmental (12/12)\n",
      "Starting Fold: 0\n",
      "Epoch 0, train_loss: 0.6977, val_loss: 0.6949\n",
      "Epoch 1, train_loss: 0.6751, val_loss: 0.6930\n",
      "Epoch 2, train_loss: 0.6813, val_loss: 0.6906\n",
      "Epoch 3, train_loss: 0.6829, val_loss: 0.6889\n",
      "Epoch 4, train_loss: 0.6609, val_loss: 0.6873\n",
      "Epoch 5, train_loss: 0.6610, val_loss: 0.6853\n",
      "Epoch 6, train_loss: 0.6476, val_loss: 0.6836\n",
      "Epoch 7, train_loss: 0.6531, val_loss: 0.6826\n",
      "Epoch 8, train_loss: 0.6194, val_loss: 0.6808\n",
      "Epoch 9, train_loss: 0.6399, val_loss: 0.6785\n",
      "Epoch 10, train_loss: 0.6289, val_loss: 0.6768\n",
      "Epoch 11, train_loss: 0.6481, val_loss: 0.6740\n",
      "Epoch 12, train_loss: 0.6051, val_loss: 0.6714\n",
      "Epoch 13, train_loss: 0.6261, val_loss: 0.6699\n",
      "Epoch 14, train_loss: 0.6271, val_loss: 0.6680\n",
      "Epoch 15, train_loss: 0.5916, val_loss: 0.6659\n",
      "Epoch 16, train_loss: 0.6050, val_loss: 0.6638\n",
      "Epoch 17, train_loss: 0.5848, val_loss: 0.6609\n",
      "Epoch 18, train_loss: 0.5996, val_loss: 0.6596\n",
      "Epoch 19, train_loss: 0.6000, val_loss: 0.6584\n",
      "Epoch 20, train_loss: 0.5930, val_loss: 0.6576\n",
      "Epoch 21, train_loss: 0.5788, val_loss: 0.6564\n",
      "Epoch 22, train_loss: 0.6041, val_loss: 0.6559\n",
      "Epoch 23, train_loss: 0.5869, val_loss: 0.6554\n",
      "Epoch 24, train_loss: 0.5475, val_loss: 0.6546\n",
      "Epoch 25, train_loss: 0.5835, val_loss: 0.6540\n",
      "Epoch 26, train_loss: 0.5834, val_loss: 0.6540\n",
      "Epoch 27, train_loss: 0.5832, val_loss: 0.6536\n",
      "Epoch 28, train_loss: 0.5628, val_loss: 0.6528\n",
      "Epoch 29, train_loss: 0.5553, val_loss: 0.6512\n",
      "Epoch 30, train_loss: 0.5640, val_loss: 0.6488\n",
      "Epoch 31, train_loss: 0.5599, val_loss: 0.6455\n",
      "Epoch 32, train_loss: 0.5390, val_loss: 0.6432\n",
      "Epoch 33, train_loss: 0.5487, val_loss: 0.6409\n",
      "Epoch 34, train_loss: 0.5494, val_loss: 0.6398\n",
      "Epoch 35, train_loss: 0.5261, val_loss: 0.6392\n",
      "Epoch 36, train_loss: 0.5206, val_loss: 0.6376\n",
      "Epoch 37, train_loss: 0.5322, val_loss: 0.6359\n",
      "Epoch 38, train_loss: 0.5293, val_loss: 0.6338\n",
      "Epoch 39, train_loss: 0.5004, val_loss: 0.6328\n",
      "Epoch 40, train_loss: 0.4948, val_loss: 0.6317\n",
      "Epoch 41, train_loss: 0.5005, val_loss: 0.6309\n",
      "Epoch 42, train_loss: 0.5059, val_loss: 0.6296\n",
      "Epoch 43, train_loss: 0.5109, val_loss: 0.6274\n",
      "Epoch 44, train_loss: 0.5149, val_loss: 0.6245\n",
      "Epoch 45, train_loss: 0.4755, val_loss: 0.6235\n",
      "Epoch 46, train_loss: 0.4683, val_loss: 0.6228\n",
      "Epoch 47, train_loss: 0.4959, val_loss: 0.6218\n",
      "Epoch 48, train_loss: 0.4941, val_loss: 0.6208\n",
      "Epoch 49, train_loss: 0.4854, val_loss: 0.6191\n",
      "Epoch 50, train_loss: 0.4586, val_loss: 0.6185\n",
      "Epoch 51, train_loss: 0.4789, val_loss: 0.6189\n",
      "Epoch 52, train_loss: 0.4237, val_loss: 0.6174\n",
      "Epoch 53, train_loss: 0.4720, val_loss: 0.6158\n",
      "Epoch 54, train_loss: 0.4571, val_loss: 0.6139\n",
      "Epoch 55, train_loss: 0.4352, val_loss: 0.6125\n",
      "Epoch 56, train_loss: 0.4413, val_loss: 0.6107\n",
      "Epoch 57, train_loss: 0.3884, val_loss: 0.6083\n",
      "Epoch 58, train_loss: 0.4051, val_loss: 0.6056\n",
      "Epoch 59, train_loss: 0.4667, val_loss: 0.6035\n",
      "Epoch 60, train_loss: 0.4267, val_loss: 0.6018\n",
      "Epoch 61, train_loss: 0.4242, val_loss: 0.6008\n",
      "Epoch 62, train_loss: 0.3677, val_loss: 0.6001\n",
      "Epoch 63, train_loss: 0.3871, val_loss: 0.5989\n",
      "Epoch 64, train_loss: 0.3880, val_loss: 0.5965\n",
      "Epoch 65, train_loss: 0.4441, val_loss: 0.5948\n",
      "Epoch 66, train_loss: 0.3698, val_loss: 0.5933\n",
      "Epoch 67, train_loss: 0.4009, val_loss: 0.5921\n",
      "Epoch 68, train_loss: 0.4230, val_loss: 0.5910\n",
      "Epoch 69, train_loss: 0.3446, val_loss: 0.5903\n",
      "Epoch 70, train_loss: 0.3320, val_loss: 0.5892\n",
      "Epoch 71, train_loss: 0.3456, val_loss: 0.5874\n",
      "Epoch 72, train_loss: 0.3453, val_loss: 0.5847\n",
      "Epoch 73, train_loss: 0.3321, val_loss: 0.5828\n",
      "Epoch 74, train_loss: 0.3041, val_loss: 0.5817\n",
      "Epoch 75, train_loss: 0.3102, val_loss: 0.5805\n",
      "Epoch 76, train_loss: 0.3441, val_loss: 0.5787\n",
      "Epoch 77, train_loss: 0.3218, val_loss: 0.5758\n",
      "Epoch 78, train_loss: 0.3119, val_loss: 0.5715\n",
      "Epoch 79, train_loss: 0.3604, val_loss: 0.5693\n",
      "Epoch 80, train_loss: 0.3344, val_loss: 0.5685\n",
      "Epoch 81, train_loss: 0.2861, val_loss: 0.5685\n",
      "Epoch 82, train_loss: 0.3223, val_loss: 0.5680\n",
      "Epoch 83, train_loss: 0.2913, val_loss: 0.5670\n",
      "Epoch 84, train_loss: 0.3365, val_loss: 0.5665\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 85, train_loss: 0.3535, val_loss: 0.5660\n",
      "Epoch 86, train_loss: 0.3227, val_loss: 0.5648\n",
      "Epoch 87, train_loss: 0.3269, val_loss: 0.5636\n",
      "Epoch 88, train_loss: 0.3317, val_loss: 0.5630\n",
      "Epoch 89, train_loss: 0.2970, val_loss: 0.5620\n",
      "Epoch 90, train_loss: 0.2678, val_loss: 0.5605\n",
      "Epoch 91, train_loss: 0.2893, val_loss: 0.5597\n",
      "Epoch 92, train_loss: 0.3333, val_loss: 0.5581\n",
      "Epoch 93, train_loss: 0.3132, val_loss: 0.5566\n",
      "Epoch 94, train_loss: 0.2553, val_loss: 0.5553\n",
      "Epoch 95, train_loss: 0.2697, val_loss: 0.5538\n",
      "Epoch 96, train_loss: 0.2889, val_loss: 0.5506\n",
      "Epoch 97, train_loss: 0.2926, val_loss: 0.5496\n",
      "Epoch 98, train_loss: 0.2552, val_loss: 0.5492\n",
      "Epoch 99, train_loss: 0.2325, val_loss: 0.5495\n",
      "Test the model on fold 0:\n",
      "{'roc_auc': 0.7803030303030303, 'pr_auc': 0.7674153870485393, 'fmax': 0.8181768595341847}\n",
      "Starting Fold: 1\n",
      "Epoch 0, train_loss: 0.7056, val_loss: 0.7026\n",
      "Epoch 1, train_loss: 0.6756, val_loss: 0.7013\n",
      "Epoch 2, train_loss: 0.6655, val_loss: 0.7009\n",
      "Epoch 3, train_loss: 0.6764, val_loss: 0.7005\n",
      "Epoch 4, train_loss: 0.6650, val_loss: 0.7005\n",
      "Epoch 5, train_loss: 0.6570, val_loss: 0.7013\n",
      "Epoch 6, train_loss: 0.6549, val_loss: 0.7028\n",
      "Epoch 7, train_loss: 0.6486, val_loss: 0.7049\n",
      "Epoch 8, train_loss: 0.6498, val_loss: 0.7068\n",
      "Epoch 9, train_loss: 0.6401, val_loss: 0.7084\n",
      "Epoch 10, train_loss: 0.6399, val_loss: 0.7094\n",
      "Epoch 11, train_loss: 0.6502, val_loss: 0.7102\n",
      "Epoch 12, train_loss: 0.6477, val_loss: 0.7101\n",
      "Epoch 13, train_loss: 0.6360, val_loss: 0.7106\n",
      "Epoch 14, train_loss: 0.6183, val_loss: 0.7106\n",
      "Epoch 15, train_loss: 0.6159, val_loss: 0.7110\n",
      "Epoch 16, train_loss: 0.6187, val_loss: 0.7116\n",
      "Early Stopping!\n",
      "Test the model on fold 1:\n",
      "{'roc_auc': 0.7698412698412698, 'pr_auc': 0.8617813068056323, 'fmax': 0.823524567502544}\n",
      "Starting Fold: 2\n",
      "Epoch 0, train_loss: 0.7013, val_loss: 0.6933\n",
      "Epoch 1, train_loss: 0.6844, val_loss: 0.6911\n",
      "Epoch 2, train_loss: 0.6699, val_loss: 0.6894\n",
      "Epoch 3, train_loss: 0.6662, val_loss: 0.6880\n",
      "Epoch 4, train_loss: 0.6712, val_loss: 0.6875\n",
      "Epoch 5, train_loss: 0.6803, val_loss: 0.6870\n",
      "Epoch 6, train_loss: 0.6469, val_loss: 0.6853\n",
      "Epoch 7, train_loss: 0.6499, val_loss: 0.6848\n",
      "Epoch 8, train_loss: 0.6309, val_loss: 0.6838\n",
      "Epoch 9, train_loss: 0.6332, val_loss: 0.6822\n",
      "Epoch 10, train_loss: 0.6206, val_loss: 0.6814\n",
      "Epoch 11, train_loss: 0.6214, val_loss: 0.6811\n",
      "Epoch 12, train_loss: 0.6245, val_loss: 0.6806\n",
      "Epoch 13, train_loss: 0.6375, val_loss: 0.6794\n",
      "Epoch 14, train_loss: 0.6368, val_loss: 0.6782\n",
      "Epoch 15, train_loss: 0.6244, val_loss: 0.6770\n",
      "Epoch 16, train_loss: 0.6086, val_loss: 0.6765\n",
      "Epoch 17, train_loss: 0.6173, val_loss: 0.6757\n",
      "Epoch 18, train_loss: 0.6086, val_loss: 0.6744\n",
      "Epoch 19, train_loss: 0.6122, val_loss: 0.6731\n",
      "Epoch 20, train_loss: 0.6132, val_loss: 0.6721\n",
      "Epoch 21, train_loss: 0.6120, val_loss: 0.6715\n",
      "Epoch 22, train_loss: 0.5946, val_loss: 0.6707\n",
      "Epoch 23, train_loss: 0.5974, val_loss: 0.6694\n",
      "Epoch 24, train_loss: 0.6046, val_loss: 0.6684\n",
      "Epoch 25, train_loss: 0.6067, val_loss: 0.6680\n",
      "Epoch 26, train_loss: 0.5938, val_loss: 0.6672\n",
      "Epoch 27, train_loss: 0.5601, val_loss: 0.6666\n",
      "Epoch 28, train_loss: 0.5530, val_loss: 0.6661\n",
      "Epoch 29, train_loss: 0.5853, val_loss: 0.6658\n",
      "Epoch 30, train_loss: 0.5792, val_loss: 0.6652\n",
      "Epoch 31, train_loss: 0.5810, val_loss: 0.6643\n",
      "Epoch 32, train_loss: 0.5755, val_loss: 0.6638\n",
      "Epoch 33, train_loss: 0.5748, val_loss: 0.6640\n",
      "Epoch 34, train_loss: 0.5888, val_loss: 0.6647\n",
      "Epoch 35, train_loss: 0.5502, val_loss: 0.6644\n",
      "Epoch 36, train_loss: 0.5642, val_loss: 0.6637\n",
      "Epoch 37, train_loss: 0.5575, val_loss: 0.6631\n",
      "Epoch 38, train_loss: 0.5493, val_loss: 0.6624\n",
      "Epoch 39, train_loss: 0.5596, val_loss: 0.6620\n",
      "Epoch 40, train_loss: 0.5334, val_loss: 0.6616\n",
      "Epoch 41, train_loss: 0.5398, val_loss: 0.6612\n",
      "Epoch 42, train_loss: 0.5411, val_loss: 0.6607\n",
      "Epoch 43, train_loss: 0.5149, val_loss: 0.6599\n",
      "Epoch 44, train_loss: 0.5286, val_loss: 0.6584\n",
      "Epoch 45, train_loss: 0.5020, val_loss: 0.6569\n",
      "Epoch 46, train_loss: 0.4859, val_loss: 0.6556\n",
      "Epoch 47, train_loss: 0.5089, val_loss: 0.6558\n",
      "Epoch 48, train_loss: 0.5146, val_loss: 0.6562\n",
      "Epoch 49, train_loss: 0.5220, val_loss: 0.6566\n",
      "Epoch 50, train_loss: 0.4739, val_loss: 0.6555\n",
      "Epoch 51, train_loss: 0.4718, val_loss: 0.6531\n",
      "Epoch 52, train_loss: 0.4965, val_loss: 0.6520\n",
      "Epoch 53, train_loss: 0.4917, val_loss: 0.6507\n",
      "Epoch 54, train_loss: 0.5017, val_loss: 0.6498\n",
      "Epoch 55, train_loss: 0.5024, val_loss: 0.6487\n",
      "Epoch 56, train_loss: 0.4484, val_loss: 0.6474\n",
      "Epoch 57, train_loss: 0.4954, val_loss: 0.6463\n",
      "Epoch 58, train_loss: 0.4855, val_loss: 0.6450\n",
      "Epoch 59, train_loss: 0.5113, val_loss: 0.6441\n",
      "Epoch 60, train_loss: 0.4962, val_loss: 0.6435\n",
      "Epoch 61, train_loss: 0.4739, val_loss: 0.6429\n",
      "Epoch 62, train_loss: 0.4607, val_loss: 0.6423\n",
      "Epoch 63, train_loss: 0.4148, val_loss: 0.6418\n",
      "Epoch 64, train_loss: 0.5017, val_loss: 0.6420\n",
      "Epoch 65, train_loss: 0.4788, val_loss: 0.6419\n",
      "Epoch 66, train_loss: 0.4799, val_loss: 0.6418\n",
      "Epoch 67, train_loss: 0.4654, val_loss: 0.6417\n",
      "Epoch 68, train_loss: 0.4359, val_loss: 0.6415\n",
      "Epoch 69, train_loss: 0.4158, val_loss: 0.6405\n",
      "Epoch 70, train_loss: 0.4146, val_loss: 0.6399\n",
      "Epoch 71, train_loss: 0.4041, val_loss: 0.6397\n",
      "Epoch 72, train_loss: 0.4085, val_loss: 0.6393\n",
      "Epoch 73, train_loss: 0.4010, val_loss: 0.6393\n",
      "Epoch 74, train_loss: 0.4420, val_loss: 0.6388\n",
      "Epoch 75, train_loss: 0.4372, val_loss: 0.6379\n",
      "Epoch 76, train_loss: 0.4082, val_loss: 0.6367\n",
      "Epoch 77, train_loss: 0.4524, val_loss: 0.6363\n",
      "Epoch 78, train_loss: 0.4257, val_loss: 0.6357\n",
      "Epoch 79, train_loss: 0.4141, val_loss: 0.6354\n",
      "Epoch 80, train_loss: 0.4066, val_loss: 0.6349\n",
      "Epoch 81, train_loss: 0.4044, val_loss: 0.6349\n",
      "Epoch 82, train_loss: 0.3774, val_loss: 0.6348\n",
      "Epoch 83, train_loss: 0.3701, val_loss: 0.6346\n",
      "Epoch 84, train_loss: 0.3935, val_loss: 0.6337\n",
      "Epoch 85, train_loss: 0.3999, val_loss: 0.6326\n",
      "Epoch 86, train_loss: 0.4251, val_loss: 0.6318\n",
      "Epoch 87, train_loss: 0.4065, val_loss: 0.6309\n",
      "Epoch 88, train_loss: 0.3804, val_loss: 0.6302\n",
      "Epoch 89, train_loss: 0.3779, val_loss: 0.6303\n",
      "Epoch 90, train_loss: 0.3513, val_loss: 0.6301\n",
      "Epoch 91, train_loss: 0.4052, val_loss: 0.6298\n",
      "Epoch 92, train_loss: 0.3540, val_loss: 0.6293\n",
      "Epoch 93, train_loss: 0.3333, val_loss: 0.6283\n",
      "Epoch 94, train_loss: 0.3883, val_loss: 0.6270\n",
      "Epoch 95, train_loss: 0.3739, val_loss: 0.6260\n",
      "Epoch 96, train_loss: 0.3393, val_loss: 0.6253\n",
      "Epoch 97, train_loss: 0.3457, val_loss: 0.6246\n",
      "Epoch 98, train_loss: 0.3259, val_loss: 0.6249\n",
      "Epoch 99, train_loss: 0.3667, val_loss: 0.6241\n",
      "Test the model on fold 2:\n",
      "{'roc_auc': 0.675, 'pr_auc': 0.7437322498622808, 'fmax': 0.8571379592116616}\n",
      "Starting Fold: 3\n",
      "Epoch 0, train_loss: 0.7024, val_loss: 0.6883\n",
      "Epoch 1, train_loss: 0.6838, val_loss: 0.6847\n",
      "Epoch 2, train_loss: 0.6882, val_loss: 0.6818\n",
      "Epoch 3, train_loss: 0.6818, val_loss: 0.6782\n",
      "Epoch 4, train_loss: 0.6563, val_loss: 0.6754\n",
      "Epoch 5, train_loss: 0.6758, val_loss: 0.6749\n",
      "Epoch 6, train_loss: 0.6532, val_loss: 0.6759\n",
      "Epoch 7, train_loss: 0.6538, val_loss: 0.6755\n",
      "Epoch 8, train_loss: 0.6535, val_loss: 0.6742\n",
      "Epoch 9, train_loss: 0.6422, val_loss: 0.6727\n",
      "Epoch 10, train_loss: 0.6497, val_loss: 0.6723\n",
      "Epoch 11, train_loss: 0.6339, val_loss: 0.6713\n",
      "Epoch 12, train_loss: 0.6405, val_loss: 0.6707\n",
      "Epoch 13, train_loss: 0.6313, val_loss: 0.6694\n",
      "Epoch 14, train_loss: 0.6160, val_loss: 0.6679\n",
      "Epoch 15, train_loss: 0.6249, val_loss: 0.6678\n",
      "Epoch 16, train_loss: 0.6222, val_loss: 0.6671\n",
      "Epoch 17, train_loss: 0.5828, val_loss: 0.6657\n",
      "Epoch 18, train_loss: 0.6166, val_loss: 0.6639\n",
      "Epoch 19, train_loss: 0.6165, val_loss: 0.6619\n",
      "Epoch 20, train_loss: 0.5997, val_loss: 0.6592\n",
      "Epoch 21, train_loss: 0.5918, val_loss: 0.6557\n",
      "Epoch 22, train_loss: 0.6026, val_loss: 0.6521\n",
      "Epoch 23, train_loss: 0.5900, val_loss: 0.6498\n",
      "Epoch 24, train_loss: 0.5814, val_loss: 0.6479\n",
      "Epoch 25, train_loss: 0.5916, val_loss: 0.6462\n",
      "Epoch 26, train_loss: 0.5968, val_loss: 0.6442\n",
      "Epoch 27, train_loss: 0.5932, val_loss: 0.6422\n",
      "Epoch 28, train_loss: 0.6037, val_loss: 0.6408\n",
      "Epoch 29, train_loss: 0.5585, val_loss: 0.6401\n",
      "Epoch 30, train_loss: 0.5594, val_loss: 0.6379\n",
      "Epoch 31, train_loss: 0.5728, val_loss: 0.6342\n",
      "Epoch 32, train_loss: 0.5526, val_loss: 0.6322\n",
      "Epoch 33, train_loss: 0.5325, val_loss: 0.6310\n",
      "Epoch 34, train_loss: 0.5723, val_loss: 0.6293\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35, train_loss: 0.5436, val_loss: 0.6281\n",
      "Epoch 36, train_loss: 0.5687, val_loss: 0.6262\n",
      "Epoch 37, train_loss: 0.5558, val_loss: 0.6245\n",
      "Epoch 38, train_loss: 0.5579, val_loss: 0.6233\n",
      "Epoch 39, train_loss: 0.5514, val_loss: 0.6222\n",
      "Epoch 40, train_loss: 0.5360, val_loss: 0.6207\n",
      "Epoch 41, train_loss: 0.4947, val_loss: 0.6193\n",
      "Epoch 42, train_loss: 0.5088, val_loss: 0.6173\n",
      "Epoch 43, train_loss: 0.5351, val_loss: 0.6162\n",
      "Epoch 44, train_loss: 0.5573, val_loss: 0.6149\n",
      "Epoch 45, train_loss: 0.5436, val_loss: 0.6144\n",
      "Epoch 46, train_loss: 0.5106, val_loss: 0.6139\n",
      "Epoch 47, train_loss: 0.5032, val_loss: 0.6138\n",
      "Epoch 48, train_loss: 0.5198, val_loss: 0.6138\n",
      "Epoch 49, train_loss: 0.5232, val_loss: 0.6131\n",
      "Epoch 50, train_loss: 0.5302, val_loss: 0.6126\n",
      "Epoch 51, train_loss: 0.5345, val_loss: 0.6123\n",
      "Epoch 52, train_loss: 0.4634, val_loss: 0.6115\n",
      "Epoch 53, train_loss: 0.5334, val_loss: 0.6102\n",
      "Epoch 54, train_loss: 0.5148, val_loss: 0.6098\n",
      "Epoch 55, train_loss: 0.4868, val_loss: 0.6092\n",
      "Epoch 56, train_loss: 0.4943, val_loss: 0.6078\n",
      "Epoch 57, train_loss: 0.5160, val_loss: 0.6065\n",
      "Epoch 58, train_loss: 0.5121, val_loss: 0.6058\n",
      "Epoch 59, train_loss: 0.4764, val_loss: 0.6047\n",
      "Epoch 60, train_loss: 0.4469, val_loss: 0.6035\n",
      "Epoch 61, train_loss: 0.5038, val_loss: 0.6012\n",
      "Epoch 62, train_loss: 0.4410, val_loss: 0.5997\n",
      "Epoch 63, train_loss: 0.4846, val_loss: 0.5979\n",
      "Epoch 64, train_loss: 0.4588, val_loss: 0.5974\n",
      "Epoch 65, train_loss: 0.4941, val_loss: 0.5974\n",
      "Epoch 66, train_loss: 0.4555, val_loss: 0.5982\n",
      "Epoch 67, train_loss: 0.4310, val_loss: 0.5988\n",
      "Epoch 68, train_loss: 0.4424, val_loss: 0.5976\n",
      "Epoch 69, train_loss: 0.4463, val_loss: 0.5959\n",
      "Epoch 70, train_loss: 0.4314, val_loss: 0.5939\n",
      "Epoch 71, train_loss: 0.4282, val_loss: 0.5910\n",
      "Epoch 72, train_loss: 0.4596, val_loss: 0.5881\n",
      "Epoch 73, train_loss: 0.4054, val_loss: 0.5865\n",
      "Epoch 74, train_loss: 0.3637, val_loss: 0.5843\n",
      "Epoch 75, train_loss: 0.4164, val_loss: 0.5824\n",
      "Epoch 76, train_loss: 0.4211, val_loss: 0.5805\n",
      "Epoch 77, train_loss: 0.4155, val_loss: 0.5788\n",
      "Epoch 78, train_loss: 0.4471, val_loss: 0.5767\n",
      "Epoch 79, train_loss: 0.3825, val_loss: 0.5754\n",
      "Epoch 80, train_loss: 0.4053, val_loss: 0.5739\n",
      "Epoch 81, train_loss: 0.4261, val_loss: 0.5732\n",
      "Epoch 82, train_loss: 0.3968, val_loss: 0.5733\n",
      "Epoch 83, train_loss: 0.4241, val_loss: 0.5736\n",
      "Epoch 84, train_loss: 0.3627, val_loss: 0.5732\n",
      "Epoch 85, train_loss: 0.4066, val_loss: 0.5717\n",
      "Epoch 86, train_loss: 0.3536, val_loss: 0.5696\n",
      "Epoch 87, train_loss: 0.4028, val_loss: 0.5682\n",
      "Epoch 88, train_loss: 0.3418, val_loss: 0.5672\n",
      "Epoch 89, train_loss: 0.4153, val_loss: 0.5656\n",
      "Epoch 90, train_loss: 0.3618, val_loss: 0.5658\n",
      "Epoch 91, train_loss: 0.3454, val_loss: 0.5654\n",
      "Epoch 92, train_loss: 0.3463, val_loss: 0.5638\n",
      "Epoch 93, train_loss: 0.3944, val_loss: 0.5614\n",
      "Epoch 94, train_loss: 0.3944, val_loss: 0.5605\n",
      "Epoch 95, train_loss: 0.3464, val_loss: 0.5591\n",
      "Epoch 96, train_loss: 0.3573, val_loss: 0.5577\n",
      "Epoch 97, train_loss: 0.3266, val_loss: 0.5569\n",
      "Epoch 98, train_loss: 0.3479, val_loss: 0.5562\n",
      "Epoch 99, train_loss: 0.3642, val_loss: 0.5562\n",
      "Test the model on fold 3:\n",
      "{'roc_auc': 0.5892857142857143, 'pr_auc': 0.4223810969609289, 'fmax': 0.5833292014181566}\n",
      "Starting Fold: 4\n",
      "Epoch 0, train_loss: 0.6972, val_loss: 0.7003\n",
      "Epoch 1, train_loss: 0.6843, val_loss: 0.7021\n",
      "Epoch 2, train_loss: 0.6845, val_loss: 0.7024\n",
      "Epoch 3, train_loss: 0.6858, val_loss: 0.7020\n",
      "Epoch 4, train_loss: 0.6583, val_loss: 0.7018\n",
      "Epoch 5, train_loss: 0.6535, val_loss: 0.7026\n",
      "Epoch 6, train_loss: 0.6522, val_loss: 0.7033\n",
      "Epoch 7, train_loss: 0.6577, val_loss: 0.7034\n",
      "Epoch 8, train_loss: 0.6352, val_loss: 0.7038\n",
      "Epoch 9, train_loss: 0.6220, val_loss: 0.7047\n",
      "Epoch 10, train_loss: 0.6440, val_loss: 0.7050\n",
      "Epoch 11, train_loss: 0.6360, val_loss: 0.7052\n",
      "Epoch 12, train_loss: 0.6353, val_loss: 0.7052\n",
      "Epoch 13, train_loss: 0.6284, val_loss: 0.7052\n",
      "Epoch 14, train_loss: 0.6278, val_loss: 0.7055\n",
      "Epoch 15, train_loss: 0.6237, val_loss: 0.7053\n",
      "Epoch 16, train_loss: 0.6212, val_loss: 0.7058\n",
      "Early Stopping!\n",
      "Test the model on fold 4:\n",
      "{'roc_auc': 0.6239316239316239, 'pr_auc': 0.5548261679840627, 'fmax': 0.6428527806418456}\n",
      "############################################################\n",
      "# START CLASSIFICATION USING PRETRAINED MODEL FROM FOLD: 5 #\n",
      "############################################################\n",
      "Evaluate pretrained model on disease class Ophthamological (1/12)\n",
      "Starting Fold: 0\n",
      "Epoch 0, train_loss: 0.6923, val_loss: 0.6970\n",
      "Epoch 1, train_loss: 0.6873, val_loss: 0.6925\n",
      "Epoch 2, train_loss: 0.6741, val_loss: 0.6876\n",
      "Epoch 3, train_loss: 0.6667, val_loss: 0.6835\n",
      "Epoch 4, train_loss: 0.6605, val_loss: 0.6806\n",
      "Epoch 5, train_loss: 0.6595, val_loss: 0.6777\n",
      "Epoch 6, train_loss: 0.6500, val_loss: 0.6721\n",
      "Epoch 7, train_loss: 0.6346, val_loss: 0.6693\n",
      "Epoch 8, train_loss: 0.6341, val_loss: 0.6695\n",
      "Epoch 9, train_loss: 0.6300, val_loss: 0.6653\n",
      "Epoch 10, train_loss: 0.6234, val_loss: 0.6602\n",
      "Epoch 11, train_loss: 0.6157, val_loss: 0.6586\n",
      "Epoch 12, train_loss: 0.6079, val_loss: 0.6564\n",
      "Epoch 13, train_loss: 0.5989, val_loss: 0.6521\n",
      "Epoch 14, train_loss: 0.6058, val_loss: 0.6474\n",
      "Epoch 15, train_loss: 0.5914, val_loss: 0.6459\n",
      "Epoch 16, train_loss: 0.5833, val_loss: 0.6406\n",
      "Epoch 17, train_loss: 0.5816, val_loss: 0.6349\n",
      "Epoch 18, train_loss: 0.5680, val_loss: 0.6329\n",
      "Epoch 19, train_loss: 0.5673, val_loss: 0.6303\n",
      "Epoch 20, train_loss: 0.5490, val_loss: 0.6244\n",
      "Epoch 21, train_loss: 0.5543, val_loss: 0.6187\n",
      "Epoch 22, train_loss: 0.5446, val_loss: 0.6153\n",
      "Epoch 23, train_loss: 0.5456, val_loss: 0.6110\n",
      "Epoch 24, train_loss: 0.5276, val_loss: 0.6079\n",
      "Epoch 25, train_loss: 0.5266, val_loss: 0.6038\n",
      "Epoch 26, train_loss: 0.5192, val_loss: 0.5982\n",
      "Epoch 27, train_loss: 0.5096, val_loss: 0.5931\n",
      "Epoch 28, train_loss: 0.5010, val_loss: 0.5877\n",
      "Epoch 29, train_loss: 0.4933, val_loss: 0.5837\n",
      "Epoch 30, train_loss: 0.4773, val_loss: 0.5769\n",
      "Epoch 31, train_loss: 0.4671, val_loss: 0.5723\n",
      "Epoch 32, train_loss: 0.4626, val_loss: 0.5681\n",
      "Epoch 33, train_loss: 0.4557, val_loss: 0.5642\n",
      "Epoch 34, train_loss: 0.4512, val_loss: 0.5588\n",
      "Epoch 35, train_loss: 0.4338, val_loss: 0.5555\n",
      "Epoch 36, train_loss: 0.4339, val_loss: 0.5530\n",
      "Epoch 37, train_loss: 0.4174, val_loss: 0.5470\n",
      "Epoch 38, train_loss: 0.4146, val_loss: 0.5409\n",
      "Epoch 39, train_loss: 0.4165, val_loss: 0.5386\n",
      "Epoch 40, train_loss: 0.4084, val_loss: 0.5345\n",
      "Epoch 41, train_loss: 0.3907, val_loss: 0.5315\n",
      "Epoch 42, train_loss: 0.3792, val_loss: 0.5293\n",
      "Epoch 43, train_loss: 0.3752, val_loss: 0.5252\n",
      "Epoch 44, train_loss: 0.3659, val_loss: 0.5225\n",
      "Epoch 45, train_loss: 0.3966, val_loss: 0.5219\n",
      "Epoch 46, train_loss: 0.3794, val_loss: 0.5192\n",
      "Epoch 47, train_loss: 0.3436, val_loss: 0.5136\n",
      "Epoch 48, train_loss: 0.3301, val_loss: 0.5071\n",
      "Epoch 49, train_loss: 0.3317, val_loss: 0.5028\n",
      "Epoch 50, train_loss: 0.3230, val_loss: 0.5026\n",
      "Epoch 51, train_loss: 0.3117, val_loss: 0.5014\n",
      "Epoch 52, train_loss: 0.3220, val_loss: 0.4987\n",
      "Epoch 53, train_loss: 0.3006, val_loss: 0.4957\n",
      "Epoch 54, train_loss: 0.3074, val_loss: 0.4939\n",
      "Epoch 55, train_loss: 0.2932, val_loss: 0.4922\n",
      "Epoch 56, train_loss: 0.2860, val_loss: 0.4885\n",
      "Epoch 57, train_loss: 0.2839, val_loss: 0.4853\n",
      "Epoch 58, train_loss: 0.2793, val_loss: 0.4844\n",
      "Epoch 59, train_loss: 0.2721, val_loss: 0.4820\n",
      "Epoch 60, train_loss: 0.2798, val_loss: 0.4842\n",
      "Epoch 61, train_loss: 0.2574, val_loss: 0.4833\n",
      "Epoch 62, train_loss: 0.2525, val_loss: 0.4772\n",
      "Epoch 63, train_loss: 0.2552, val_loss: 0.4736\n",
      "Epoch 64, train_loss: 0.2566, val_loss: 0.4770\n",
      "Epoch 65, train_loss: 0.2460, val_loss: 0.4710\n",
      "Epoch 66, train_loss: 0.2434, val_loss: 0.4716\n",
      "Epoch 67, train_loss: 0.2371, val_loss: 0.4691\n",
      "Epoch 68, train_loss: 0.2357, val_loss: 0.4702\n",
      "Epoch 69, train_loss: 0.2245, val_loss: 0.4676\n",
      "Epoch 70, train_loss: 0.2293, val_loss: 0.4688\n",
      "Epoch 71, train_loss: 0.2231, val_loss: 0.4657\n",
      "Epoch 72, train_loss: 0.2134, val_loss: 0.4651\n",
      "Epoch 73, train_loss: 0.2175, val_loss: 0.4688\n",
      "Epoch 74, train_loss: 0.2140, val_loss: 0.4672\n",
      "Epoch 75, train_loss: 0.2098, val_loss: 0.4643\n",
      "Epoch 76, train_loss: 0.2027, val_loss: 0.4649\n",
      "Epoch 77, train_loss: 0.2093, val_loss: 0.4662\n",
      "Epoch 78, train_loss: 0.2054, val_loss: 0.4638\n",
      "Epoch 79, train_loss: 0.2011, val_loss: 0.4603\n",
      "Epoch 80, train_loss: 0.2034, val_loss: 0.4622\n",
      "Epoch 81, train_loss: 0.1997, val_loss: 0.4601\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 82, train_loss: 0.1864, val_loss: 0.4631\n",
      "Epoch 83, train_loss: 0.1883, val_loss: 0.4642\n",
      "Epoch 84, train_loss: 0.1909, val_loss: 0.4594\n",
      "Epoch 85, train_loss: 0.1907, val_loss: 0.4601\n",
      "Epoch 86, train_loss: 0.1785, val_loss: 0.4595\n",
      "Epoch 87, train_loss: 0.1907, val_loss: 0.4613\n",
      "Epoch 88, train_loss: 0.1916, val_loss: 0.4604\n",
      "Epoch 89, train_loss: 0.1741, val_loss: 0.4583\n",
      "Epoch 90, train_loss: 0.1813, val_loss: 0.4575\n",
      "Epoch 91, train_loss: 0.1813, val_loss: 0.4587\n",
      "Epoch 92, train_loss: 0.1812, val_loss: 0.4624\n",
      "Epoch 93, train_loss: 0.1727, val_loss: 0.4636\n",
      "Epoch 94, train_loss: 0.1818, val_loss: 0.4549\n",
      "Epoch 95, train_loss: 0.1762, val_loss: 0.4558\n",
      "Epoch 96, train_loss: 0.1791, val_loss: 0.4611\n",
      "Epoch 97, train_loss: 0.1792, val_loss: 0.4568\n",
      "Epoch 98, train_loss: 0.1695, val_loss: 0.4559\n",
      "Epoch 99, train_loss: 0.1720, val_loss: 0.4539\n",
      "Test the model on fold 0:\n",
      "{'roc_auc': 0.8932291666666666, 'pr_auc': 0.9321762706732819, 'fmax': 0.8571379592116616}\n",
      "Starting Fold: 1\n",
      "Epoch 0, train_loss: 0.6922, val_loss: 0.6841\n",
      "Epoch 1, train_loss: 0.6751, val_loss: 0.6725\n",
      "Epoch 2, train_loss: 0.6638, val_loss: 0.6666\n",
      "Epoch 3, train_loss: 0.6572, val_loss: 0.6592\n",
      "Epoch 4, train_loss: 0.6453, val_loss: 0.6526\n",
      "Epoch 5, train_loss: 0.6464, val_loss: 0.6479\n",
      "Epoch 6, train_loss: 0.6339, val_loss: 0.6448\n",
      "Epoch 7, train_loss: 0.6286, val_loss: 0.6410\n",
      "Epoch 8, train_loss: 0.6241, val_loss: 0.6378\n",
      "Epoch 9, train_loss: 0.6226, val_loss: 0.6350\n",
      "Epoch 10, train_loss: 0.6110, val_loss: 0.6308\n",
      "Epoch 11, train_loss: 0.6091, val_loss: 0.6292\n",
      "Epoch 12, train_loss: 0.6049, val_loss: 0.6272\n",
      "Epoch 13, train_loss: 0.5913, val_loss: 0.6241\n",
      "Epoch 14, train_loss: 0.5836, val_loss: 0.6204\n",
      "Epoch 15, train_loss: 0.5829, val_loss: 0.6176\n",
      "Epoch 16, train_loss: 0.5728, val_loss: 0.6146\n",
      "Epoch 17, train_loss: 0.5633, val_loss: 0.6124\n",
      "Epoch 18, train_loss: 0.5625, val_loss: 0.6108\n",
      "Epoch 19, train_loss: 0.5544, val_loss: 0.6079\n",
      "Epoch 20, train_loss: 0.5443, val_loss: 0.6038\n",
      "Epoch 21, train_loss: 0.5340, val_loss: 0.6001\n",
      "Epoch 22, train_loss: 0.5292, val_loss: 0.5957\n",
      "Epoch 23, train_loss: 0.5252, val_loss: 0.5929\n",
      "Epoch 24, train_loss: 0.5148, val_loss: 0.5895\n",
      "Epoch 25, train_loss: 0.5121, val_loss: 0.5863\n",
      "Epoch 26, train_loss: 0.4977, val_loss: 0.5821\n",
      "Epoch 27, train_loss: 0.4845, val_loss: 0.5786\n",
      "Epoch 28, train_loss: 0.4905, val_loss: 0.5757\n",
      "Epoch 29, train_loss: 0.4811, val_loss: 0.5722\n",
      "Epoch 30, train_loss: 0.4651, val_loss: 0.5687\n",
      "Epoch 31, train_loss: 0.4683, val_loss: 0.5659\n",
      "Epoch 32, train_loss: 0.4478, val_loss: 0.5624\n",
      "Epoch 33, train_loss: 0.4514, val_loss: 0.5587\n",
      "Epoch 34, train_loss: 0.4409, val_loss: 0.5563\n",
      "Epoch 35, train_loss: 0.4318, val_loss: 0.5542\n",
      "Epoch 36, train_loss: 0.4160, val_loss: 0.5524\n",
      "Epoch 37, train_loss: 0.4146, val_loss: 0.5482\n",
      "Epoch 38, train_loss: 0.4052, val_loss: 0.5459\n",
      "Epoch 39, train_loss: 0.3976, val_loss: 0.5430\n",
      "Epoch 40, train_loss: 0.3905, val_loss: 0.5403\n",
      "Epoch 41, train_loss: 0.3939, val_loss: 0.5373\n",
      "Epoch 42, train_loss: 0.3839, val_loss: 0.5358\n",
      "Epoch 43, train_loss: 0.3644, val_loss: 0.5337\n",
      "Epoch 44, train_loss: 0.3559, val_loss: 0.5313\n",
      "Epoch 45, train_loss: 0.3578, val_loss: 0.5285\n",
      "Epoch 46, train_loss: 0.3512, val_loss: 0.5257\n",
      "Epoch 47, train_loss: 0.3597, val_loss: 0.5245\n",
      "Epoch 48, train_loss: 0.3337, val_loss: 0.5231\n",
      "Epoch 49, train_loss: 0.3218, val_loss: 0.5228\n",
      "Epoch 50, train_loss: 0.3221, val_loss: 0.5221\n",
      "Epoch 51, train_loss: 0.3164, val_loss: 0.5214\n",
      "Epoch 52, train_loss: 0.3132, val_loss: 0.5192\n",
      "Epoch 53, train_loss: 0.3120, val_loss: 0.5191\n",
      "Epoch 54, train_loss: 0.2999, val_loss: 0.5177\n",
      "Epoch 55, train_loss: 0.2943, val_loss: 0.5167\n",
      "Epoch 56, train_loss: 0.2775, val_loss: 0.5163\n",
      "Epoch 57, train_loss: 0.2846, val_loss: 0.5170\n",
      "Epoch 58, train_loss: 0.2716, val_loss: 0.5168\n",
      "Epoch 59, train_loss: 0.2716, val_loss: 0.5137\n",
      "Epoch 60, train_loss: 0.2702, val_loss: 0.5118\n",
      "Epoch 61, train_loss: 0.2575, val_loss: 0.5112\n",
      "Epoch 62, train_loss: 0.2609, val_loss: 0.5088\n",
      "Epoch 63, train_loss: 0.2558, val_loss: 0.5097\n",
      "Epoch 64, train_loss: 0.2509, val_loss: 0.5116\n",
      "Epoch 65, train_loss: 0.2462, val_loss: 0.5097\n",
      "Epoch 66, train_loss: 0.2400, val_loss: 0.5079\n",
      "Epoch 67, train_loss: 0.2370, val_loss: 0.5065\n",
      "Epoch 68, train_loss: 0.2425, val_loss: 0.5046\n",
      "Epoch 69, train_loss: 0.2393, val_loss: 0.5055\n",
      "Epoch 70, train_loss: 0.2327, val_loss: 0.5075\n",
      "Epoch 71, train_loss: 0.2321, val_loss: 0.5080\n",
      "Epoch 72, train_loss: 0.2272, val_loss: 0.5076\n",
      "Epoch 73, train_loss: 0.2173, val_loss: 0.5085\n",
      "Epoch 74, train_loss: 0.2115, val_loss: 0.5073\n",
      "Epoch 75, train_loss: 0.2150, val_loss: 0.5087\n",
      "Epoch 76, train_loss: 0.2117, val_loss: 0.5104\n",
      "Epoch 77, train_loss: 0.2060, val_loss: 0.5067\n",
      "Epoch 78, train_loss: 0.2040, val_loss: 0.5068\n",
      "Epoch 79, train_loss: 0.2055, val_loss: 0.5044\n",
      "Epoch 80, train_loss: 0.2005, val_loss: 0.5064\n",
      "Epoch 81, train_loss: 0.2042, val_loss: 0.5057\n",
      "Epoch 82, train_loss: 0.1890, val_loss: 0.5058\n",
      "Epoch 83, train_loss: 0.1924, val_loss: 0.5056\n",
      "Epoch 84, train_loss: 0.1936, val_loss: 0.5052\n",
      "Epoch 85, train_loss: 0.1920, val_loss: 0.5061\n",
      "Epoch 86, train_loss: 0.1902, val_loss: 0.5060\n",
      "Epoch 87, train_loss: 0.1885, val_loss: 0.5051\n",
      "Epoch 88, train_loss: 0.1881, val_loss: 0.5064\n",
      "Epoch 89, train_loss: 0.1816, val_loss: 0.5066\n",
      "Epoch 90, train_loss: 0.1858, val_loss: 0.5069\n",
      "Epoch 91, train_loss: 0.1766, val_loss: 0.5057\n",
      "Epoch 92, train_loss: 0.1886, val_loss: 0.5038\n",
      "Epoch 93, train_loss: 0.1743, val_loss: 0.5064\n",
      "Epoch 94, train_loss: 0.1799, val_loss: 0.5021\n",
      "Epoch 95, train_loss: 0.1798, val_loss: 0.5029\n",
      "Epoch 96, train_loss: 0.1699, val_loss: 0.5063\n",
      "Epoch 97, train_loss: 0.1697, val_loss: 0.5031\n",
      "Epoch 98, train_loss: 0.1707, val_loss: 0.5034\n",
      "Epoch 99, train_loss: 0.1690, val_loss: 0.5041\n",
      "Test the model on fold 1:\n",
      "{'roc_auc': 0.8770562770562771, 'pr_auc': 0.9159778416976525, 'fmax': 0.8450654235566494}\n",
      "Starting Fold: 2\n",
      "Epoch 0, train_loss: 0.6961, val_loss: 0.6840\n",
      "Epoch 1, train_loss: 0.6732, val_loss: 0.6770\n",
      "Epoch 2, train_loss: 0.6621, val_loss: 0.6735\n",
      "Epoch 3, train_loss: 0.6499, val_loss: 0.6703\n",
      "Epoch 4, train_loss: 0.6412, val_loss: 0.6659\n",
      "Epoch 5, train_loss: 0.6313, val_loss: 0.6631\n",
      "Epoch 6, train_loss: 0.6256, val_loss: 0.6604\n",
      "Epoch 7, train_loss: 0.6176, val_loss: 0.6590\n",
      "Epoch 8, train_loss: 0.6051, val_loss: 0.6568\n",
      "Epoch 9, train_loss: 0.6057, val_loss: 0.6531\n",
      "Epoch 10, train_loss: 0.5998, val_loss: 0.6514\n",
      "Epoch 11, train_loss: 0.5880, val_loss: 0.6497\n",
      "Epoch 12, train_loss: 0.5881, val_loss: 0.6464\n",
      "Epoch 13, train_loss: 0.5826, val_loss: 0.6439\n",
      "Epoch 14, train_loss: 0.5730, val_loss: 0.6427\n",
      "Epoch 15, train_loss: 0.5601, val_loss: 0.6389\n",
      "Epoch 16, train_loss: 0.5534, val_loss: 0.6373\n",
      "Epoch 17, train_loss: 0.5437, val_loss: 0.6339\n",
      "Epoch 18, train_loss: 0.5394, val_loss: 0.6315\n",
      "Epoch 19, train_loss: 0.5277, val_loss: 0.6279\n",
      "Epoch 20, train_loss: 0.5207, val_loss: 0.6250\n",
      "Epoch 21, train_loss: 0.5159, val_loss: 0.6233\n",
      "Epoch 22, train_loss: 0.5098, val_loss: 0.6198\n",
      "Epoch 23, train_loss: 0.4933, val_loss: 0.6175\n",
      "Epoch 24, train_loss: 0.4874, val_loss: 0.6137\n",
      "Epoch 25, train_loss: 0.4855, val_loss: 0.6110\n",
      "Epoch 26, train_loss: 0.4869, val_loss: 0.6088\n",
      "Epoch 27, train_loss: 0.4724, val_loss: 0.6060\n",
      "Epoch 28, train_loss: 0.4603, val_loss: 0.6032\n",
      "Epoch 29, train_loss: 0.4662, val_loss: 0.6000\n",
      "Epoch 30, train_loss: 0.4404, val_loss: 0.5952\n",
      "Epoch 31, train_loss: 0.4267, val_loss: 0.5928\n",
      "Epoch 32, train_loss: 0.4287, val_loss: 0.5898\n",
      "Epoch 33, train_loss: 0.4253, val_loss: 0.5859\n",
      "Epoch 34, train_loss: 0.4167, val_loss: 0.5849\n",
      "Epoch 35, train_loss: 0.3885, val_loss: 0.5811\n",
      "Epoch 36, train_loss: 0.3972, val_loss: 0.5792\n",
      "Epoch 37, train_loss: 0.3903, val_loss: 0.5776\n",
      "Epoch 38, train_loss: 0.3766, val_loss: 0.5753\n",
      "Epoch 39, train_loss: 0.3651, val_loss: 0.5714\n",
      "Epoch 40, train_loss: 0.3631, val_loss: 0.5705\n",
      "Epoch 41, train_loss: 0.3531, val_loss: 0.5687\n",
      "Epoch 42, train_loss: 0.3550, val_loss: 0.5654\n",
      "Epoch 43, train_loss: 0.3451, val_loss: 0.5631\n",
      "Epoch 44, train_loss: 0.3315, val_loss: 0.5606\n",
      "Epoch 45, train_loss: 0.3238, val_loss: 0.5565\n",
      "Epoch 46, train_loss: 0.3200, val_loss: 0.5545\n",
      "Epoch 47, train_loss: 0.3088, val_loss: 0.5526\n",
      "Epoch 48, train_loss: 0.3184, val_loss: 0.5520\n",
      "Epoch 49, train_loss: 0.3042, val_loss: 0.5502\n",
      "Epoch 50, train_loss: 0.2875, val_loss: 0.5480\n",
      "Epoch 51, train_loss: 0.2927, val_loss: 0.5481\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 52, train_loss: 0.2882, val_loss: 0.5456\n",
      "Epoch 53, train_loss: 0.2808, val_loss: 0.5439\n",
      "Epoch 54, train_loss: 0.2746, val_loss: 0.5458\n",
      "Epoch 55, train_loss: 0.2662, val_loss: 0.5439\n",
      "Epoch 56, train_loss: 0.2626, val_loss: 0.5395\n",
      "Epoch 57, train_loss: 0.2606, val_loss: 0.5389\n",
      "Epoch 58, train_loss: 0.2586, val_loss: 0.5381\n",
      "Epoch 59, train_loss: 0.2669, val_loss: 0.5382\n",
      "Epoch 60, train_loss: 0.2434, val_loss: 0.5366\n",
      "Epoch 61, train_loss: 0.2429, val_loss: 0.5370\n",
      "Epoch 62, train_loss: 0.2350, val_loss: 0.5350\n",
      "Epoch 63, train_loss: 0.2277, val_loss: 0.5359\n",
      "Epoch 64, train_loss: 0.2246, val_loss: 0.5331\n",
      "Epoch 65, train_loss: 0.2300, val_loss: 0.5326\n",
      "Epoch 66, train_loss: 0.2135, val_loss: 0.5321\n",
      "Epoch 67, train_loss: 0.2115, val_loss: 0.5342\n",
      "Epoch 68, train_loss: 0.2102, val_loss: 0.5328\n",
      "Epoch 69, train_loss: 0.2227, val_loss: 0.5339\n",
      "Epoch 70, train_loss: 0.2022, val_loss: 0.5333\n",
      "Epoch 71, train_loss: 0.2383, val_loss: 0.5336\n",
      "Epoch 72, train_loss: 0.2113, val_loss: 0.5280\n",
      "Epoch 73, train_loss: 0.1991, val_loss: 0.5277\n",
      "Epoch 74, train_loss: 0.2037, val_loss: 0.5268\n",
      "Epoch 75, train_loss: 0.2000, val_loss: 0.5325\n",
      "Epoch 76, train_loss: 0.1929, val_loss: 0.5306\n",
      "Epoch 77, train_loss: 0.2183, val_loss: 0.5264\n",
      "Epoch 78, train_loss: 0.1884, val_loss: 0.5274\n",
      "Epoch 79, train_loss: 0.1883, val_loss: 0.5270\n",
      "Epoch 80, train_loss: 0.1974, val_loss: 0.5267\n",
      "Epoch 81, train_loss: 0.1854, val_loss: 0.5263\n",
      "Epoch 82, train_loss: 0.1853, val_loss: 0.5253\n",
      "Epoch 83, train_loss: 0.1805, val_loss: 0.5263\n",
      "Epoch 84, train_loss: 0.1815, val_loss: 0.5248\n",
      "Epoch 85, train_loss: 0.1825, val_loss: 0.5253\n",
      "Epoch 86, train_loss: 0.1682, val_loss: 0.5266\n",
      "Epoch 87, train_loss: 0.1799, val_loss: 0.5263\n",
      "Epoch 88, train_loss: 0.1654, val_loss: 0.5287\n",
      "Epoch 89, train_loss: 0.1672, val_loss: 0.5271\n",
      "Epoch 90, train_loss: 0.1693, val_loss: 0.5260\n",
      "Epoch 91, train_loss: 0.1653, val_loss: 0.5233\n",
      "Epoch 92, train_loss: 0.1710, val_loss: 0.5240\n",
      "Epoch 93, train_loss: 0.1657, val_loss: 0.5228\n",
      "Epoch 94, train_loss: 0.1689, val_loss: 0.5244\n",
      "Epoch 95, train_loss: 0.1624, val_loss: 0.5240\n",
      "Epoch 96, train_loss: 0.1695, val_loss: 0.5213\n",
      "Epoch 97, train_loss: 0.1609, val_loss: 0.5216\n",
      "Epoch 98, train_loss: 0.1572, val_loss: 0.5225\n",
      "Epoch 99, train_loss: 0.1630, val_loss: 0.5227\n",
      "Test the model on fold 2:\n",
      "{'roc_auc': 0.8480902777777777, 'pr_auc': 0.8627463206410391, 'fmax': 0.7868802580267797}\n",
      "Starting Fold: 3\n",
      "Epoch 0, train_loss: 0.6901, val_loss: 0.6879\n",
      "Epoch 1, train_loss: 0.6748, val_loss: 0.6799\n",
      "Epoch 2, train_loss: 0.6568, val_loss: 0.6759\n",
      "Epoch 3, train_loss: 0.6466, val_loss: 0.6712\n",
      "Epoch 4, train_loss: 0.6371, val_loss: 0.6660\n",
      "Epoch 5, train_loss: 0.6180, val_loss: 0.6610\n",
      "Epoch 6, train_loss: 0.6172, val_loss: 0.6599\n",
      "Epoch 7, train_loss: 0.6095, val_loss: 0.6558\n",
      "Epoch 8, train_loss: 0.5989, val_loss: 0.6519\n",
      "Epoch 9, train_loss: 0.5978, val_loss: 0.6509\n",
      "Epoch 10, train_loss: 0.5825, val_loss: 0.6478\n",
      "Epoch 11, train_loss: 0.5767, val_loss: 0.6436\n",
      "Epoch 12, train_loss: 0.5618, val_loss: 0.6400\n",
      "Epoch 13, train_loss: 0.5452, val_loss: 0.6366\n",
      "Epoch 14, train_loss: 0.5504, val_loss: 0.6325\n",
      "Epoch 15, train_loss: 0.5389, val_loss: 0.6298\n",
      "Epoch 16, train_loss: 0.5344, val_loss: 0.6245\n",
      "Epoch 17, train_loss: 0.5249, val_loss: 0.6232\n",
      "Epoch 18, train_loss: 0.5210, val_loss: 0.6181\n",
      "Epoch 19, train_loss: 0.4972, val_loss: 0.6155\n",
      "Epoch 20, train_loss: 0.4945, val_loss: 0.6135\n",
      "Epoch 21, train_loss: 0.4867, val_loss: 0.6148\n",
      "Epoch 22, train_loss: 0.4797, val_loss: 0.6121\n",
      "Epoch 23, train_loss: 0.4527, val_loss: 0.6074\n",
      "Epoch 24, train_loss: 0.4574, val_loss: 0.6024\n",
      "Epoch 25, train_loss: 0.4365, val_loss: 0.6021\n",
      "Epoch 26, train_loss: 0.4388, val_loss: 0.6004\n",
      "Epoch 27, train_loss: 0.4281, val_loss: 0.5955\n",
      "Epoch 28, train_loss: 0.4099, val_loss: 0.5921\n",
      "Epoch 29, train_loss: 0.4032, val_loss: 0.5867\n",
      "Epoch 30, train_loss: 0.4036, val_loss: 0.5863\n",
      "Epoch 31, train_loss: 0.3868, val_loss: 0.5846\n",
      "Epoch 32, train_loss: 0.3789, val_loss: 0.5823\n",
      "Epoch 33, train_loss: 0.3677, val_loss: 0.5824\n",
      "Epoch 34, train_loss: 0.3564, val_loss: 0.5820\n",
      "Epoch 35, train_loss: 0.3544, val_loss: 0.5815\n",
      "Epoch 36, train_loss: 0.3380, val_loss: 0.5804\n",
      "Epoch 37, train_loss: 0.3336, val_loss: 0.5762\n",
      "Epoch 38, train_loss: 0.3317, val_loss: 0.5739\n",
      "Epoch 39, train_loss: 0.3264, val_loss: 0.5742\n",
      "Epoch 40, train_loss: 0.3200, val_loss: 0.5717\n",
      "Epoch 41, train_loss: 0.2971, val_loss: 0.5712\n",
      "Epoch 42, train_loss: 0.3025, val_loss: 0.5698\n",
      "Epoch 43, train_loss: 0.2918, val_loss: 0.5675\n",
      "Epoch 44, train_loss: 0.2896, val_loss: 0.5674\n",
      "Epoch 45, train_loss: 0.2743, val_loss: 0.5650\n",
      "Epoch 46, train_loss: 0.2779, val_loss: 0.5651\n",
      "Epoch 47, train_loss: 0.2725, val_loss: 0.5677\n",
      "Epoch 48, train_loss: 0.2686, val_loss: 0.5694\n",
      "Epoch 49, train_loss: 0.2573, val_loss: 0.5695\n",
      "Epoch 50, train_loss: 0.2534, val_loss: 0.5674\n",
      "Epoch 51, train_loss: 0.2494, val_loss: 0.5661\n",
      "Epoch 52, train_loss: 0.2445, val_loss: 0.5659\n",
      "Epoch 53, train_loss: 0.2327, val_loss: 0.5654\n",
      "Epoch 54, train_loss: 0.2330, val_loss: 0.5646\n",
      "Epoch 55, train_loss: 0.2397, val_loss: 0.5626\n",
      "Epoch 56, train_loss: 0.2145, val_loss: 0.5631\n",
      "Epoch 57, train_loss: 0.2183, val_loss: 0.5651\n",
      "Epoch 58, train_loss: 0.2197, val_loss: 0.5642\n",
      "Epoch 59, train_loss: 0.2175, val_loss: 0.5656\n",
      "Epoch 60, train_loss: 0.2099, val_loss: 0.5645\n",
      "Epoch 61, train_loss: 0.2424, val_loss: 0.5661\n",
      "Epoch 62, train_loss: 0.2022, val_loss: 0.5681\n",
      "Epoch 63, train_loss: 0.2054, val_loss: 0.5677\n",
      "Epoch 64, train_loss: 0.2058, val_loss: 0.5656\n",
      "Epoch 65, train_loss: 0.1914, val_loss: 0.5654\n",
      "Epoch 66, train_loss: 0.1923, val_loss: 0.5645\n",
      "Epoch 67, train_loss: 0.1945, val_loss: 0.5637\n",
      "Epoch 68, train_loss: 0.1848, val_loss: 0.5641\n",
      "Epoch 69, train_loss: 0.1848, val_loss: 0.5606\n",
      "Epoch 70, train_loss: 0.1821, val_loss: 0.5629\n",
      "Epoch 71, train_loss: 0.1972, val_loss: 0.5631\n",
      "Epoch 72, train_loss: 0.1777, val_loss: 0.5635\n",
      "Epoch 73, train_loss: 0.1727, val_loss: 0.5619\n",
      "Epoch 74, train_loss: 0.1762, val_loss: 0.5602\n",
      "Epoch 75, train_loss: 0.1862, val_loss: 0.5633\n",
      "Epoch 76, train_loss: 0.1759, val_loss: 0.5622\n",
      "Epoch 77, train_loss: 0.1772, val_loss: 0.5645\n",
      "Epoch 78, train_loss: 0.1606, val_loss: 0.5641\n",
      "Epoch 79, train_loss: 0.1730, val_loss: 0.5628\n",
      "Epoch 80, train_loss: 0.1654, val_loss: 0.5592\n",
      "Epoch 81, train_loss: 0.1773, val_loss: 0.5605\n",
      "Epoch 82, train_loss: 0.1623, val_loss: 0.5625\n",
      "Epoch 83, train_loss: 0.1596, val_loss: 0.5618\n",
      "Epoch 84, train_loss: 0.1546, val_loss: 0.5615\n",
      "Epoch 85, train_loss: 0.1670, val_loss: 0.5640\n",
      "Epoch 86, train_loss: 0.1546, val_loss: 0.5646\n",
      "Early Stopping!\n",
      "Test the model on fold 3:\n",
      "{'roc_auc': 0.8736979166666665, 'pr_auc': 0.8893138008993168, 'fmax': 0.8450654235566494}\n",
      "Starting Fold: 4\n",
      "Epoch 0, train_loss: 0.6931, val_loss: 0.6903\n",
      "Epoch 1, train_loss: 0.6814, val_loss: 0.6838\n",
      "Epoch 2, train_loss: 0.6536, val_loss: 0.6790\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import gzip\n",
    "import pickle\n",
    "\n",
    "pretrained_folds = 5\n",
    "all_final_disease_class_metrics = []\n",
    "results_file = osp.join(RESULTS_STORAGE, f'disease_classification_results.gz')\n",
    "for i in range(1, pretrained_folds + 1):\n",
    "    print('############################################################')\n",
    "    print(f'# START CLASSIFICATION USING PRETRAINED MODEL FROM FOLD: {i} #')\n",
    "    print('############################################################')\n",
    "    results = train_disease_classification(osp.join(RESULTS_STORAGE, f'model_fold_{i}.ptm'))\n",
    "    all_final_disease_class_metrics.append(results)\n",
    "    with gzip.open(results_file, mode='wb') as f:\n",
    "        pickle.dump(all_final_disease_class_metrics, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eval the results.\n",
    "* Load the results.\n",
    "* Compute the mean of all models and folds.\n",
    "* Plot comparison between DiGI, SmuDGE and our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with gzip.open(results_file, mode='rb') as file:\n",
    "    all_final_disease_class_metrics = pickle.load(file, encoding='bytes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ophthamological (support: 170)\n",
      "roc_auc:\t0.86\n",
      "pr_auc: \t0.87\n",
      "fmax:   \t0.82\n",
      "\n",
      "Connective tissue (support: 56)\n",
      "roc_auc:\t0.74\n",
      "pr_auc: \t0.78\n",
      "fmax:   \t0.73\n",
      "\n",
      "Endocrine (support: 124)\n",
      "roc_auc:\t0.82\n",
      "pr_auc: \t0.83\n",
      "fmax:   \t0.79\n",
      "\n",
      "Skeletal (support: 88)\n",
      "roc_auc:\t0.87\n",
      "pr_auc: \t0.89\n",
      "fmax:   \t0.86\n",
      "\n",
      "Metabolic (support: 326)\n",
      "roc_auc:\t0.89\n",
      "pr_auc: \t0.89\n",
      "fmax:   \t0.85\n",
      "\n",
      "Cardiovascular (support: 117)\n",
      "roc_auc:\t0.74\n",
      "pr_auc: \t0.78\n",
      "fmax:   \t0.74\n",
      "\n",
      "Dermatological (support: 102)\n",
      "roc_auc:\t0.88\n",
      "pr_auc: \t0.91\n",
      "fmax:   \t0.86\n",
      "\n",
      "Renal (support: 66)\n",
      "roc_auc:\t0.74\n",
      "pr_auc: \t0.77\n",
      "fmax:   \t0.76\n",
      "\n",
      "Hematological (support: 189)\n",
      "roc_auc:\t0.85\n",
      "pr_auc: \t0.87\n",
      "fmax:   \t0.80\n",
      "\n",
      "Immunological (support: 128)\n",
      "roc_auc:\t0.89\n",
      "pr_auc: \t0.91\n",
      "fmax:   \t0.85\n",
      "\n",
      "Muscular (support: 85)\n",
      "roc_auc:\t0.84\n",
      "pr_auc: \t0.86\n",
      "fmax:   \t0.82\n",
      "\n",
      "Developmental (support: 57)\n",
      "roc_auc:\t0.70\n",
      "pr_auc: \t0.69\n",
      "fmax:   \t0.74\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ours_roc_auc = []\n",
    "ours_pr_auc = []\n",
    "ours_fmax = []\n",
    "\n",
    "\n",
    "for disease_class in disease_class_target_classes:\n",
    "    print(f'{disease_class} (support: {disease_class_counts[disease_class]})')\n",
    "    roc_aucs, pr_aucs, fmaxs = [], [], []\n",
    "    for pretrained_results_by_model in all_final_disease_class_metrics:\n",
    "        for fold in range(5):\n",
    "            roc_aucs.append(pretrained_results_by_model[f'{disease_class}_{fold}']['roc_auc'])\n",
    "            pr_aucs.append(pretrained_results_by_model[f'{disease_class}_{fold}']['pr_auc'])\n",
    "            fmaxs.append(pretrained_results_by_model[f'{disease_class}_{fold}']['fmax'])\n",
    "    \n",
    "    ours_roc_auc.append(np.mean(roc_aucs))\n",
    "    ours_pr_auc.append(np.mean(pr_aucs))\n",
    "    ours_fmax.append(np.mean(fmaxs))\n",
    "    \n",
    "    print(f'roc_auc:\\t{np.mean(roc_aucs):0.2f}')\n",
    "    print(f'pr_auc: \\t{np.mean(pr_aucs):0.2f}')\n",
    "    print(f'fmax:   \\t{np.mean(fmaxs):0.2f}')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1656x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1656x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1656x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1656x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib\n",
    "import matplotlib.mlab as mlab\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import metrics\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "results = {\n",
    "    'DiGI': {\n",
    "        'ROC-auc': [0.71, 0.71, 0.76, 0.81, 0.83, 0.71, 0.79, 0.72, 0.70, 0.86, 0.74, 0.69],\n",
    "        'Pr-auc': [0.71, 0.75, 0.78, 0.84, 0.87, 0.77, 0.85, 0.72, 0.75, 0.92, 0.81, 0.64],\n",
    "        'fmax': [0.73, 0.80, 0.81, 0.84, 0.87, 0.81, 0.82, 0.71, 0.79, 0.93, 0.84, 0.60]\n",
    "    },\n",
    "    'SmuDGE': {\n",
    "        'ROC-auc': [0.80, 0.43, 0.82, 0.65, 0.85, 0.69, 0.82, 0.76, 0.68, 0.82, 0.78, 0.65],\n",
    "        'Pr-auc': [0.83, 0.45, 0.82, 0.70, 0.81, 0.67, 0.80, 0.77, 0.70, 0.76, 0.82, 0.72],\n",
    "        'fmax': [0.84, 0.68, 0.78, 0.69, 0.80, 0.69, 0.80, 0.77, 0.69, 0.80, 0.75, 0.69]\n",
    "    },\n",
    "    'Ours': {\n",
    "        'ROC-auc': ours_roc_auc,\n",
    "        'Pr-auc': ours_pr_auc,\n",
    "        'fmax': ours_fmax\n",
    "    }\n",
    "}\n",
    "\n",
    "colors = [\n",
    "    [0.427450980392156, 0.6078431372549019, 0.5019607843137255, 1],\n",
    "    [0.149019607843137, 0.2156862745098039, 0.3215686274509804, 1],\n",
    "    [0.788235294117647, 0.6235294117647059, 0.0745098039215686, 1]\n",
    "]\n",
    "\n",
    "\n",
    "def plot_bar_chart(plot_only_metrics, width=0.1):\n",
    "    plt.figure(figsize=[23, 8])\n",
    "    ax = plt.gca()\n",
    "    ax.set(ylim=[0.3, 1])\n",
    "\n",
    "    X = np.arange(len(results['Ours']['ROC-auc']))\n",
    "\n",
    "    offset = 0\n",
    "    for i, (method, metrics) in enumerate(results.items()):\n",
    "        opacity = 0.5\n",
    "        for metric_name, metric_values in metrics.items():\n",
    "            if metric_name in plot_only_metrics:\n",
    "                color = colors[i]\n",
    "                color[-1] = opacity\n",
    "                plt.bar(X + offset, metric_values, color=color, width=width)\n",
    "                opacity += 0.2\n",
    "                offset += width\n",
    "\n",
    "    legends = [f'{method} {metric}' for method in results.keys() for metric in results[method].keys() if metric in plot_only_metrics]\n",
    "    plt.legend(legends, loc='upper right')\n",
    "\n",
    "    # Overiding the x axis ticks with the disease names\n",
    "    tick_offset = (3*len(plot_only_metrics) - 1)*width/2\n",
    "    plt.xticks(\n",
    "        [i + tick_offset for i in range(len(disease_classes))],\n",
    "        disease_classes\n",
    "    )\n",
    "    plt.title('Results for disease classification. ({})'.format(', '.join(plot_only_metrics)))\n",
    "    plt.xlabel('Disease class')\n",
    "    plt.ylabel('Score')\n",
    "    plt.savefig(\n",
    "        osp.join(RESULTS_STORAGE, 'disease_gene_classification_result_bar_chart_{}.pdf'.format('_'.join(plot_only_metrics)))\n",
    "    )\n",
    "    plt.show()\n",
    "\n",
    "plot_bar_chart(plot_only_metrics=['ROC-auc', 'Pr-auc', 'fmax'], width=0.1)\n",
    "plot_bar_chart(plot_only_metrics=['ROC-auc'], width=0.3)\n",
    "plot_bar_chart(plot_only_metrics=['Pr-auc'], width=0.3)\n",
    "plot_bar_chart(plot_only_metrics=['fmax'], width=0.3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
